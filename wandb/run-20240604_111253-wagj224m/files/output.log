Epoch 0/1000, Train Loss: 2.3281081914901733, Val Loss: 0.5390220880508423, Train Acc: 0.525, Val Acc: 1.0, LR: 3.776202506787275e-05
Epoch 10/1000, Train Loss: 1.4089096784591675, Val Loss: 0.5319215059280396, Train Acc: 0.65, Val Acc: 1.0, LR: 3.701285361204635e-05
Epoch 20/1000, Train Loss: 1.9976631999015808, Val Loss: 0.5160030722618103, Train Acc: 0.475, Val Acc: 1.0, LR: 3.626368215621995e-05
Epoch 30/1000, Train Loss: 2.50224232673645, Val Loss: 0.4958057999610901, Train Acc: 0.425, Val Acc: 1.0, LR: 3.551451070039355e-05
Epoch 40/1000, Train Loss: 1.5894100666046143, Val Loss: 0.49257412552833557, Train Acc: 0.575, Val Acc: 1.0, LR: 3.476533924456715e-05
Epoch 50/1000, Train Loss: 2.1044872999191284, Val Loss: 0.4708731770515442, Train Acc: 0.5, Val Acc: 1.0, LR: 3.401616778874074e-05
Epoch 60/1000, Train Loss: 2.2119975090026855, Val Loss: 0.4488278925418854, Train Acc: 0.375, Val Acc: 1.0, LR: 3.3266996332914334e-05
Epoch 70/1000, Train Loss: 1.9828075766563416, Val Loss: 0.444769948720932, Train Acc: 0.525, Val Acc: 1.0, LR: 3.251782487708792e-05
Epoch 80/1000, Train Loss: 2.4525957107543945, Val Loss: 0.4429566264152527, Train Acc: 0.425, Val Acc: 1.0, LR: 3.176865342126151e-05
Epoch 90/1000, Train Loss: 1.9240102767944336, Val Loss: 0.4415265917778015, Train Acc: 0.425, Val Acc: 1.0, LR: 3.101948196543513e-05
Epoch 100/1000, Train Loss: 2.6083970069885254, Val Loss: 0.46014541387557983, Train Acc: 0.325, Val Acc: 1.0, LR: 3.0270310509608714e-05
Epoch 110/1000, Train Loss: 2.0072025656700134, Val Loss: 0.45165643095970154, Train Acc: 0.525, Val Acc: 1.0, LR: 2.9521139053782297e-05
Epoch 120/1000, Train Loss: 1.8715202808380127, Val Loss: 0.4332803189754486, Train Acc: 0.5, Val Acc: 1.0, LR: 2.8771967597955873e-05
Epoch 130/1000, Train Loss: 2.9062216877937317, Val Loss: 0.4429575800895691, Train Acc: 0.55, Val Acc: 1.0, LR: 2.8022796142129456e-05
Epoch 140/1000, Train Loss: 2.2998523116111755, Val Loss: 0.4499148428440094, Train Acc: 0.525, Val Acc: 1.0, LR: 2.7273624686303036e-05
Epoch 150/1000, Train Loss: 1.165599137544632, Val Loss: 0.457916259765625, Train Acc: 0.6, Val Acc: 1.0, LR: 2.652445323047661e-05
Epoch 160/1000, Train Loss: 2.1402264833450317, Val Loss: 0.46587419509887695, Train Acc: 0.525, Val Acc: 1.0, LR: 2.5775281774650186e-05
Epoch 170/1000, Train Loss: 1.9398841261863708, Val Loss: 0.4533526301383972, Train Acc: 0.55, Val Acc: 1.0, LR: 2.502611031882377e-05
Epoch 180/1000, Train Loss: 1.905491590499878, Val Loss: 0.47204190492630005, Train Acc: 0.425, Val Acc: 1.0, LR: 2.4276938862997352e-05
Epoch 190/1000, Train Loss: 1.7627577781677246, Val Loss: 0.4816325306892395, Train Acc: 0.475, Val Acc: 1.0, LR: 2.352776740717092e-05
Epoch 200/1000, Train Loss: 1.9741783142089844, Val Loss: 0.47140973806381226, Train Acc: 0.475, Val Acc: 1.0, LR: 2.2778595951344494e-05
Epoch 210/1000, Train Loss: 2.159459710121155, Val Loss: 0.4839780926704407, Train Acc: 0.6, Val Acc: 1.0, LR: 2.2029424495518074e-05
Epoch 220/1000, Train Loss: 1.8804779648780823, Val Loss: 0.4774321913719177, Train Acc: 0.5, Val Acc: 1.0, LR: 2.1280253039691647e-05
Epoch 230/1000, Train Loss: 1.0675290822982788, Val Loss: 0.4808503985404968, Train Acc: 0.725, Val Acc: 1.0, LR: 2.053108158386522e-05
Epoch 240/1000, Train Loss: 2.051429867744446, Val Loss: 0.47344499826431274, Train Acc: 0.55, Val Acc: 1.0, LR: 1.9781910128038803e-05
Epoch 250/1000, Train Loss: 1.6238550543785095, Val Loss: 0.4677909016609192, Train Acc: 0.625, Val Acc: 1.0, LR: 1.903273867221238e-05
Epoch 260/1000, Train Loss: 1.170023500919342, Val Loss: 0.447486937046051, Train Acc: 0.6, Val Acc: 1.0, LR: 1.8283567216385953e-05
Epoch 270/1000, Train Loss: 1.1626036167144775, Val Loss: 0.4289211332798004, Train Acc: 0.65, Val Acc: 1.0, LR: 1.7534395760559522e-05
Epoch 280/1000, Train Loss: 1.7820073962211609, Val Loss: 0.4153504967689514, Train Acc: 0.55, Val Acc: 1.0, LR: 1.678522430473311e-05
Epoch 290/1000, Train Loss: 1.5324093699455261, Val Loss: 0.41066789627075195, Train Acc: 0.525, Val Acc: 1.0, LR: 1.6036052848906675e-05
Epoch 300/1000, Train Loss: 1.5194172859191895, Val Loss: 0.38148772716522217, Train Acc: 0.575, Val Acc: 1.0, LR: 1.528688139308024e-05
Epoch 310/1000, Train Loss: 1.7232086658477783, Val Loss: 0.3628539741039276, Train Acc: 0.65, Val Acc: 1.0, LR: 1.4537709937253824e-05
Epoch 320/1000, Train Loss: 1.6305452585220337, Val Loss: 0.3568190634250641, Train Acc: 0.6, Val Acc: 1.0, LR: 1.3788538481427406e-05
Epoch 330/1000, Train Loss: 1.995884358882904, Val Loss: 0.3512541651725769, Train Acc: 0.625, Val Acc: 1.0, LR: 1.303936702560099e-05
Epoch 340/1000, Train Loss: 1.1674797534942627, Val Loss: 0.33477047085762024, Train Acc: 0.725, Val Acc: 1.0, LR: 1.2290195569774579e-05
Epoch 350/1000, Train Loss: 1.4576433300971985, Val Loss: 0.3220786154270172, Train Acc: 0.575, Val Acc: 1.0, LR: 1.1541024113948165e-05
Epoch 360/1000, Train Loss: 1.985931396484375, Val Loss: 0.31731733679771423, Train Acc: 0.6, Val Acc: 1.0, LR: 1.0791852658121745e-05
Epoch 370/1000, Train Loss: 1.5065906643867493, Val Loss: 0.3192625343799591, Train Acc: 0.6, Val Acc: 1.0, LR: 1.0042681202295335e-05
Epoch 380/1000, Train Loss: 1.1637804508209229, Val Loss: 0.31838133931159973, Train Acc: 0.675, Val Acc: 1.0, LR: 9.293509746468925e-06
Epoch 390/1000, Train Loss: 1.6627490520477295, Val Loss: 0.31418952345848083, Train Acc: 0.55, Val Acc: 1.0, LR: 8.544338290642511e-06
Epoch 400/1000, Train Loss: 1.2225000262260437, Val Loss: 0.30843743681907654, Train Acc: 0.825, Val Acc: 1.0, LR: 7.795166834816096e-06
Epoch 410/1000, Train Loss: 1.9066947102546692, Val Loss: 0.3007659316062927, Train Acc: 0.6, Val Acc: 1.0, LR: 7.04599537898968e-06
Epoch 420/1000, Train Loss: 1.9613658785820007, Val Loss: 0.29743462800979614, Train Acc: 0.525, Val Acc: 1.0, LR: 6.296823923163261e-06
Epoch 430/1000, Train Loss: 1.6270891427993774, Val Loss: 0.29744720458984375, Train Acc: 0.525, Val Acc: 1.0, LR: 5.5476524673368395e-06
Epoch 440/1000, Train Loss: 2.320667862892151, Val Loss: 0.2922925353050232, Train Acc: 0.55, Val Acc: 1.0, LR: 4.79848101151042e-06
Epoch 450/1000, Train Loss: 1.0650398135185242, Val Loss: 0.2891783118247986, Train Acc: 0.725, Val Acc: 1.0, LR: 4.049309555684004e-06
Epoch 460/1000, Train Loss: 1.6064081192016602, Val Loss: 0.287637859582901, Train Acc: 0.625, Val Acc: 1.0, LR: 3.300138099857585e-06
Epoch 470/1000, Train Loss: 1.2181420922279358, Val Loss: 0.2819278836250305, Train Acc: 0.625, Val Acc: 1.0, LR: 2.5509666440311674e-06
Epoch 480/1000, Train Loss: 1.8155559301376343, Val Loss: 0.2796275019645691, Train Acc: 0.5, Val Acc: 1.0, LR: 1.80179518820475e-06
Epoch 490/1000, Train Loss: 1.6831575632095337, Val Loss: 0.2794524133205414, Train Acc: 0.6, Val Acc: 1.0, LR: 1.0526237323783315e-06
Epoch 500/1000, Train Loss: 1.2560688853263855, Val Loss: 0.27908533811569214, Train Acc: 0.65, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 510/1000, Train Loss: 1.8874893188476562, Val Loss: 0.2790159583091736, Train Acc: 0.525, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 520/1000, Train Loss: 1.2004023790359497, Val Loss: 0.2788864076137543, Train Acc: 0.65, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 530/1000, Train Loss: 2.050528347492218, Val Loss: 0.2783358097076416, Train Acc: 0.475, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 540/1000, Train Loss: 1.3882628679275513, Val Loss: 0.27773410081863403, Train Acc: 0.65, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 550/1000, Train Loss: 1.1182727068662643, Val Loss: 0.27706170082092285, Train Acc: 0.625, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 560/1000, Train Loss: 2.0053268671035767, Val Loss: 0.27674850821495056, Train Acc: 0.575, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 570/1000, Train Loss: 1.290950059890747, Val Loss: 0.27613556385040283, Train Acc: 0.675, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 580/1000, Train Loss: 1.4517369866371155, Val Loss: 0.2758466601371765, Train Acc: 0.525, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 590/1000, Train Loss: 2.209238648414612, Val Loss: 0.2757989764213562, Train Acc: 0.5, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 600/1000, Train Loss: 1.473814606666565, Val Loss: 0.27519527077674866, Train Acc: 0.55, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 610/1000, Train Loss: 2.095578968524933, Val Loss: 0.27550333738327026, Train Acc: 0.575, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 620/1000, Train Loss: 1.9466736316680908, Val Loss: 0.2753531336784363, Train Acc: 0.625, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 630/1000, Train Loss: 1.2588096261024475, Val Loss: 0.27506962418556213, Train Acc: 0.675, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 640/1000, Train Loss: 1.8848198652267456, Val Loss: 0.27464964985847473, Train Acc: 0.575, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 650/1000, Train Loss: 1.4160418510437012, Val Loss: 0.2746410667896271, Train Acc: 0.7, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 660/1000, Train Loss: 1.433243751525879, Val Loss: 0.2739856243133545, Train Acc: 0.725, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 670/1000, Train Loss: 1.9166308045387268, Val Loss: 0.273549884557724, Train Acc: 0.475, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 680/1000, Train Loss: 1.4570928812026978, Val Loss: 0.27294859290122986, Train Acc: 0.675, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 690/1000, Train Loss: 1.8351276516914368, Val Loss: 0.271626353263855, Train Acc: 0.575, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 700/1000, Train Loss: 1.6911932229995728, Val Loss: 0.27101558446884155, Train Acc: 0.525, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 710/1000, Train Loss: 1.4667174220085144, Val Loss: 0.2708685100078583, Train Acc: 0.6, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 720/1000, Train Loss: 1.7756630778312683, Val Loss: 0.2707383930683136, Train Acc: 0.7, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 730/1000, Train Loss: 1.3843685984611511, Val Loss: 0.2708014249801636, Train Acc: 0.625, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 740/1000, Train Loss: 2.0828371047973633, Val Loss: 0.2708534300327301, Train Acc: 0.55, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 750/1000, Train Loss: 1.0753092169761658, Val Loss: 0.27081766724586487, Train Acc: 0.8, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 760/1000, Train Loss: 1.6913309693336487, Val Loss: 0.27044162154197693, Train Acc: 0.5, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 770/1000, Train Loss: 1.6848905682563782, Val Loss: 0.26995566487312317, Train Acc: 0.625, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 780/1000, Train Loss: 1.0278517603874207, Val Loss: 0.26977354288101196, Train Acc: 0.675, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 790/1000, Train Loss: 1.8426254391670227, Val Loss: 0.2694385349750519, Train Acc: 0.675, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 800/1000, Train Loss: 1.7214200496673584, Val Loss: 0.26895907521247864, Train Acc: 0.6, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 810/1000, Train Loss: 1.552154779434204, Val Loss: 0.2687971591949463, Train Acc: 0.6, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 820/1000, Train Loss: 1.7828829288482666, Val Loss: 0.2686423659324646, Train Acc: 0.575, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 830/1000, Train Loss: 1.3773357272148132, Val Loss: 0.26832103729248047, Train Acc: 0.65, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 840/1000, Train Loss: 1.1699810028076172, Val Loss: 0.2682536542415619, Train Acc: 0.6, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 850/1000, Train Loss: 1.608565628528595, Val Loss: 0.2680084705352783, Train Acc: 0.75, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 860/1000, Train Loss: 1.7369715571403503, Val Loss: 0.2672542631626129, Train Acc: 0.55, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 870/1000, Train Loss: 1.4419238567352295, Val Loss: 0.26693063974380493, Train Acc: 0.675, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 880/1000, Train Loss: 1.4188691973686218, Val Loss: 0.2667863965034485, Train Acc: 0.7, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 890/1000, Train Loss: 1.463614583015442, Val Loss: 0.26683783531188965, Train Acc: 0.675, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 900/1000, Train Loss: 1.2459495663642883, Val Loss: 0.26647669076919556, Train Acc: 0.7, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 910/1000, Train Loss: 1.6296630501747131, Val Loss: 0.26603832840919495, Train Acc: 0.675, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 920/1000, Train Loss: 1.401503562927246, Val Loss: 0.26564812660217285, Train Acc: 0.65, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 930/1000, Train Loss: 1.4857513904571533, Val Loss: 0.26557523012161255, Train Acc: 0.575, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 940/1000, Train Loss: 2.1382031440734863, Val Loss: 0.26507842540740967, Train Acc: 0.55, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 950/1000, Train Loss: 1.6081941723823547, Val Loss: 0.2645701766014099, Train Acc: 0.675, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 960/1000, Train Loss: 1.9152604937553406, Val Loss: 0.2642107903957367, Train Acc: 0.65, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 970/1000, Train Loss: 1.5597153902053833, Val Loss: 0.2639329135417938, Train Acc: 0.65, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 980/1000, Train Loss: 0.9620126485824585, Val Loss: 0.26357170939445496, Train Acc: 0.8, Val Acc: 1.0, LR: 3.783694221345552e-07
Epoch 990/1000, Train Loss: 1.2727811336517334, Val Loss: 0.26327213644981384, Train Acc: 0.775, Val Acc: 1.0, LR: 3.783694221345552e-07
Finished training model...
Simulating on true reward function...
 ****** Running generation 0 ******
[33m[W 2024-06-04 11:13:05,362][39m Trial 0 failed with parameters: {'hidden_size': 67, 'learning_rate': 3.783694221345539e-05, 'weight_decay': 0.0004334321751212527} because of the following error: The value None could not be cast to float..
[33m[W 2024-06-04 11:13:05,362][39m Trial 0 failed with value None.
[34m[1mwandb[39m[22m: [33mWARNING[39m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'Train Loss': 2.3281081914901733, 'Validation Loss': 0.5390220880508423, 'Train Accuracy': 0.525, 'Validation Accuracy': 1.0, '_timestamp': 1717513974.613573}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.weight': {'_type': 'histogram', 'values': [958, 934, 960, 913, 960, 902, 1023, 942, 885, 951, 928, 1022, 922, 950, 943, 948, 852, 923, 891, 914, 915, 931, 964, 965, 910, 981, 934, 907, 920, 963, 911, 985, 944, 965, 945, 916, 945, 962, 909, 962, 929, 927, 1001, 938, 931, 1011, 923, 951, 950, 935, 928, 957, 949, 918, 923, 931, 927, 977, 977, 973, 914, 960, 975, 940], 'bins': [-0.11145175248384476, -0.10796879976987839, -0.10448584705591202, -0.10100290179252625, -0.09751994907855988, -0.0940369963645935, -0.09055404365062714, -0.08707109838724136, -0.083588145673275, -0.08010519295930862, -0.07662224024534225, -0.07313928753137589, -0.06965634226799011, -0.06617338955402374, -0.06269043684005737, -0.0592074878513813, -0.05572453513741493, -0.05224158242344856, -0.04875863343477249, -0.04527568072080612, -0.04179273173213005, -0.03830977901816368, -0.03482683002948761, -0.03134387731552124, -0.02786092646420002, -0.0243779756128788, -0.02089502476155758, -0.01741207391023636, -0.013929122127592564, -0.010446171276271343, -0.0069632199592888355, -0.0034802688751369715, 2.682209014892578e-06, 0.0034856332931667566, 0.006968584377318621, 0.010451535694301128, 0.013934486545622349, 0.017417438328266144, 0.020900389179587364, 0.024383340030908585, 0.027866290882229805, 0.031349241733551025, 0.034832194447517395, 0.038315143436193466, 0.041798096150159836, 0.04528104513883591, 0.04876399785280228, 0.05224694684147835, 0.05572989955544472, 0.05921285226941109, 0.06269580125808716, 0.06617875397205353, 0.0696617066860199, 0.07314465194940567, 0.07662760466337204, 0.08011055737733841, 0.08359351009130478, 0.08707646280527115, 0.09055940806865692, 0.09404236078262329, 0.09752531349658966, 0.10100826621055603, 0.1044912114739418, 0.10797416418790817, 0.11145711690187454]}, '_timestamp': 1717513974.6147368}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.bias': {'_type': 'histogram', 'values': [3, 0, 0, 0, 2, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 3, 2, 3, 1, 1, 2, 3, 1, 1, 0, 0, 3, 1, 1, 0, 0, 1, 1, 2, 1, 1, 2, 0, 1, 1, 3, 1, 2, 0, 1, 2, 1, 0, 1, 0, 1, 0, 0, 3, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1], 'bins': [-0.03238842263817787, -0.03136648237705231, -0.030344540253281593, -0.02932259999215603, -0.028300657868385315, -0.02727871760725975, -0.026256775483489037, -0.025234835222363472, -0.024212893098592758, -0.023190952837467194, -0.02216901257634163, -0.021147070452570915, -0.02012513019144535, -0.019103188067674637, -0.018081247806549072, -0.01705930568277836, -0.016037365421652794, -0.015015424229204655, -0.013993483036756516, -0.012971541844308376, -0.011949600651860237, -0.010927660390734673, -0.009905719198286533, -0.008883778005838394, -0.007861836813390255, -0.006839895620942116, -0.005817954428493977, -0.004796013701707125, -0.0037740725092589855, -0.0027521313168108463, -0.0017301903571933508, -0.0007082492811605334, 0.00031369179487228394, 0.0013356328709051013, 0.0023575739469379187, 0.003379514906555414, 0.004401456099003553, 0.005423397291451693, 0.0064453380182385445, 0.007467279210686684, 0.008489220403134823, 0.009511161595582962, 0.010533102788031101, 0.01155504398047924, 0.012576984241604805, 0.013598925434052944, 0.014620866626501083, 0.015642808750271797, 0.016664749011397362, 0.017686689272522926, 0.01870863139629364, 0.019730571657419205, 0.02075251378118992, 0.021774454042315483, 0.022796396166086197, 0.02381833642721176, 0.024840276688337326, 0.02586221881210804, 0.026884159073233604, 0.027906101197004318, 0.028928041458129883, 0.029949983581900597, 0.03097192384302616, 0.031993865966796875, 0.03301580622792244]}, '_timestamp': 1717513974.6149602}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.weight': {'_type': 'histogram', 'values': [36, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 2, 1, 2, 5], 'bins': [0.9999243021011353, 0.9999266862869263, 0.9999290108680725, 0.9999313950538635, 0.9999337792396545, 0.9999361038208008, 0.9999384880065918, 0.9999408721923828, 0.9999432563781738, 0.9999455809593201, 0.9999479651451111, 0.9999503493309021, 0.9999526739120483, 0.9999550580978394, 0.9999574422836304, 0.9999597668647766, 0.9999621510505676, 0.9999645352363586, 0.9999668598175049, 0.9999692440032959, 0.9999716281890869, 0.9999739527702332, 0.9999763369560242, 0.9999787211418152, 0.9999810457229614, 0.9999834299087524, 0.9999858140945435, 0.9999881982803345, 0.9999905228614807, 0.9999929070472717, 0.9999952912330627, 0.999997615814209, 1.0, 1.000002384185791, 1.000004768371582, 1.000007152557373, 1.0000094175338745, 1.0000118017196655, 1.0000141859054565, 1.0000165700912476, 1.0000189542770386, 1.0000213384628296, 1.000023603439331, 1.000025987625122, 1.000028371810913, 1.000030755996704, 1.0000331401824951, 1.0000355243682861, 1.0000379085540771, 1.0000401735305786, 1.0000425577163696, 1.0000449419021606, 1.0000473260879517, 1.0000497102737427, 1.0000520944595337, 1.0000543594360352, 1.0000567436218262, 1.0000591278076172, 1.0000615119934082, 1.0000638961791992, 1.0000662803649902, 1.0000685453414917, 1.0000709295272827, 1.0000733137130737, 1.0000756978988647]}, '_timestamp': 1717513974.615157}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.bias': {'_type': 'histogram', 'values': [5, 2, 0, 0, 0, 6, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 25, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 2, 3, 2, 2, 7], 'bins': [-7.562291284557432e-05, -7.325916521949694e-05, -7.089542486937717e-05, -6.853167724329978e-05, -6.616793689318001e-05, -6.380418926710263e-05, -6.144044891698286e-05, -5.907670492888428e-05, -5.6712960940785706e-05, -5.434921331470832e-05, -5.1985469326609746e-05, -4.962172533851117e-05, -4.725798135041259e-05, -4.4894237362314016e-05, -4.253049337421544e-05, -4.016674938611686e-05, -3.7803005398018286e-05, -3.543926140991971e-05, -3.307551742182113e-05, -3.0711773433722556e-05, -2.834802944562398e-05, -2.5984283638536e-05, -2.3620539650437422e-05, -2.1256795662338845e-05, -1.889305167424027e-05, -1.6529307686141692e-05, -1.4165563698043115e-05, -1.1801818800449837e-05, -9.43807481235126e-06, -7.074330824252684e-06, -4.710586381406756e-06, -2.346842165934504e-06, 1.69020495377481e-08, 2.38064626501e-06, 4.744390480482252e-06, 7.10813492332818e-06, 9.471878911426757e-06, 1.1835622899525333e-05, 1.4199367797118612e-05, 1.6563111785217188e-05, 1.8926855773315765e-05, 2.129059976141434e-05, 2.3654343749512918e-05, 2.6018087737611495e-05, 2.8381833544699475e-05, 3.074557753279805e-05, 3.310932152089663e-05, 3.5473065508995205e-05, 3.783680949709378e-05, 4.020055348519236e-05, 4.2564297473290935e-05, 4.492804146138951e-05, 4.729178544948809e-05, 4.9655529437586665e-05, 5.201927342568524e-05, 5.438301741378382e-05, 5.67467650398612e-05, 5.911050902795978e-05, 6.147425301605836e-05, 6.383799336617813e-05, 6.620174099225551e-05, 6.856548134237528e-05, 7.092922896845266e-05, 7.329296931857243e-05, 7.565671694464982e-05]}, '_timestamp': 1717513974.615318}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.weight': {'_type': 'histogram', 'values': [76, 64, 82, 77, 63, 50, 67, 64, 72, 76, 74, 75, 64, 71, 85, 75, 69, 64, 82, 64, 76, 56, 73, 72, 70, 66, 81, 66, 75, 78, 64, 63, 71, 70, 77, 73, 63, 69, 65, 74, 61, 75, 67, 79, 65, 64, 79, 75, 70, 71, 70, 84, 64, 61, 72, 63, 73, 70, 79, 55, 65, 69, 69, 73], 'bins': [-0.2991413176059723, -0.2897917628288269, -0.2804422080516815, -0.2710926830768585, -0.26174312829971313, -0.25239357352256775, -0.24304403364658356, -0.23369447886943817, -0.22434493899345398, -0.2149953842163086, -0.2056458294391632, -0.19629628956317902, -0.18694673478603363, -0.17759719491004944, -0.16824764013290405, -0.15889810025691986, -0.14954854547977448, -0.1401989907026291, -0.1308494508266449, -0.12149990350008011, -0.11215035617351532, -0.10280080139636993, -0.09345125406980515, -0.08410170674324036, -0.07475215941667557, -0.06540261209011078, -0.05605306476354599, -0.0467035137116909, -0.037353966385126114, -0.028004419058561325, -0.018654869869351387, -0.009305321611464024, 4.4226646423339844e-05, 0.009393774904310703, 0.018743323162198067, 0.028092872351408005, 0.037442419677972794, 0.04679196700453758, 0.05614151805639267, 0.06549106538295746, 0.07484061270952225, 0.08419016003608704, 0.09353970736265182, 0.10288925468921661, 0.112238809466362, 0.12158835679292679, 0.13093790411949158, 0.14028744399547577, 0.14963699877262115, 0.15898655354976654, 0.16833609342575073, 0.17768564820289612, 0.1870351880788803, 0.1963847428560257, 0.2057342827320099, 0.21508383750915527, 0.22443339228630066, 0.23378293216228485, 0.24313248693943024, 0.25248202681541443, 0.2618315815925598, 0.2711811363697052, 0.2805306613445282, 0.2898802161216736, 0.29922977089881897]}, '_timestamp': 1717513974.61554}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.bias': {'_type': 'histogram', 'values': [1, 1, 1, 0, 0, 1, 1, 1, 0, 3, 2, 2, 1, 0, 0, 2, 2, 2, 1, 3, 2, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 0, 0, 1, 1, 0, 1, 3, 0, 2, 0, 1, 0, 2, 2, 1, 4, 2, 1, 1, 2, 3, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 2], 'bins': [-0.1199418231844902, -0.11619710922241211, -0.11245240271091461, -0.10870768874883652, -0.10496298223733902, -0.10121826827526093, -0.09747356176376343, -0.09372884780168533, -0.08998414129018784, -0.08623942732810974, -0.08249472081661224, -0.07875000685453415, -0.07500530034303665, -0.07126058638095856, -0.06751587986946106, -0.06377116590738297, -0.06002645567059517, -0.05628174543380737, -0.05253703519701958, -0.04879232496023178, -0.045047614723443985, -0.04130290448665619, -0.03755819424986839, -0.0338134840130806, -0.03006877191364765, -0.026324061676859856, -0.02257935144007206, -0.018834641203284264, -0.015089930035173893, -0.011345219798386097, -0.007600509095937014, -0.003855798626318574, -0.00011108815670013428, 0.0036336223129183054, 0.007378332782536745, 0.011123043484985828, 0.014867753721773624, 0.018612464889883995, 0.02235717512667179, 0.026101885363459587, 0.029846595600247383, 0.03359130769968033, 0.037336017936468124, 0.04108072817325592, 0.044825438410043716, 0.04857014864683151, 0.05231485888361931, 0.056059569120407104, 0.0598042793571949, 0.0635489895939827, 0.06729370355606079, 0.07103841006755829, 0.07478312402963638, 0.07852783054113388, 0.08227254450321198, 0.08601725101470947, 0.08976196497678757, 0.09350667148828506, 0.09725138545036316, 0.10099609196186066, 0.10474080592393875, 0.10848551243543625, 0.11223022639751434, 0.11597493290901184, 0.11971964687108994]}, '_timestamp': 1717513974.6157}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.weight': {'_type': 'histogram', 'values': [11, 3, 8, 1, 2, 2, 2, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 2, 1, 0, 0, 1, 1, 2, 2, 0, 1, 0, 0, 1, 2, 2, 3, 0, 0, 2, 2, 6], 'bins': [0.99992436170578, 0.999926745891571, 0.9999290704727173, 0.9999314546585083, 0.9999337792396545, 0.9999361634254456, 0.9999384880065918, 0.9999408721923828, 0.999943196773529, 0.9999455809593201, 0.9999479055404663, 0.9999502897262573, 0.9999526739120483, 0.9999549984931946, 0.9999573826789856, 0.9999597072601318, 0.9999620914459229, 0.9999644160270691, 0.9999668002128601, 0.9999691247940063, 0.9999715089797974, 0.9999738335609436, 0.9999762177467346, 0.9999786019325256, 0.9999809265136719, 0.9999833106994629, 0.9999856352806091, 0.9999880194664001, 0.9999903440475464, 0.9999927282333374, 0.9999950528144836, 0.9999974370002747, 0.9999997615814209, 1.000002145767212, 1.000004529953003, 1.000006914138794, 1.0000091791152954, 1.0000115633010864, 1.0000139474868774, 1.0000163316726685, 1.00001859664917, 1.000020980834961, 1.000023365020752, 1.000025749206543, 1.000028133392334, 1.0000303983688354, 1.0000327825546265, 1.0000351667404175, 1.0000375509262085, 1.00003981590271, 1.000042200088501, 1.000044584274292, 1.000046968460083, 1.0000492334365845, 1.0000516176223755, 1.0000540018081665, 1.0000563859939575, 1.0000587701797485, 1.00006103515625, 1.000063419342041, 1.000065803527832, 1.000068187713623, 1.0000704526901245, 1.0000728368759155, 1.0000752210617065]}, '_timestamp': 1717513974.6158562}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.bias': {'_type': 'histogram', 'values': [4, 3, 4, 2, 2, 2, 0, 1, 1, 0, 0, 0, 2, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 3, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 3, 3, 1, 0, 0, 1, 2, 4, 2, 2, 5, 0, 6], 'bins': [-7.56109511712566e-05, -7.324691978283226e-05, -7.088288839440793e-05, -6.851885700598359e-05, -6.615482561755925e-05, -6.379079422913492e-05, -6.142676284071058e-05, -5.9062731452286243e-05, -5.6698700063861907e-05, -5.433466503745876e-05, -5.1970633649034426e-05, -4.960660226061009e-05, -4.724257087218575e-05, -4.4878539483761415e-05, -4.251450809533708e-05, -4.015047670691274e-05, -3.7786445318488404e-05, -3.542241393006407e-05, -3.305838254163973e-05, -3.0694351153215393e-05, -2.8330319764791057e-05, -2.5966286557377316e-05, -2.360225516895298e-05, -2.1238223780528642e-05, -1.8874192392104305e-05, -1.651016100367997e-05, -1.4146129615255632e-05, -1.1782097317336593e-05, -9.418065928912256e-06, -7.054034540487919e-06, -4.690002697316231e-06, -2.325971081518219e-06, 3.80605342797935e-08, 2.402092150077806e-06, 4.766123765875818e-06, 7.130155609047506e-06, 9.494186997471843e-06, 1.185821838589618e-05, 1.4222250683815219e-05, 1.6586282072239555e-05, 1.8950313460663892e-05, 2.131434484908823e-05, 2.3678376237512566e-05, 2.6042407625936903e-05, 2.8406440833350644e-05, 3.077047222177498e-05, 3.313450361019932e-05, 3.5498534998623654e-05, 3.786256638704799e-05, 4.022659777547233e-05, 4.2590629163896665e-05, 4.4954660552321e-05, 4.731869194074534e-05, 4.9682723329169676e-05, 5.204675471759401e-05, 5.441078610601835e-05, 5.6774821132421494e-05, 5.913885252084583e-05, 6.150288390927017e-05, 6.38669152976945e-05, 6.623094668611884e-05, 6.859497807454318e-05, 7.095900946296751e-05, 7.332304085139185e-05, 7.568707223981619e-05]}, '_timestamp': 1717513974.616006}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.weight': {'_type': 'histogram', 'values': [73, 59, 53, 68, 66, 81, 72, 82, 63, 90, 72, 78, 57, 58, 72, 72, 62, 68, 65, 58, 76, 64, 61, 65, 75, 74, 74, 60, 65, 62, 90, 70, 71, 79, 74, 84, 55, 52, 81, 69, 68, 78, 74, 60, 68, 67, 62, 83, 76, 69, 72, 70, 66, 78, 78, 72, 75, 85, 59, 89, 66, 61, 69, 74], 'bins': [-0.29907914996147156, -0.28973251581192017, -0.2803858816623688, -0.2710392475128174, -0.2616926431655884, -0.252346009016037, -0.2429993748664856, -0.2336527407169342, -0.2243061065673828, -0.21495948731899261, -0.20561285316944122, -0.19626621901988983, -0.18691959977149963, -0.17757296562194824, -0.16822633147239685, -0.15887971222400665, -0.14953307807445526, -0.14018644392490387, -0.13083982467651367, -0.12149319052696228, -0.11214655637741089, -0.1027999296784401, -0.0934533029794693, -0.08410666882991791, -0.07476004213094711, -0.06541341543197632, -0.05606678128242493, -0.04672015458345413, -0.03737352415919304, -0.028026893734931946, -0.018680265173316002, -0.009333635680377483, 1.2993812561035156e-05, 0.009359623305499554, 0.018706252798438072, 0.028052881360054016, 0.03739951178431511, 0.0467461422085762, 0.056092768907547, 0.06543940305709839, 0.07478602975606918, 0.08413265645503998, 0.09347929060459137, 0.10282591730356216, 0.11217254400253296, 0.12151917815208435, 0.13086581230163574, 0.14021243155002594, 0.14955906569957733, 0.15890569984912872, 0.16825231909751892, 0.1775989532470703, 0.1869455873966217, 0.1962922066450119, 0.2056388407945633, 0.21498547494411469, 0.22433209419250488, 0.23367872834205627, 0.24302536249160767, 0.25237199664115906, 0.26171863079071045, 0.27106523513793945, 0.28041186928749084, 0.28975850343704224, 0.29910513758659363]}, '_timestamp': 1717513974.616191}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.bias': {'_type': 'histogram', 'values': [2, 1, 2, 2, 0, 0, 0, 2, 0, 0, 0, 3, 0, 2, 0, 0, 0, 3, 2, 0, 0, 0, 1, 1, 0, 2, 0, 2, 2, 0, 2, 3, 0, 2, 0, 2, 2, 1, 1, 3, 0, 0, 1, 0, 0, 1, 2, 0, 2, 1, 0, 4, 0, 1, 2, 0, 1, 1, 0, 2, 1, 2, 1, 2], 'bins': [-0.11952056735754013, -0.11575619876384735, -0.11199183017015457, -0.10822746902704239, -0.10446310043334961, -0.10069873183965683, -0.09693436324596405, -0.09317000210285187, -0.08940563350915909, -0.08564126491546631, -0.08187689632177353, -0.07811253517866135, -0.07434816658496857, -0.07058379799127579, -0.06681942939758301, -0.06305506825447083, -0.059290699660778046, -0.055526331067085266, -0.05176196247339249, -0.047997597604990005, -0.044233229011297226, -0.040468864142894745, -0.036704495549201965, -0.032940130680799484, -0.029175762087106705, -0.025411395356059074, -0.021647028625011444, -0.017882661893963814, -0.014118295162916183, -0.010353928431868553, -0.006589561700820923, -0.0028251949697732925, 0.0009391717612743378, 0.004703538492321968, 0.008467905223369598, 0.012232271954417229, 0.01599663868546486, 0.01976100541651249, 0.02352537214756012, 0.02728973887860775, 0.03105410560965538, 0.03481847420334816, 0.03858283907175064, 0.04234720766544342, 0.0461115725338459, 0.04987594112753868, 0.05364030599594116, 0.05740467458963394, 0.06116904318332672, 0.0649334117770195, 0.06869777292013168, 0.07246214151382446, 0.07622651010751724, 0.07999087870121002, 0.0837552398443222, 0.08751960843801498, 0.09128397703170776, 0.09504834562540054, 0.09881270676851273, 0.1025770753622055, 0.10634144395589828, 0.11010581254959106, 0.11387017369270325, 0.11763454228639603, 0.1213989108800888]}, '_timestamp': 1717513974.61634}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.weight': {'_type': 'histogram', 'values': [20, 2, 4, 5, 2, 1, 2, 0, 1, 1, 0, 2, 1, 0, 2, 2, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 1, 0, 1, 0, 2, 2, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 2, 1], 'bins': [0.9999243021011353, 0.9999266266822815, 0.9999289512634277, 0.9999313354492188, 0.999933660030365, 0.9999359846115112, 0.9999383091926575, 0.9999406933784485, 0.9999430179595947, 0.999945342540741, 0.9999476671218872, 0.9999499917030334, 0.9999523758888245, 0.9999547004699707, 0.9999570250511169, 0.9999593496322632, 0.9999617338180542, 0.9999640583992004, 0.9999663829803467, 0.9999687075614929, 0.9999710321426392, 0.9999734163284302, 0.9999757409095764, 0.9999780654907227, 0.9999803900718689, 0.9999827146530151, 0.9999850988388062, 0.9999874234199524, 0.9999897480010986, 0.9999920725822449, 0.9999944567680359, 0.9999967813491821, 0.9999991059303284, 1.0000014305114746, 1.0000038146972656, 1.000006079673767, 1.000008463859558, 1.0000108480453491, 1.0000131130218506, 1.0000154972076416, 1.000017762184143, 1.000020146369934, 1.000022530555725, 1.0000247955322266, 1.0000271797180176, 1.000029444694519, 1.00003182888031, 1.000034213066101, 1.0000364780426025, 1.0000388622283936, 1.000041127204895, 1.000043511390686, 1.000045895576477, 1.0000481605529785, 1.0000505447387695, 1.0000529289245605, 1.000055193901062, 1.000057578086853, 1.0000598430633545, 1.0000622272491455, 1.0000646114349365, 1.000066876411438, 1.000069260597229, 1.0000715255737305, 1.0000739097595215]}, '_timestamp': 1717513974.6165}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.bias': {'_type': 'histogram', 'values': [8, 6, 1, 3, 1, 4, 1, 0, 0, 0, 0, 0, 1, 2, 0, 3, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 2, 0, 0, 2, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 2, 1, 0, 4, 1, 1, 4, 3], 'bins': [-7.565098349004984e-05, -7.329908839892596e-05, -7.094719330780208e-05, -6.85952982166782e-05, -6.624340312555432e-05, -6.389150803443044e-05, -6.153961294330657e-05, -5.9187721490161493e-05, -5.6835826399037614e-05, -5.4483931307913736e-05, -5.2132036216789857e-05, -4.978014112566598e-05, -4.7428249672520906e-05, -4.507635458139703e-05, -4.272445949027315e-05, -4.037256439914927e-05, -3.802066930802539e-05, -3.566877421690151e-05, -3.331687912577763e-05, -3.096498403465375e-05, -2.8613090762519278e-05, -2.62611956713954e-05, -2.3909302399260923e-05, -2.1557407308137044e-05, -1.9205512217013165e-05, -1.6853617125889286e-05, -1.450172294426011e-05, -1.2149828762630932e-05, -9.797933671507053e-06, -7.446039035130525e-06, -5.094144398753997e-06, -2.742249762377469e-06, -3.903551260009408e-07, 1.9615395103755873e-06, 4.313434146752115e-06, 6.665328783128643e-06, 9.017223419505171e-06, 1.136911851062905e-05, 1.3721012692258228e-05, 1.6072906873887405e-05, 1.8424801965011284e-05, 2.0776697056135163e-05, 2.312859214725904e-05, 2.5480485419393517e-05, 2.7832380510517396e-05, 3.0184275601641275e-05, 3.253616887377575e-05, 3.488806396489963e-05, 3.723995905602351e-05, 3.959185414714739e-05, 4.1943749238271266e-05, 4.4295644329395145e-05, 4.6647539420519024e-05, 4.8999430873664096e-05, 5.1351325964787975e-05, 5.3703221055911854e-05, 5.605511614703573e-05, 5.840701123815961e-05, 6.075890632928349e-05, 6.311079778242856e-05, 6.546269287355244e-05, 6.781458796467632e-05, 7.01664830558002e-05, 7.251837814692408e-05, 7.487027323804796e-05]}, '_timestamp': 1717513974.6166432}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.weight': {'_type': 'histogram', 'values': [3, 1, 0, 1, 3, 1, 1, 0, 0, 1, 0, 0, 0, 2, 1, 2, 1, 1, 4, 0, 1, 1, 3, 2, 0, 0, 0, 1, 0, 0, 1, 1, 2, 2, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 2, 1, 2, 1, 0, 1, 0, 0, 2, 1, 0, 0, 2, 0, 1, 2, 0, 2, 4], 'bins': [-0.4153755009174347, -0.4024410843849182, -0.3895066976547241, -0.37657228112220764, -0.36363786458969116, -0.35070347785949707, -0.3377690613269806, -0.3248346447944641, -0.31190025806427, -0.29896584153175354, -0.28603142499923706, -0.2730970084667206, -0.2601626217365265, -0.24722820520401, -0.23429380357265472, -0.22135938704013824, -0.20842498540878296, -0.19549058377742767, -0.1825561672449112, -0.1696217656135559, -0.15668734908103943, -0.14375294744968414, -0.13081854581832886, -0.11788413673639297, -0.10494972765445709, -0.09201531857252121, -0.07908090949058533, -0.06614650785923004, -0.05321209877729416, -0.040277689695358276, -0.027343284338712692, -0.014408877119421959, -0.0014744699001312256, 0.011459937319159508, 0.02439434453845024, 0.037328749895095825, 0.05026315897703171, 0.06319756805896759, 0.07613196969032288, 0.08906637877225876, 0.10200078785419464, 0.11493519693613052, 0.1278696060180664, 0.1408040076494217, 0.15373840928077698, 0.16667282581329346, 0.17960722744464874, 0.19254164397716522, 0.2054760456085205, 0.2184104472398758, 0.23134486377239227, 0.24427926540374756, 0.25721368193626404, 0.27014806866645813, 0.2830824851989746, 0.2960169017314911, 0.30895131826400757, 0.32188570499420166, 0.33482012152671814, 0.3477545380592346, 0.3606889247894287, 0.3736233413219452, 0.38655775785446167, 0.39949214458465576, 0.41242656111717224]}, '_timestamp': 1717513974.6167989}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.bias': {'_type': 'histogram', 'values': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'bins': [-0.5705195069313049, -0.5548945069313049, -0.5392695069313049, -0.5236445069313049, -0.5080195069313049, -0.4923945367336273, -0.4767695367336273, -0.4611445367336273, -0.4455195367336273, -0.4298945367336273, -0.4142695367336273, -0.3986445367336273, -0.3830195367336273, -0.3673945367336273, -0.3517695367336273, -0.3361445367336273, -0.3205195367336273, -0.3048945367336273, -0.2892695367336273, -0.2736445367336273, -0.2580195367336273, -0.24239453673362732, -0.22676953673362732, -0.21114453673362732, -0.19551953673362732, -0.17989453673362732, -0.16426953673362732, -0.14864453673362732, -0.13301953673362732, -0.11739452928304672, -0.10176952928304672, -0.08614452928304672, -0.07051952928304672, -0.05489452928304672, -0.03926952928304672, -0.023644529283046722, -0.008019529283046722, 0.007605470716953278, 0.023230470716953278, 0.03885547071695328, 0.05448047071695328, 0.07010547071695328, 0.08573047071695328, 0.10135547071695328, 0.11698047071695328, 0.13260546326637268, 0.14823046326637268, 0.16385546326637268, 0.17948046326637268, 0.19510546326637268, 0.21073046326637268, 0.22635546326637268, 0.24198046326637268, 0.2576054632663727, 0.2732304632663727, 0.2888554632663727, 0.3044804632663727, 0.3201054632663727, 0.3357304632663727, 0.3513554632663727, 0.3669804632663727, 0.3826054632663727, 0.3982304632663727, 0.4138554632663727, 0.4294804632663727]}, '_timestamp': 1717513974.616972}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TP Reward Distribution': {'_type': 'histogram', 'values': [2, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], 'bins': [3.4, 3.678125, 3.95625, 4.234375, 4.5125, 4.790625, 5.06875, 5.346875, 5.625, 5.903125, 6.18125, 6.459375, 6.737500000000001, 7.015625, 7.29375, 7.571875, 7.85, 8.128125, 8.40625, 8.684375, 8.9625, 9.240625, 9.51875, 9.796875, 10.075000000000001, 10.353125, 10.63125, 10.909375, 11.1875, 11.465625000000001, 11.74375, 12.021875000000001, 12.3, 12.578125, 12.856250000000001, 13.134375, 13.412500000000001, 13.690625, 13.96875, 14.246875000000001, 14.525, 14.803125000000001, 15.08125, 15.359375, 15.637500000000001, 15.915625, 16.19375, 16.471875, 16.75, 17.028125, 17.30625, 17.584375, 17.8625, 18.140625, 18.41875, 18.696875, 18.975, 19.253125, 19.53125, 19.809375, 20.0875, 20.365624999999998, 20.64375, 20.921875, 21.2]}, '_timestamp': 1717513974.617333}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TN Reward Distribution': {'_type': 'histogram', 'values': [5, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [4.0, 8.144791666666666, 12.289583333333333, 16.434375, 20.579166666666666, 24.723958333333332, 28.86875, 33.01354166666667, 37.15833333333333, 41.303124999999994, 45.447916666666664, 49.592708333333334, 53.7375, 57.88229166666666, 62.02708333333333, 66.171875, 70.31666666666666, 74.46145833333333, 78.60624999999999, 82.75104166666667, 86.89583333333333, 91.04062499999999, 95.18541666666667, 99.33020833333333, 103.475, 107.61979166666666, 111.76458333333332, 115.909375, 120.05416666666666, 124.19895833333332, 128.34375, 132.48854166666666, 136.63333333333333, 140.778125, 144.92291666666665, 149.06770833333331, 153.21249999999998, 157.35729166666667, 161.50208333333333, 165.646875, 169.79166666666666, 173.93645833333332, 178.08124999999998, 182.22604166666665, 186.37083333333334, 190.515625, 194.66041666666666, 198.80520833333333, 202.95, 207.09479166666665, 211.23958333333331, 215.38437499999998, 219.52916666666664, 223.67395833333333, 227.81875, 231.96354166666666, 236.10833333333332, 240.25312499999998, 244.39791666666665, 248.5427083333333, 252.6875, 256.83229166666666, 260.9770833333333, 265.121875, 269.26666666666665]}, '_timestamp': 1717513974.6175208}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FP Reward Distribution': {'_type': 'histogram', 'values': [5, 2, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [2.6666666666666665, 5.452083333333333, 8.2375, 11.022916666666667, 13.808333333333334, 16.59375, 19.37916666666667, 22.164583333333336, 24.950000000000003, 27.73541666666667, 30.520833333333336, 33.30625, 36.09166666666667, 38.87708333333333, 41.6625, 44.447916666666664, 47.233333333333334, 50.018750000000004, 52.80416666666667, 55.58958333333334, 58.375, 61.16041666666667, 63.94583333333333, 66.73125, 69.51666666666668, 72.30208333333334, 75.0875, 77.87291666666668, 80.65833333333335, 83.44375000000001, 86.22916666666667, 89.01458333333335, 91.80000000000001, 94.58541666666667, 97.37083333333335, 100.15625000000001, 102.94166666666668, 105.72708333333334, 108.51250000000002, 111.29791666666668, 114.08333333333334, 116.86875000000002, 119.65416666666668, 122.43958333333335, 125.22500000000001, 128.01041666666669, 130.79583333333332, 133.58125, 136.36666666666667, 139.15208333333334, 141.9375, 144.72291666666666, 147.50833333333333, 150.29375, 153.07916666666668, 155.86458333333334, 158.65, 161.43541666666667, 164.22083333333333, 167.00625, 169.79166666666666, 172.57708333333335, 175.3625, 178.14791666666667, 180.93333333333334]}, '_timestamp': 1717513974.617683}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FN Reward Distribution': {'_type': 'histogram', 'values': [1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [2.8666666666666667, 3.904166666666667, 4.941666666666666, 5.979166666666667, 7.0166666666666675, 8.054166666666667, 9.091666666666667, 10.129166666666668, 11.166666666666668, 12.204166666666667, 13.241666666666667, 14.279166666666669, 15.316666666666668, 16.354166666666668, 17.39166666666667, 18.429166666666667, 19.46666666666667, 20.50416666666667, 21.541666666666668, 22.57916666666667, 23.616666666666667, 24.65416666666667, 25.69166666666667, 26.729166666666668, 27.76666666666667, 28.80416666666667, 29.84166666666667, 30.87916666666667, 31.91666666666667, 32.954166666666666, 33.99166666666667, 35.02916666666667, 36.06666666666667, 37.10416666666667, 38.14166666666667, 39.17916666666667, 40.21666666666667, 41.25416666666667, 42.29166666666667, 43.32916666666667, 44.36666666666667, 45.40416666666667, 46.44166666666667, 47.47916666666667, 48.51666666666667, 49.554166666666674, 50.59166666666667, 51.62916666666667, 52.66666666666667, 53.70416666666667, 54.741666666666674, 55.77916666666667, 56.81666666666667, 57.85416666666667, 58.89166666666667, 59.929166666666674, 60.966666666666676, 62.00416666666667, 63.04166666666667, 64.07916666666667, 65.11666666666667, 66.15416666666667, 67.19166666666666, 68.22916666666667, 69.26666666666667]}, '_timestamp': 1717513974.617832}).
Population's average fitness: 2333.29667 stdev: 8885.46747
Best fitness: 40948.86667 - size: (4, 20) - species 1 - id 16
Average adjusted fitness: 0.057
Mean genetic distance 1.120, standard deviation 0.240
Population of 20 members in 1 species:
   ID   age  size  fitness  adj fit  stag
  ====  ===  ====  =======  =======  ====
     1    0    20  40948.9    0.057     0
Total extinctions: 0
Generation time: 7.849 sec
 ****** Running generation 1 ******
Traceback (most recent call last):
  File "/Users/alextang/Documents/EmergeLab/ai-car-preference-learning/main.py", line 95, in <module>
    start_simulation("./config/agent_config.txt", args.generations[0])
  File "/Users/alextang/Documents/EmergeLab/ai-car-preference-learning/main.py", line 39, in start_simulation
    population.run(run_simulation, max_generations)
  File "/Users/alextang/.pyenv/versions/nocturne_lab/lib/python3.10/site-packages/neat/population.py", line 89, in run
    fitness_function(list(iteritems(self.population)), self.config)
  File "/Users/alextang/Documents/EmergeLab/ai-car-preference-learning/agent.py", line 380, in run_simulation
    clock.tick(60)  # 60 FPS
KeyboardInterrupt