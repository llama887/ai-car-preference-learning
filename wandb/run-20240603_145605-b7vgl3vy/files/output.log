Epoch 0/1000, Train Loss: 2.6127776503562927, Val Loss: 0.7638857364654541, Train Acc: 0.575, Val Acc: 0.0, LR: 0.0007173665013555387
Epoch 10/1000, Train Loss: 1.9534142017364502, Val Loss: 0.5522900819778442, Train Acc: 0.6, Val Acc: 0.8999999761581421, LR: 0.0007031344651971053
Epoch 20/1000, Train Loss: 1.6209726333618164, Val Loss: 0.4845600724220276, Train Acc: 0.575, Val Acc: 0.8999999761581421, LR: 0.000688902429038672
Epoch 30/1000, Train Loss: 1.5956174731254578, Val Loss: 0.45663923025131226, Train Acc: 0.625, Val Acc: 0.8999999761581421, LR: 0.0006746703928802387
Epoch 40/1000, Train Loss: 1.5178576707839966, Val Loss: 0.4482344090938568, Train Acc: 0.55, Val Acc: 0.8999999761581421, LR: 0.0006604383567218054
Epoch 50/1000, Train Loss: 1.8631129264831543, Val Loss: 0.41113147139549255, Train Acc: 0.675, Val Acc: 0.8999999761581421, LR: 0.000646206320563372
Epoch 60/1000, Train Loss: 0.8492043316364288, Val Loss: 0.3861834704875946, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 0.0006319742844049387
Epoch 70/1000, Train Loss: 1.583102285861969, Val Loss: 0.33951956033706665, Train Acc: 0.55, Val Acc: 0.8999999761581421, LR: 0.0006177422482465054
Epoch 80/1000, Train Loss: 0.9747374355792999, Val Loss: 0.31101399660110474, Train Acc: 0.7, Val Acc: 0.8999999761581421, LR: 0.0006035102120880721
Epoch 90/1000, Train Loss: 1.1064739227294922, Val Loss: 0.33138221502304077, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 0.0005892781759296387
Epoch 100/1000, Train Loss: 1.165092945098877, Val Loss: 0.30052870512008667, Train Acc: 0.675, Val Acc: 0.8999999761581421, LR: 0.0005750461397712054
Epoch 110/1000, Train Loss: 0.8481608629226685, Val Loss: 0.30222100019454956, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 0.0005608141036127721
Epoch 120/1000, Train Loss: 0.8960229754447937, Val Loss: 0.31200557947158813, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.0005465820674543388
Epoch 130/1000, Train Loss: 1.0620210766792297, Val Loss: 0.2838360369205475, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.0005323500312959054
Epoch 140/1000, Train Loss: 0.9656274914741516, Val Loss: 0.2879967987537384, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.0005181179951374721
Epoch 150/1000, Train Loss: 1.4223161339759827, Val Loss: 0.29853329062461853, Train Acc: 0.7, Val Acc: 0.8999999761581421, LR: 0.0005038859589790388
Epoch 160/1000, Train Loss: 0.9732833206653595, Val Loss: 0.31027719378471375, Train Acc: 0.7, Val Acc: 0.8999999761581421, LR: 0.0004896539228206055
Epoch 170/1000, Train Loss: 0.6279926002025604, Val Loss: 0.28841108083724976, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 0.0004754218866621721
Epoch 180/1000, Train Loss: 0.9881449937820435, Val Loss: 0.2779741585254669, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.0004611898505037387
Epoch 190/1000, Train Loss: 0.9979256987571716, Val Loss: 0.2921278476715088, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.0004469578143453054
Epoch 200/1000, Train Loss: 1.043062448501587, Val Loss: 0.29615849256515503, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.00043272577818687205
Epoch 210/1000, Train Loss: 0.8767960965633392, Val Loss: 0.28510868549346924, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.00041849374202843873
Epoch 220/1000, Train Loss: 0.6120091378688812, Val Loss: 0.2892628610134125, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 0.0004042617058700054
Epoch 230/1000, Train Loss: 0.9344040751457214, Val Loss: 0.26743367314338684, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 0.0003900296697115721
Epoch 240/1000, Train Loss: 0.8773273229598999, Val Loss: 0.2840119004249573, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 0.00037579763355313875
Epoch 250/1000, Train Loss: 0.7956814765930176, Val Loss: 0.27506083250045776, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 0.0003615655973947054
Epoch 260/1000, Train Loss: 0.8842589259147644, Val Loss: 0.2825356125831604, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 0.0003473335612362721
Epoch 270/1000, Train Loss: 0.9278584122657776, Val Loss: 0.297688364982605, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 0.0003331015250778388
Epoch 280/1000, Train Loss: 0.8958300948143005, Val Loss: 0.28624632954597473, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 0.00031886948891940545
Epoch 290/1000, Train Loss: 0.8402698040008545, Val Loss: 0.2740958631038666, Train Acc: 0.675, Val Acc: 0.8999999761581421, LR: 0.0003046374527609721
Epoch 300/1000, Train Loss: 0.9820564687252045, Val Loss: 0.2838001847267151, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.0002904054166025388
Epoch 310/1000, Train Loss: 0.7601687908172607, Val Loss: 0.291472852230072, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 0.00027617338044410547
Epoch 320/1000, Train Loss: 0.7962494492530823, Val Loss: 0.2745392620563507, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.00026194134428567215
Epoch 330/1000, Train Loss: 0.7301918566226959, Val Loss: 0.2679728865623474, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 0.0002477093081272388
Epoch 340/1000, Train Loss: 0.8364413380622864, Val Loss: 0.2669186592102051, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.00023347727196880547
Epoch 350/1000, Train Loss: 0.8195204436779022, Val Loss: 0.2722168564796448, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 0.00021924523581037212
Epoch 360/1000, Train Loss: 1.1351350247859955, Val Loss: 0.2685159146785736, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 0.00020501319965193863
Epoch 370/1000, Train Loss: 0.8423886001110077, Val Loss: 0.2723615765571594, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 0.00019078116349350525
Epoch 380/1000, Train Loss: 0.8349225521087646, Val Loss: 0.2735670804977417, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 0.0001765491273350719
Epoch 390/1000, Train Loss: 0.992801308631897, Val Loss: 0.25924283266067505, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 0.0001623170911766385
Epoch 400/1000, Train Loss: 1.012797087430954, Val Loss: 0.26352035999298096, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.0001480850550182051
Epoch 410/1000, Train Loss: 0.6757672727108002, Val Loss: 0.2698907256126404, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 0.00013385301885977176
Epoch 420/1000, Train Loss: 0.46312403678894043, Val Loss: 0.27209264039993286, Train Acc: 0.9, Val Acc: 0.8999999761581421, LR: 0.0001196209827013384
Epoch 430/1000, Train Loss: 0.9899050295352936, Val Loss: 0.2546268105506897, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.00010538894654290494
Epoch 440/1000, Train Loss: 0.6656603515148163, Val Loss: 0.2692159414291382, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 9.115691038447146e-05
Epoch 450/1000, Train Loss: 0.9456430971622467, Val Loss: 0.2695838510990143, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 7.692487422603801e-05
Epoch 460/1000, Train Loss: 0.6508595049381256, Val Loss: 0.2683067321777344, Train Acc: 0.9, Val Acc: 0.8999999761581421, LR: 6.269283806760455e-05
Epoch 470/1000, Train Loss: 0.8291483223438263, Val Loss: 0.26839637756347656, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.846080190917105e-05
Epoch 480/1000, Train Loss: 0.8046047389507294, Val Loss: 0.2638385593891144, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 3.422876575073757e-05
Epoch 490/1000, Train Loss: 0.7957872748374939, Val Loss: 0.2640961706638336, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 1.9996729592304062e-05
Epoch 500/1000, Train Loss: 0.899957686662674, Val Loss: 0.2634047865867615, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 510/1000, Train Loss: 0.7009349465370178, Val Loss: 0.2633194327354431, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 520/1000, Train Loss: 0.7166847884654999, Val Loss: 0.2638404965400696, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 530/1000, Train Loss: 0.7504660189151764, Val Loss: 0.2638019323348999, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 540/1000, Train Loss: 0.5224749892950058, Val Loss: 0.2640218734741211, Train Acc: 0.9, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 550/1000, Train Loss: 0.6326384842395782, Val Loss: 0.26373690366744995, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 560/1000, Train Loss: 0.8537033200263977, Val Loss: 0.2626565098762512, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 570/1000, Train Loss: 0.7866472601890564, Val Loss: 0.2622458040714264, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 580/1000, Train Loss: 0.6777139902114868, Val Loss: 0.2623915672302246, Train Acc: 0.9, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 590/1000, Train Loss: 0.7441169321537018, Val Loss: 0.26250725984573364, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 600/1000, Train Loss: 0.6582405418157578, Val Loss: 0.262251615524292, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 610/1000, Train Loss: 0.7423763275146484, Val Loss: 0.26178178191185, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 620/1000, Train Loss: 0.6888952255249023, Val Loss: 0.26157012581825256, Train Acc: 0.9, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 630/1000, Train Loss: 0.5672729015350342, Val Loss: 0.2615082859992981, Train Acc: 0.925, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 640/1000, Train Loss: 0.7810322642326355, Val Loss: 0.2615882456302643, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 650/1000, Train Loss: 0.9706459939479828, Val Loss: 0.26183658838272095, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 660/1000, Train Loss: 0.6710457801818848, Val Loss: 0.260903537273407, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 670/1000, Train Loss: 0.5993362069129944, Val Loss: 0.26104485988616943, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 680/1000, Train Loss: 0.8047987818717957, Val Loss: 0.26137658953666687, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 690/1000, Train Loss: 0.7843270003795624, Val Loss: 0.2610158622264862, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 700/1000, Train Loss: 0.8522418141365051, Val Loss: 0.2600327432155609, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 710/1000, Train Loss: 0.5668507516384125, Val Loss: 0.25990837812423706, Train Acc: 0.9, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 720/1000, Train Loss: 0.8960057497024536, Val Loss: 0.2601049542427063, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 730/1000, Train Loss: 0.7275865375995636, Val Loss: 0.25862982869148254, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 740/1000, Train Loss: 0.7616182863712311, Val Loss: 0.2577612102031708, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 750/1000, Train Loss: 0.9672245979309082, Val Loss: 0.2577992081642151, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 760/1000, Train Loss: 0.700959324836731, Val Loss: 0.2588427662849426, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 770/1000, Train Loss: 0.7789394855499268, Val Loss: 0.2594481408596039, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 780/1000, Train Loss: 0.7765386998653412, Val Loss: 0.25979188084602356, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 790/1000, Train Loss: 0.6837519407272339, Val Loss: 0.260310560464859, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 800/1000, Train Loss: 0.7871775925159454, Val Loss: 0.2601242661476135, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 810/1000, Train Loss: 0.7712383568286896, Val Loss: 0.25943976640701294, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 820/1000, Train Loss: 0.7201573848724365, Val Loss: 0.25945523381233215, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
[34m[1mwandb[39m[22m: [33mWARNING[39m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'Train Loss': 2.6127776503562927, 'Validation Loss': 0.7638857364654541, 'Train Accuracy': 0.575, 'Validation Accuracy': 0.0, '_timestamp': 1717440966.8314261}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.weight': {'_type': 'histogram', 'values': [3573, 4805, 4868, 4852, 4679, 4688, 4705, 4816, 4641, 4783, 4792, 4817, 4856, 4850, 4702, 4767, 4772, 4762, 4850, 4752, 4820, 4883, 4775, 4705, 4869, 4790, 4862, 4560, 4756, 4770, 4809, 4831, 4687, 4706, 4812, 4701, 4872, 4668, 4685, 4682, 4765, 4818, 4870, 4789, 4832, 4841, 4796, 4864, 4806, 4792, 4753, 4803, 4804, 4730, 4819, 4661, 4752, 4814, 4763, 4841, 4793, 4779, 4682, 1760], 'bins': [-0.09997526556253433, -0.09685090184211731, -0.09372653812170029, -0.09060217440128326, -0.08747781068086624, -0.08435343950986862, -0.0812290757894516, -0.07810471206903458, -0.07498034834861755, -0.07185598462820053, -0.06873162090778351, -0.06560725718736649, -0.062482889741659164, -0.05935852602124214, -0.05623416230082512, -0.053109798580408096, -0.049985431134700775, -0.04686106741428375, -0.04373670369386673, -0.04061233997344971, -0.037487976253032684, -0.03436360880732536, -0.03123924508690834, -0.028114881366491318, -0.024990517646074295, -0.021866152063012123, -0.0187417883425951, -0.015617422759532928, -0.012493059039115906, -0.009368694387376308, -0.006244329735636711, -0.003119965083897114, 4.3995678424835205e-06, 0.003128764219582081, 0.006253128871321678, 0.009377493523061275, 0.012501858174800873, 0.015626221895217896, 0.018750587478280067, 0.02187495119869709, 0.024999316781759262, 0.028123680502176285, 0.031248044222593307, 0.03437240794301033, 0.03749677538871765, 0.040621139109134674, 0.0437455028295517, 0.04686986654996872, 0.04999423027038574, 0.05311859771609306, 0.056242961436510086, 0.05936732515692711, 0.06249168887734413, 0.06561605632305145, 0.06874042004346848, 0.0718647837638855, 0.07498914748430252, 0.07811351120471954, 0.08123787492513657, 0.08436223864555359, 0.08748660981655121, 0.09061097353696823, 0.09373533725738525, 0.09685970097780228, 0.0999840646982193]}, '_timestamp': 1717440966.837883}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.bias': {'_type': 'histogram', 'values': [5, 6, 6, 5, 8, 6, 4, 5, 7, 5, 4, 6, 5, 6, 5, 6, 6, 3, 6, 4, 5, 5, 6, 6, 7, 9, 2, 7, 5, 2, 15, 10, 1, 3, 4, 4, 9, 2, 5, 3, 10, 6, 5, 7, 4, 3, 9, 7, 2, 3, 2, 4, 3, 4, 6, 8, 4, 6, 2, 7, 4, 3, 2, 6], 'bins': [-0.031078368425369263, -0.03009597770869732, -0.029113588854670525, -0.02813119813799858, -0.027148807421326637, -0.026166416704654694, -0.0251840278506279, -0.024201637133955956, -0.023219246417284012, -0.022236857563257217, -0.021254466846585274, -0.02027207612991333, -0.019289685413241386, -0.018307296559214592, -0.01732490584254265, -0.016342515125870705, -0.015360125340521336, -0.014377735555171967, -0.013395344838500023, -0.012412955053150654, -0.01143056433647871, -0.010448174551129341, -0.009465783834457397, -0.008483394049108028, -0.007501003798097372, -0.006518613547086716, -0.005536223296076059, -0.004553833045065403, -0.0035714430268853903, -0.002589052775874734, -0.0016066626412793994, -0.000624272448476404, 0.0003581177443265915, 0.0013405079953372478, 0.002322898246347904, 0.003305288264527917, 0.004287678748369217, 0.005270068533718586, 0.006252458784729242, 0.007234849035739899, 0.008217239752411842, 0.009199629537761211, 0.01018201932311058, 0.011164410039782524, 0.012146799825131893, 0.013129190541803837, 0.014111580327153206, 0.01509397104382515, 0.016076359897851944, 0.017058750614523888, 0.01804114133119583, 0.019023532047867775, 0.02000592090189457, 0.020988311618566513, 0.021970702335238457, 0.0229530930519104, 0.023935481905937195, 0.02491787262260914, 0.025900263339281082, 0.026882652193307877, 0.02786504290997982, 0.028847433626651764, 0.029829824343323708, 0.030812213197350502, 0.031794603914022446]}, '_timestamp': 1717440966.8381472}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.weight': {'_type': 'histogram', 'values': [159, 10, 15, 13, 6, 1, 3, 3, 3, 2, 1, 2, 0, 2, 2, 2, 4, 3, 2, 1, 0, 1, 0, 1, 5, 1, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 6, 3, 1, 2, 2, 2, 1, 0, 4, 5, 5, 2, 1, 2, 1, 2, 2, 1, 0, 1, 0, 2, 0, 3, 6, 5, 9, 18], 'bins': [0.9985622763633728, 0.9986072182655334, 0.9986521601676941, 0.99869704246521, 0.9987419843673706, 0.9987869262695312, 0.9988318681716919, 0.9988767504692078, 0.9989216923713684, 0.998966634273529, 0.9990115761756897, 0.9990564584732056, 0.9991014003753662, 0.9991463422775269, 0.9991912841796875, 0.9992361664772034, 0.999281108379364, 0.9993260502815247, 0.9993709921836853, 0.999415934085846, 0.9994608163833618, 0.9995057582855225, 0.9995507001876831, 0.9995956420898438, 0.9996405243873596, 0.9996854662895203, 0.9997304081916809, 0.9997753500938416, 0.9998202323913574, 0.9998651742935181, 0.9999101161956787, 0.9999550580978394, 1.0, 1.0000449419021606, 1.0000898838043213, 1.0001347064971924, 1.000179648399353, 1.0002245903015137, 1.0002695322036743, 1.000314474105835, 1.0003594160079956, 1.0004043579101562, 1.000449299812317, 1.000494122505188, 1.0005390644073486, 1.0005840063095093, 1.00062894821167, 1.0006738901138306, 1.0007188320159912, 1.0007637739181519, 1.0008087158203125, 1.0008536577224731, 1.0008984804153442, 1.0009434223175049, 1.0009883642196655, 1.0010333061218262, 1.0010782480239868, 1.0011231899261475, 1.001168131828308, 1.0012130737304688, 1.0012578964233398, 1.0013028383255005, 1.0013477802276611, 1.0013927221298218, 1.0014376640319824]}, '_timestamp': 1717440966.83835}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.bias': {'_type': 'histogram', 'values': [15, 3, 10, 7, 0, 16, 2, 1, 2, 2, 2, 1, 2, 4, 0, 1, 0, 2, 1, 2, 6, 1, 5, 2, 2, 4, 0, 3, 0, 0, 0, 131, 0, 0, 0, 0, 1, 4, 3, 4, 4, 2, 1, 6, 1, 2, 3, 2, 3, 1, 5, 1, 1, 2, 4, 1, 2, 1, 3, 2, 5, 6, 12, 26], 'bins': [-0.0014364236267283559, -0.00139151350595057, -0.0013466032687574625, -0.0013016931479796767, -0.001256783027201891, -0.0012118729064241052, -0.0011669627856463194, -0.0011220525484532118, -0.001077142427675426, -0.0010322323068976402, -0.0009873220697045326, -0.0009424119489267468, -0.0008975018281489611, -0.0008525917073711753, -0.0008076815283857286, -0.0007627713494002819, -0.0007178612286224961, -0.0006729511078447104, -0.0006280409288592637, -0.000583130749873817, -0.0005382206290960312, -0.0004933105083182454, -0.0004484003293327987, -0.0004034901794511825, -0.00035858002956956625, -0.00031366987968795, -0.0002687597298063338, -0.00022384957992471755, -0.0001789394300431013, -0.00013402928016148508, -8.911913027986884e-05, -4.4208980398252606e-05, 7.011694833636284e-07, 4.561131936497986e-05, 9.05214692465961e-05, 0.00013543161912821233, 0.00018034176900982857, 0.0002252519188914448, 0.00027016206877306104, 0.00031507221865467727, 0.0003599823685362935, 0.00040489251841790974, 0.000449802668299526, 0.0004947128472849727, 0.0005396229680627584, 0.0005845330888405442, 0.0006294432678259909, 0.0006743534468114376, 0.0007192635675892234, 0.0007641736883670092, 0.0008090838673524559, 0.0008539940463379025, 0.0008989041671156883, 0.0009438142878934741, 0.0009887244086712599, 0.0010336346458643675, 0.0010785447666421533, 0.001123454887419939, 0.0011683651246130466, 0.0012132752453908324, 0.0012581853661686182, 0.001303095486946404, 0.0013480056077241898, 0.0013929158449172974, 0.0014378259656950831]}, '_timestamp': 1717440966.838518}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.weight': {'_type': 'histogram', 'values': [885, 1747, 1734, 1742, 1842, 1750, 1831, 1742, 1763, 1745, 1809, 1749, 1802, 1780, 1727, 1741, 1727, 1735, 1828, 1750, 1705, 1842, 1799, 1844, 1694, 1800, 1751, 1797, 1860, 1722, 1712, 2037, 2049, 1827, 1761, 1791, 1827, 1798, 1759, 1837, 1789, 1749, 1777, 1809, 1811, 1725, 1767, 1823, 1742, 1794, 1763, 1778, 1782, 1716, 1800, 1764, 1783, 1681, 1771, 1751, 1767, 1736, 1822, 884], 'bins': [-0.1352052092552185, -0.13097986578941345, -0.1267545372247696, -0.12252919375896454, -0.11830385774374008, -0.11407851427793503, -0.10985317826271057, -0.10562784224748611, -0.10140249878168106, -0.0971771627664566, -0.09295182675123215, -0.0887264832854271, -0.08450114727020264, -0.08027581125497818, -0.07605046778917313, -0.07182513177394867, -0.06759978830814362, -0.06337445229291916, -0.0591491162776947, -0.05492377653717995, -0.05069843679666519, -0.046473100781440735, -0.04224776104092598, -0.038022421300411224, -0.03379708528518677, -0.029571745544672012, -0.025346405804157257, -0.02112106792628765, -0.016895730048418045, -0.01267039030790329, -0.008445052430033684, -0.004219713620841503, 5.62518835067749e-06, 0.004230963997542858, 0.008456302806735039, 0.012681640684604645, 0.0169069804251194, 0.021132318302989006, 0.025357656180858612, 0.029582995921373367, 0.03380833566188812, 0.03803367167711258, 0.042259011417627335, 0.04648435115814209, 0.05070968717336655, 0.0549350269138813, 0.05916036665439606, 0.06338570266962051, 0.06761103868484497, 0.07183638215065002, 0.07606171816587448, 0.08028706163167953, 0.08451239764690399, 0.08873773366212845, 0.0929630771279335, 0.09718841314315796, 0.10141374915838242, 0.10563909262418747, 0.10986442863941193, 0.11408976465463638, 0.11831510812044144, 0.1225404441356659, 0.12676578760147095, 0.1309911161661148, 0.13521645963191986]}, '_timestamp': 1717440966.839296}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.bias': {'_type': 'histogram', 'values': [1, 7, 5, 4, 5, 5, 6, 7, 4, 1, 4, 8, 8, 5, 4, 10, 4, 8, 3, 6, 6, 5, 9, 5, 7, 5, 5, 8, 4, 4, 1, 5, 9, 2, 3, 6, 3, 4, 7, 4, 9, 7, 5, 6, 6, 5, 6, 5, 5, 3, 5, 4, 5, 4, 3, 8, 3, 7, 3, 9, 4, 5, 4, 7], 'bins': [-0.05482891574501991, -0.053111761808395386, -0.05139460787177086, -0.04967745393514633, -0.047960296273231506, -0.04624314233660698, -0.04452598839998245, -0.042808834463357925, -0.0410916805267334, -0.03937452659010887, -0.037657372653484344, -0.03594021499156952, -0.03422306105494499, -0.032505907118320465, -0.030788753181695938, -0.02907159924507141, -0.027354445308446884, -0.025637289509177208, -0.02392013557255268, -0.022202981635928154, -0.020485825836658478, -0.01876867190003395, -0.017051517963409424, -0.015334363095462322, -0.01361720822751522, -0.011900054290890694, -0.010182899422943592, -0.008465745486319065, -0.0067485906183719635, -0.005031436216086149, -0.003314281813800335, -0.0015971274115145206, 0.00012002699077129364, 0.001837181393057108, 0.003554335795342922, 0.0052714901976287365, 0.006988644599914551, 0.008705799467861652, 0.01042295340448618, 0.012140108272433281, 0.013857262209057808, 0.01557441707700491, 0.01729157194495201, 0.019008725881576538, 0.020725879818201065, 0.02244303561747074, 0.024160189554095268, 0.025877343490719795, 0.02759449928998947, 0.029311653226614, 0.031028807163238525, 0.03274596109986305, 0.03446311503648758, 0.036180268973112106, 0.03789742663502693, 0.03961458057165146, 0.041331734508275986, 0.04304888844490051, 0.04476604238152504, 0.04648319631814957, 0.048200350254774094, 0.04991750791668892, 0.051634661853313446, 0.05335181578993797, 0.0550689697265625]}, '_timestamp': 1717440966.839458}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.weight': {'_type': 'histogram', 'values': [65, 19, 18, 21, 9, 5, 5, 7, 2, 2, 1, 1, 3, 0, 3, 3, 1, 2, 2, 4, 1, 3, 7, 3, 5, 3, 5, 3, 0, 0, 0, 0, 0, 0, 0, 0, 8, 5, 2, 4, 6, 7, 3, 2, 7, 1, 2, 4, 4, 2, 3, 5, 4, 0, 6, 1, 3, 3, 5, 2, 9, 9, 10, 15], 'bins': [0.9985622763633728, 0.9986072182655334, 0.9986521601676941, 0.99869704246521, 0.9987419843673706, 0.9987869262695312, 0.9988318681716919, 0.9988768100738525, 0.9989216923713684, 0.998966634273529, 0.9990115761756897, 0.9990565180778503, 0.999101459980011, 0.9991463422775269, 0.9991912841796875, 0.9992362260818481, 0.9992811679840088, 0.9993261098861694, 0.9993709921836853, 0.999415934085846, 0.9994608759880066, 0.9995058178901672, 0.9995507597923279, 0.9995956420898438, 0.9996405839920044, 0.999685525894165, 0.9997304677963257, 0.9997754096984863, 0.9998202919960022, 0.9998652338981628, 0.9999101758003235, 0.9999551177024841, 1.0, 1.0000449419021606, 1.0000898838043213, 1.000134825706482, 1.0001797676086426, 1.0002247095108032, 1.0002696514129639, 1.0003145933151245, 1.0003594160079956, 1.0004043579101562, 1.000449299812317, 1.0004942417144775, 1.0005391836166382, 1.0005841255187988, 1.0006290674209595, 1.0006740093231201, 1.0007189512252808, 1.0007638931274414, 1.0008087158203125, 1.0008536577224731, 1.0008985996246338, 1.0009435415267944, 1.000988483428955, 1.0010334253311157, 1.0010783672332764, 1.001123309135437, 1.0011682510375977, 1.0012131929397583, 1.0012580156326294, 1.00130295753479, 1.0013478994369507, 1.0013928413391113, 1.001437783241272]}, '_timestamp': 1717440966.839613}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.bias': {'_type': 'histogram', 'values': [32, 14, 12, 17, 9, 17, 5, 6, 3, 0, 1, 5, 3, 2, 5, 0, 3, 1, 3, 4, 2, 1, 2, 4, 5, 2, 2, 5, 0, 0, 0, 0, 14, 0, 0, 0, 5, 6, 7, 5, 4, 0, 3, 6, 3, 1, 4, 2, 3, 7, 2, 2, 6, 1, 1, 3, 2, 5, 18, 8, 12, 11, 5, 24], 'bins': [-0.0014377676416188478, -0.0013928498374298215, -0.0013479320332407951, -0.0013030142290517688, -0.0012580964248627424, -0.001213178620673716, -0.0011682608164846897, -0.0011233428958803415, -0.0010784250916913152, -0.0010335072875022888, -0.0009885894833132625, -0.0009436716791242361, -0.0008987538749352098, -0.0008538360707461834, -0.000808918266557157, -0.0007640004623681307, -0.0007190826581791043, -0.0006741647957824171, -0.0006292469915933907, -0.0005843291874043643, -0.000539411383215338, -0.0004944935790263116, -0.0004495757457334548, -0.00040465794154442847, -0.00035974010825157166, -0.0003148223040625453, -0.00026990449987351894, -0.00022498668113257736, -0.00018006886239163578, -0.00013515105820260942, -9.023323946166784e-05, -4.5315427996683866e-05, -3.9761653169989586e-07, 4.4520194933284074e-05, 8.943800639826804e-05, 0.00013435582513920963, 0.00017927362932823598, 0.00022419144806917757, 0.00026910926681011915, 0.0003140270709991455, 0.00035894487518817186, 0.0004038627084810287, 0.00044878051267005503, 0.0004936983459629118, 0.0005386161501519382, 0.0005835339543409646, 0.0006284517585299909, 0.0006733695627190173, 0.0007182874251157045, 0.0007632052293047309, 0.0008081230334937572, 0.0008530408376827836, 0.00089795864187181, 0.0009428764460608363, 0.0009877942502498627, 0.001032712054438889, 0.0010776298586279154, 0.0011225476628169417, 0.00116746558342129, 0.0012123833876103163, 0.0012573011917993426, 0.001302218995988369, 0.0013471368001773953, 0.0013920546043664217, 0.001436972408555448]}, '_timestamp': 1717440966.839775}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.weight': {'_type': 'histogram', 'values': [1169, 1786, 1745, 1743, 1781, 1854, 1878, 1804, 1764, 1736, 1801, 1721, 1763, 1804, 1829, 1738, 1783, 1759, 1833, 1757, 1753, 1760, 1742, 1808, 1790, 1750, 1796, 1826, 1766, 1789, 1751, 1765, 1849, 1780, 1726, 1717, 1763, 1780, 1826, 1662, 1774, 1762, 1723, 1839, 1750, 1801, 1723, 1882, 1718, 1712, 1833, 1792, 1734, 1731, 1768, 1766, 1752, 1762, 1748, 1724, 1811, 1797, 1743, 1133], 'bins': [-0.13517849147319794, -0.13095471262931824, -0.12673093378543854, -0.12250716239213943, -0.11828338354825974, -0.11405960470438004, -0.10983582586050034, -0.10561205446720123, -0.10138827562332153, -0.09716449677944183, -0.09294071793556213, -0.08871693909168243, -0.08449316769838333, -0.08026938885450363, -0.07604561001062393, -0.07182183116674423, -0.06759805977344513, -0.06337428092956543, -0.05915050208568573, -0.05492672324180603, -0.05070294812321663, -0.04647916927933693, -0.04225539416074753, -0.03803161531686783, -0.03380783647298813, -0.029584061354398727, -0.025360284373164177, -0.021136507391929626, -0.016912728548049927, -0.012688952498137951, -0.008465174585580826, -0.004241397604346275, -1.7620623111724854e-05, 0.004206156358122826, 0.008429933339357376, 0.012653711251914501, 0.016877487301826477, 0.021101266145706177, 0.025325043126940727, 0.029548820108175278, 0.03377259522676468, 0.03799637407064438, 0.04222015291452408, 0.04644392803311348, 0.05066770687699318, 0.05489148199558258, 0.05911526083946228, 0.06333903968334198, 0.06756281852722168, 0.07178658992052078, 0.07601036876440048, 0.08023414760828018, 0.08445792645215988, 0.08868169784545898, 0.09290547668933868, 0.09712925553321838, 0.10135303437709808, 0.10557681322097778, 0.10980058461427689, 0.11402436345815659, 0.11824814230203629, 0.12247192114591599, 0.1266956925392151, 0.1309194713830948, 0.1351432502269745]}, '_timestamp': 1717440966.840541}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.bias': {'_type': 'histogram', 'values': [5, 11, 2, 6, 8, 1, 5, 6, 3, 9, 8, 6, 5, 5, 4, 3, 5, 7, 10, 6, 4, 3, 6, 3, 1, 6, 2, 5, 4, 2, 5, 8, 3, 4, 6, 8, 5, 7, 5, 4, 2, 5, 4, 5, 6, 7, 3, 3, 6, 12, 4, 4, 12, 1, 5, 5, 6, 11, 4, 4, 4, 5, 6, 5], 'bins': [-0.05536353588104248, -0.05365010350942612, -0.05193667113780975, -0.05022324249148369, -0.048509810119867325, -0.04679637774825096, -0.0450829453766346, -0.043369513005018234, -0.04165608435869217, -0.039942651987075806, -0.03822921961545944, -0.03651578724384308, -0.034802354872226715, -0.03308892622590065, -0.031375493854284286, -0.029662061482667923, -0.02794862911105156, -0.026235198602080345, -0.02452176623046398, -0.022808335721492767, -0.021094903349876404, -0.01938147284090519, -0.017668040469288826, -0.015954608097672462, -0.014241177588701248, -0.012527745217084885, -0.010814313776791096, -0.009100882336497307, -0.007387450896203518, -0.005674018990248442, -0.003960587549954653, -0.0022471558768302202, -0.0005337242037057877, 0.001179707469418645, 0.0028931391425430775, 0.004606570582836866, 0.006320002488791943, 0.008033433929085732, 0.00974686536937952, 0.01146029680967331, 0.013173729181289673, 0.014887160621583462, 0.01660059206187725, 0.018314024433493614, 0.02002745494246483, 0.021740887314081192, 0.023454317823052406, 0.02516775019466877, 0.026881180703639984, 0.028594613075256348, 0.03030804544687271, 0.032021477818489075, 0.03373490646481514, 0.0354483388364315, 0.03716177120804787, 0.03887520357966423, 0.040588635951280594, 0.04230206459760666, 0.04401549696922302, 0.045728929340839386, 0.04744236171245575, 0.04915579408407211, 0.05086922273039818, 0.05258265510201454, 0.054296087473630905]}, '_timestamp': 1717440966.840693}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.weight': {'_type': 'histogram', 'values': [83, 15, 20, 17, 8, 3, 4, 1, 7, 3, 6, 4, 3, 4, 0, 1, 2, 1, 3, 5, 3, 2, 3, 1, 0, 1, 5, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 11, 7, 3, 4, 2, 5, 4, 4, 2, 4, 0, 1, 4, 4, 1, 4, 3, 4, 5, 5, 2, 0, 3, 8, 7, 8, 19], 'bins': [0.9985621571540833, 0.9986070990562439, 0.9986520409584045, 0.9986969232559204, 0.998741865158081, 0.9987868070602417, 0.9988317489624023, 0.998876690864563, 0.9989216327667236, 0.9989665150642395, 0.9990114569664001, 0.9990563988685608, 0.9991013407707214, 0.9991462826728821, 0.9991912245750427, 0.9992361068725586, 0.9992810487747192, 0.9993259906768799, 0.9993709325790405, 0.9994158744812012, 0.9994608163833618, 0.9995056986808777, 0.9995506405830383, 0.999595582485199, 0.9996405243873596, 0.9996854662895203, 0.9997304081916809, 0.9997752904891968, 0.9998202323913574, 0.9998651742935181, 0.9999101161956787, 0.9999550580978394, 1.0, 1.0000449419021606, 1.0000898838043213, 1.0001347064971924, 1.000179648399353, 1.0002245903015137, 1.0002695322036743, 1.000314474105835, 1.0003594160079956, 1.0004043579101562, 1.000449299812317, 1.0004942417144775, 1.0005391836166382, 1.0005841255187988, 1.0006290674209595, 1.0006738901138306, 1.0007188320159912, 1.0007637739181519, 1.0008087158203125, 1.0008536577224731, 1.0008985996246338, 1.0009435415267944, 1.000988483428955, 1.0010334253311157, 1.0010783672332764, 1.001123309135437, 1.0011682510375977, 1.0012130737304688, 1.0012580156326294, 1.00130295753479, 1.0013478994369507, 1.0013928413391113, 1.001437783241272]}, '_timestamp': 1717440966.840845}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.bias': {'_type': 'histogram', 'values': [41, 20, 6, 8, 4, 18, 4, 2, 4, 2, 3, 1, 1, 4, 1, 3, 5, 1, 3, 1, 3, 4, 3, 2, 3, 9, 2, 7, 0, 0, 0, 1, 0, 1, 0, 0, 3, 4, 8, 2, 12, 2, 0, 8, 2, 5, 5, 2, 2, 4, 1, 2, 2, 9, 1, 6, 2, 5, 15, 6, 7, 11, 6, 36], 'bins': [-0.0014377394691109657, -0.0013928086264058948, -0.0013478777837008238, -0.0013029469409957528, -0.00125801598187536, -0.001213085139170289, -0.001168154296465218, -0.001123223453760147, -0.0010782926110550761, -0.0010333617683500051, -0.0009884308092296124, -0.0009435000247322023, -0.0008985691238194704, -0.0008536382811143994, -0.0008087074384093285, -0.0007637765374965966, -0.0007188456947915256, -0.0006739148520864546, -0.0006289839511737227, -0.0005840531084686518, -0.0005391222657635808, -0.0004941913648508489, -0.00044926052214577794, -0.00040432967944070697, -0.00035939880763180554, -0.0003144679358229041, -0.00026953709311783314, -0.0002246062213089317, -0.0001796753640519455, -0.0001347445067949593, -8.981364226201549e-05, -4.4882781367050484e-05, 4.807952791452408e-08, 4.497894042287953e-05, 8.990980131784454e-05, 0.00013484066585078835, 0.00017977152310777456, 0.00022470238036476076, 0.0002696332521736622, 0.00031456409487873316, 0.0003594949666876346, 0.000404425838496536, 0.000449356681201607, 0.000494287523906678, 0.0005392184248194098, 0.0005841492675244808, 0.0006290801102295518, 0.0006740110111422837, 0.0007189418538473547, 0.0007638726965524256, 0.0008088035974651575, 0.0008537344401702285, 0.0008986652828752995, 0.0009435961837880313, 0.0009885269682854414, 0.0010334579274058342, 0.0010783887701109052, 0.0011233196128159761, 0.0011682504555210471, 0.001213181298226118, 0.001258112140931189, 0.0013030431000515819, 0.0013479739427566528, 0.0013929047854617238, 0.0014378356281667948]}, '_timestamp': 1717440966.840992}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.weight': {'_type': 'histogram', 'values': [7, 3, 4, 4, 7, 2, 8, 8, 3, 7, 1, 6, 4, 6, 8, 3, 7, 6, 7, 4, 3, 8, 5, 3, 4, 7, 5, 7, 2, 5, 6, 12, 5, 2, 3, 4, 3, 11, 11, 7, 5, 1, 6, 5, 4, 3, 6, 5, 3, 5, 2, 4, 11, 5, 4, 6, 4, 4, 8, 7, 4, 5, 4, 6], 'bins': [-0.18772703409194946, -0.1818583458662033, -0.17598967254161835, -0.1701209843158722, -0.16425231099128723, -0.15838362276554108, -0.15251494944095612, -0.14664626121520996, -0.140777587890625, -0.13490889966487885, -0.1290402114391327, -0.12317153811454773, -0.11730284988880157, -0.11143416911363602, -0.10556548833847046, -0.0996968075633049, -0.09382812678813934, -0.08795944601297379, -0.08209076523780823, -0.07622208446264267, -0.07035340368747711, -0.06448471546173096, -0.0586160346865654, -0.05274735391139984, -0.04687867313623428, -0.041009992361068726, -0.03514131158590317, -0.02927262708544731, -0.023403946310281754, -0.017535265535116196, -0.011666582897305489, -0.005797901190817356, 7.078051567077637e-05, 0.005939462222158909, 0.011808143928647041, 0.01767682656645775, 0.023545507341623306, 0.029414188116788864, 0.03528287261724472, 0.04115155339241028, 0.047020234167575836, 0.052888914942741394, 0.05875759571790695, 0.06462627649307251, 0.07049496471881866, 0.07636364549398422, 0.08223232626914978, 0.08810100704431534, 0.0939696878194809, 0.09983836859464645, 0.10570704936981201, 0.11157573014497757, 0.11744441092014313, 0.12331309914588928, 0.12918177247047424, 0.1350504606962204, 0.14091914892196655, 0.1467878222465515, 0.15265651047229767, 0.15852518379688263, 0.16439387202262878, 0.17026254534721375, 0.1761312335729599, 0.18199990689754486, 0.18786859512329102]}, '_timestamp': 1717440966.841147}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.bias': {'_type': 'histogram', 'values': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'bins': [-0.4485546946525574, -0.4329296946525574, -0.4173046946525574, -0.4016796946525574, -0.3860546946525574, -0.3704296946525574, -0.3548046946525574, -0.3391796946525574, -0.3235546946525574, -0.3079296946525574, -0.2923046946525574, -0.2766796946525574, -0.2610546946525574, -0.24542969465255737, -0.22980469465255737, -0.21417969465255737, -0.19855469465255737, -0.18292969465255737, -0.16730469465255737, -0.15167969465255737, -0.13605469465255737, -0.12042968720197678, -0.10480468720197678, -0.08917968720197678, -0.07355468720197678, -0.057929687201976776, -0.042304687201976776, -0.026679687201976776, -0.011054687201976776, 0.004570312798023224, 0.020195312798023224, 0.035820312798023224, 0.051445312798023224, 0.06707031279802322, 0.08269531279802322, 0.09832031279802322, 0.11394531279802322, 0.12957030534744263, 0.14519530534744263, 0.16082030534744263, 0.17644530534744263, 0.19207030534744263, 0.20769530534744263, 0.22332030534744263, 0.23894530534744263, 0.2545703053474426, 0.2701953053474426, 0.2858203053474426, 0.3014453053474426, 0.3170703053474426, 0.3326953053474426, 0.3483203053474426, 0.3639453053474426, 0.3795703053474426, 0.3951953053474426, 0.4108203053474426, 0.4264453053474426, 0.4420703053474426, 0.4576953053474426, 0.4733203053474426, 0.4889453053474426, 0.5045703053474426, 0.5201953053474426, 0.5358203053474426, 0.5514453053474426]}, '_timestamp': 1717440966.8413172}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TP Reward Distribution': {'_type': 'histogram', 'values': [10, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [2.6666666666666665, 5.503125, 8.339583333333334, 11.176041666666666, 14.0125, 16.848958333333332, 19.68541666666667, 22.521875, 25.358333333333334, 28.194791666666667, 31.03125, 33.86770833333333, 36.704166666666666, 39.540625, 42.37708333333333, 45.213541666666664, 48.05, 50.88645833333333, 53.72291666666666, 56.559374999999996, 59.39583333333333, 62.23229166666666, 65.06875, 67.90520833333333, 70.74166666666667, 73.578125, 76.41458333333334, 79.25104166666667, 82.0875, 84.92395833333333, 87.76041666666667, 90.596875, 93.43333333333334, 96.26979166666668, 99.10625, 101.94270833333334, 104.77916666666667, 107.61562500000001, 110.45208333333333, 113.28854166666667, 116.125, 118.96145833333334, 121.79791666666667, 124.634375, 127.47083333333333, 130.30729166666666, 133.14374999999998, 135.9802083333333, 138.81666666666666, 141.653125, 144.48958333333331, 147.32604166666667, 150.1625, 152.99895833333332, 155.83541666666665, 158.671875, 161.50833333333333, 164.34479166666665, 167.18124999999998, 170.01770833333333, 172.85416666666666, 175.69062499999998, 178.5270833333333, 181.36354166666666, 184.2]}, '_timestamp': 1717440966.841698}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TN Reward Distribution': {'_type': 'histogram', 'values': [4, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [4.8, 7.547916666666666, 10.295833333333333, 13.04375, 15.791666666666664, 18.539583333333333, 21.287499999999998, 24.035416666666666, 26.78333333333333, 29.531249999999996, 32.27916666666666, 35.02708333333333, 37.77499999999999, 40.52291666666666, 43.27083333333333, 46.01874999999999, 48.76666666666666, 51.51458333333333, 54.26249999999999, 57.01041666666666, 59.758333333333326, 62.50624999999999, 65.25416666666666, 68.00208333333333, 70.74999999999999, 73.49791666666665, 76.24583333333332, 78.99374999999999, 81.74166666666666, 84.48958333333331, 87.23749999999998, 89.98541666666665, 92.73333333333332, 95.48124999999999, 98.22916666666666, 100.97708333333333, 103.72499999999998, 106.47291666666665, 109.22083333333332, 111.96874999999999, 114.71666666666665, 117.46458333333332, 120.21249999999998, 122.96041666666665, 125.70833333333331, 128.45624999999998, 131.20416666666665, 133.95208333333332, 136.7, 139.44791666666666, 142.19583333333333, 144.94375, 147.69166666666666, 150.43958333333333, 153.1875, 155.93541666666667, 158.68333333333334, 161.43125, 164.17916666666665, 166.92708333333331, 169.67499999999998, 172.42291666666665, 175.17083333333332, 177.91875, 180.66666666666666]}, '_timestamp': 1717440966.8419}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FP Reward Distribution': {'_type': 'histogram', 'values': [2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [3.533333333333333, 3.8010416666666664, 4.06875, 4.336458333333333, 4.604166666666666, 4.871874999999999, 5.139583333333333, 5.407291666666667, 5.675, 5.942708333333333, 6.210416666666666, 6.478125, 6.745833333333334, 7.013541666666667, 7.28125, 7.548958333333333, 7.816666666666666, 8.084375, 8.352083333333333, 8.619791666666666, 8.8875, 9.155208333333334, 9.422916666666666, 9.690625, 9.958333333333332, 10.226041666666667, 10.493749999999999, 10.761458333333334, 11.029166666666667, 11.296875, 11.564583333333333, 11.832291666666666, 12.1, 12.367708333333333, 12.635416666666666, 12.903125, 13.170833333333333, 13.438541666666666, 13.706249999999999, 13.973958333333332, 14.241666666666665, 14.509375, 14.777083333333334, 15.044791666666667, 15.3125, 15.580208333333333, 15.847916666666666, 16.115625, 16.383333333333333, 16.651041666666664, 16.91875, 17.186458333333334, 17.454166666666666, 17.721874999999997, 17.989583333333332, 18.257291666666667, 18.525, 18.792708333333334, 19.06041666666667, 19.328125, 19.59583333333333, 19.863541666666663, 20.13125, 20.398958333333333, 20.666666666666668]}, '_timestamp': 1717440966.8420632}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FN Reward Distribution': {'_type': 'histogram', 'values': [2, 0, 1, 3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [2.6666666666666665, 3.0614583333333334, 3.45625, 3.8510416666666663, 4.245833333333334, 4.640625, 5.035416666666666, 5.430208333333333, 5.824999999999999, 6.219791666666666, 6.614583333333333, 7.009375, 7.404166666666667, 7.798958333333333, 8.19375, 8.588541666666666, 8.983333333333333, 9.378124999999999, 9.772916666666665, 10.167708333333334, 10.5625, 10.957291666666666, 11.352083333333333, 11.746875, 12.141666666666666, 12.536458333333332, 12.931249999999999, 13.326041666666665, 13.720833333333333, 14.115625, 14.510416666666666, 14.905208333333333, 15.299999999999999, 15.694791666666665, 16.089583333333334, 16.484375, 16.879166666666666, 17.273958333333333, 17.66875, 18.063541666666666, 18.458333333333332, 18.853125000000002, 19.24791666666667, 19.642708333333335, 20.0375, 20.432291666666668, 20.827083333333334, 21.221875, 21.616666666666667, 22.011458333333334, 22.40625, 22.801041666666666, 23.195833333333333, 23.590625, 23.985416666666666, 24.380208333333332, 24.775000000000002, 25.16979166666667, 25.564583333333335, 25.959375, 26.354166666666668, 26.748958333333334, 27.14375, 27.538541666666667, 27.933333333333334]}, '_timestamp': 1717440966.8422132}).
Epoch 830/1000, Train Loss: 0.7328695356845856, Val Loss: 0.2585570812225342, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 840/1000, Train Loss: 0.701203316450119, Val Loss: 0.2571154534816742, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 850/1000, Train Loss: 0.7162565737962723, Val Loss: 0.2566361427307129, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 860/1000, Train Loss: 0.5718505084514618, Val Loss: 0.25688451528549194, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 870/1000, Train Loss: 0.8364890217781067, Val Loss: 0.25633591413497925, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 880/1000, Train Loss: 0.7815094888210297, Val Loss: 0.25666654109954834, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 890/1000, Train Loss: 0.7008256316184998, Val Loss: 0.25681155920028687, Train Acc: 0.925, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 900/1000, Train Loss: 0.6672148704528809, Val Loss: 0.25726574659347534, Train Acc: 0.925, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 910/1000, Train Loss: 0.7167867124080658, Val Loss: 0.25731992721557617, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 920/1000, Train Loss: 0.7861465513706207, Val Loss: 0.2575107216835022, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 930/1000, Train Loss: 0.9732286930084229, Val Loss: 0.25747182965278625, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 940/1000, Train Loss: 0.7211211919784546, Val Loss: 0.25692397356033325, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 950/1000, Train Loss: 0.6064595431089401, Val Loss: 0.25675904750823975, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 960/1000, Train Loss: 0.6545917391777039, Val Loss: 0.2568921744823456, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 970/1000, Train Loss: 0.7820656001567841, Val Loss: 0.25690633058547974, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 980/1000, Train Loss: 0.6759690642356873, Val Loss: 0.25815442204475403, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
Epoch 990/1000, Train Loss: 0.842980295419693, Val Loss: 0.25839632749557495, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 7.187897049713904e-06
[32m[I 2024-06-03 14:56:26,637][39m Trial 0 finished with value: 0.14490383863449097 and parameters: {'hidden_size': 335, 'learning_rate': 0.000718789704971382, 'weight_decay': 0.00013259908970249677}. Best is trial 0 with value: 0.14490383863449097.