Epoch 0/1000, Train Loss: 2.6548032760620117, Val Loss: 0.5894827842712402, Train Acc: 0.55, Val Acc: 0.800000011920929, LR: 0.0006869363789790007
Epoch 10/1000, Train Loss: 2.2900789380073547, Val Loss: 0.5241591334342957, Train Acc: 0.6, Val Acc: 0.800000011920929, LR: 0.0006733080545929321
Epoch 20/1000, Train Loss: 1.7134943008422852, Val Loss: 0.5246749520301819, Train Acc: 0.55, Val Acc: 0.800000011920929, LR: 0.0006596797302068637
Epoch 30/1000, Train Loss: 1.4860951900482178, Val Loss: 0.510155200958252, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 0.0006460514058207949
Epoch 40/1000, Train Loss: 1.0388223826885223, Val Loss: 0.47649532556533813, Train Acc: 0.575, Val Acc: 0.800000011920929, LR: 0.0006324230814347263
Epoch 50/1000, Train Loss: 1.5497965216636658, Val Loss: 0.44912052154541016, Train Acc: 0.575, Val Acc: 0.800000011920929, LR: 0.0006187947570486576
Epoch 60/1000, Train Loss: 1.16743004322052, Val Loss: 0.4297863841056824, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 0.0006051664326625886
Epoch 70/1000, Train Loss: 1.110424280166626, Val Loss: 0.40846022963523865, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 0.0005915381082765195
Epoch 80/1000, Train Loss: 1.3900240361690521, Val Loss: 0.3962891697883606, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 0.0005779097838904507
Epoch 90/1000, Train Loss: 1.0548396706581116, Val Loss: 0.3942858576774597, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 0.0005642814595043822
Epoch 100/1000, Train Loss: 0.7852669507265091, Val Loss: 0.38768380880355835, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 0.0005506531351183134
Epoch 110/1000, Train Loss: 1.0080708861351013, Val Loss: 0.38871651887893677, Train Acc: 0.625, Val Acc: 0.800000011920929, LR: 0.0005370248107322447
Epoch 120/1000, Train Loss: 1.1812463402748108, Val Loss: 0.3766012191772461, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 0.000523396486346176
Epoch 130/1000, Train Loss: 0.8783828318119049, Val Loss: 0.38261932134628296, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 0.0005097681619601074
Epoch 140/1000, Train Loss: 0.8780710995197296, Val Loss: 0.3991827964782715, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 0.0004961398375740387
Epoch 150/1000, Train Loss: 0.8457880318164825, Val Loss: 0.3729475438594818, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 0.00048251151318797007
Epoch 160/1000, Train Loss: 0.6682463586330414, Val Loss: 0.4029912054538727, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 0.00046888318880190154
Epoch 170/1000, Train Loss: 0.7903019040822983, Val Loss: 0.37651175260543823, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 0.00045525486441583296
Epoch 180/1000, Train Loss: 0.6972092092037201, Val Loss: 0.3840676248073578, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 0.0004416265400297644
Epoch 190/1000, Train Loss: 0.6776387691497803, Val Loss: 0.3816677927970886, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 0.00042799821564369595
Epoch 200/1000, Train Loss: 0.7497248947620392, Val Loss: 0.38339030742645264, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 0.0004143698912576274
Epoch 210/1000, Train Loss: 0.6865820586681366, Val Loss: 0.38981854915618896, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 0.00040074156687155895
Epoch 220/1000, Train Loss: 0.7653493285179138, Val Loss: 0.3952966630458832, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 0.0003871132424854905
Epoch 230/1000, Train Loss: 0.6524257063865662, Val Loss: 0.39125627279281616, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 0.000373484918099422
Epoch 240/1000, Train Loss: 0.9015122354030609, Val Loss: 0.38473400473594666, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 0.00035985659371335347
Epoch 250/1000, Train Loss: 0.9836290776729584, Val Loss: 0.39715686440467834, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 0.000346228269327285
Epoch 260/1000, Train Loss: 0.764764815568924, Val Loss: 0.3940545916557312, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 0.0003325999449412165
Epoch 270/1000, Train Loss: 0.6500764489173889, Val Loss: 0.39636459946632385, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 0.00031897162055514805
Epoch 280/1000, Train Loss: 0.6861584484577179, Val Loss: 0.3964603841304779, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 0.00030534329616907957
Epoch 290/1000, Train Loss: 0.8535225689411163, Val Loss: 0.3941175043582916, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 0.0002917149717830111
Epoch 300/1000, Train Loss: 0.8312455117702484, Val Loss: 0.40205615758895874, Train Acc: 0.65, Val Acc: 0.800000011920929, LR: 0.0002780866473969426
Epoch 310/1000, Train Loss: 0.6808819770812988, Val Loss: 0.3942415714263916, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 0.00026445832301087415
Epoch 320/1000, Train Loss: 0.5315140932798386, Val Loss: 0.3872799277305603, Train Acc: 0.85, Val Acc: 0.800000011920929, LR: 0.00025082999862480567
Epoch 330/1000, Train Loss: 0.8051448464393616, Val Loss: 0.4036843180656433, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 0.00023720167423873706
Epoch 340/1000, Train Loss: 0.7787506282329559, Val Loss: 0.39687857031822205, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 0.00022357334985266842
Epoch 350/1000, Train Loss: 0.7535147666931152, Val Loss: 0.39673131704330444, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 0.00020994502546659973
Epoch 360/1000, Train Loss: 0.9737036824226379, Val Loss: 0.3931753635406494, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 0.000196316701080531
Epoch 370/1000, Train Loss: 0.7044690698385239, Val Loss: 0.39594417810440063, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 0.00018268837669446238
Epoch 380/1000, Train Loss: 0.6764654517173767, Val Loss: 0.38844090700149536, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 0.0001690600523083938
Epoch 390/1000, Train Loss: 0.6785187125205994, Val Loss: 0.38676247000694275, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 0.00015543172792232507
Epoch 400/1000, Train Loss: 0.5897798091173172, Val Loss: 0.38803109526634216, Train Acc: 0.85, Val Acc: 0.800000011920929, LR: 0.00014180340353625638
Epoch 410/1000, Train Loss: 0.7785888314247131, Val Loss: 0.39283663034439087, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 0.00012817507915018775
Epoch 420/1000, Train Loss: 0.7251028120517731, Val Loss: 0.39319783449172974, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 0.00011454675476411905
Epoch 430/1000, Train Loss: 0.6600407361984253, Val Loss: 0.38525205850601196, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 0.00010091843037805031
Epoch 440/1000, Train Loss: 0.669471949338913, Val Loss: 0.38716548681259155, Train Acc: 0.85, Val Acc: 0.800000011920929, LR: 8.729010599198156e-05
Epoch 450/1000, Train Loss: 0.7293778657913208, Val Loss: 0.3955308794975281, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 7.366178160591282e-05
Epoch 460/1000, Train Loss: 0.5614169538021088, Val Loss: 0.38243168592453003, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 6.003345721984408e-05
Epoch 470/1000, Train Loss: 0.6044248938560486, Val Loss: 0.38765761256217957, Train Acc: 0.85, Val Acc: 0.800000011920929, LR: 4.640513283377537e-05
Epoch 480/1000, Train Loss: 0.8714694678783417, Val Loss: 0.3948448896408081, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 3.277680844770665e-05
Epoch 490/1000, Train Loss: 0.6807478070259094, Val Loss: 0.3947121500968933, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 1.914848406163795e-05
Epoch 500/1000, Train Loss: 0.6684220880270004, Val Loss: 0.39332401752471924, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 510/1000, Train Loss: 0.6783466339111328, Val Loss: 0.39365822076797485, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 520/1000, Train Loss: 0.7814174592494965, Val Loss: 0.39371031522750854, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 530/1000, Train Loss: 0.5712352991104126, Val Loss: 0.3931363821029663, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 540/1000, Train Loss: 0.5310063660144806, Val Loss: 0.392520010471344, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 550/1000, Train Loss: 0.7498484551906586, Val Loss: 0.39226168394088745, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 560/1000, Train Loss: 0.6508381366729736, Val Loss: 0.3909538686275482, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 570/1000, Train Loss: 0.6720716655254364, Val Loss: 0.3898816704750061, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 580/1000, Train Loss: 0.681328684091568, Val Loss: 0.39023709297180176, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 590/1000, Train Loss: 0.6466029584407806, Val Loss: 0.39097529649734497, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 600/1000, Train Loss: 0.7007345259189606, Val Loss: 0.39108312129974365, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 610/1000, Train Loss: 0.5824094116687775, Val Loss: 0.38968029618263245, Train Acc: 0.85, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 620/1000, Train Loss: 0.6882654130458832, Val Loss: 0.3888024687767029, Train Acc: 0.9, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 630/1000, Train Loss: 0.7413457930088043, Val Loss: 0.3886120915412903, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 640/1000, Train Loss: 0.6350629776716232, Val Loss: 0.3882397413253784, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 650/1000, Train Loss: 0.5578280389308929, Val Loss: 0.3881097733974457, Train Acc: 0.875, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 660/1000, Train Loss: 0.7891681790351868, Val Loss: 0.38831135630607605, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 670/1000, Train Loss: 0.5519903898239136, Val Loss: 0.38904500007629395, Train Acc: 0.875, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 680/1000, Train Loss: 0.6652425825595856, Val Loss: 0.388777494430542, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
[34m[1mwandb[39m[22m: [33mWARNING[39m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'Train Loss': 2.6548032760620117, 'Validation Loss': 0.5894827842712402, 'Train Accuracy': 0.55, 'Validation Accuracy': 0.800000011920929, '_timestamp': 1717511830.940435}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.weight': {'_type': 'histogram', 'values': [2221, 5899, 5831, 5901, 5922, 5947, 5848, 6013, 5896, 5917, 5887, 5943, 6005, 5852, 6055, 5822, 5769, 5764, 5785, 5882, 6028, 5864, 5859, 5836, 5851, 5955, 6027, 5831, 6008, 6008, 5746, 5887, 5897, 5905, 5781, 5684, 5874, 6084, 5921, 6077, 5863, 5855, 5996, 5989, 5973, 5932, 5889, 6137, 5980, 6028, 6090, 6044, 5933, 5822, 5974, 6001, 5982, 5978, 5869, 5960, 5890, 5975, 6016, 2242], 'bins': [-0.09694993495941162, -0.09391987323760986, -0.0908898189663887, -0.08785975724458694, -0.08482970297336578, -0.08179964125156403, -0.07876957952976227, -0.07573952525854111, -0.07270946353673935, -0.06967940926551819, -0.06664934754371643, -0.06361928582191467, -0.060589227825403214, -0.057559169828891754, -0.054529111832380295, -0.051499053835868835, -0.04846899211406708, -0.04543893411755562, -0.04240887612104416, -0.0393788181245327, -0.03634876012802124, -0.03331869840621948, -0.030288640409708023, -0.027258582413196564, -0.024228524416685104, -0.021198464557528496, -0.018168406561017036, -0.015138346701860428, -0.012108288705348969, -0.009078229777514935, -0.006048170849680901, -0.0030181119218468666, 1.1947005987167358e-05, 0.0030420059338212013, 0.006072064861655235, 0.00910212378948927, 0.012132182717323303, 0.015162240713834763, 0.01819230057299137, 0.02122235856950283, 0.02425241842865944, 0.0272824764251709, 0.030312534421682358, 0.03334259241819382, 0.036372654139995575, 0.039402712136507034, 0.042432770133018494, 0.04546282812952995, 0.04849288612604141, 0.05152294784784317, 0.05455300584435463, 0.05758306384086609, 0.06061312183737755, 0.06364317983388901, 0.06667324155569077, 0.06970330327749252, 0.07273335754871368, 0.07576341927051544, 0.0787934735417366, 0.08182353526353836, 0.08485359698534012, 0.08788365125656128, 0.09091371297836304, 0.0939437672495842, 0.09697382897138596]}, '_timestamp': 1717511830.9434502}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.bias': {'_type': 'histogram', 'values': [5, 8, 9, 7, 5, 8, 1, 8, 6, 6, 4, 13, 5, 5, 7, 5, 5, 6, 5, 2, 8, 5, 2, 5, 2, 8, 7, 7, 4, 9, 13, 7, 15, 6, 2, 10, 5, 5, 5, 4, 5, 6, 9, 9, 13, 4, 3, 7, 7, 7, 6, 4, 10, 2, 8, 8, 7, 6, 6, 7, 9, 5, 9, 7], 'bins': [-0.03193264082074165, -0.030934948474168777, -0.0299372561275959, -0.028939565643668175, -0.0279418732970953, -0.026944180950522423, -0.025946490466594696, -0.02494879812002182, -0.023951105773448944, -0.022953413426876068, -0.021955721080303192, -0.020958030596375465, -0.01996033824980259, -0.018962645903229713, -0.017964955419301987, -0.01696726307272911, -0.015969570726156235, -0.014971878379583359, -0.013974186964333057, -0.012976495549082756, -0.01197880320250988, -0.010981110855937004, -0.009983419440686703, -0.008985728025436401, -0.007988035678863525, -0.006990343797951937, -0.005992651917040348, -0.004994960036128759, -0.003997268155217171, -0.002999576274305582, -0.0020018843933939934, -0.0010041925124824047, -6.50063157081604e-06, 0.0009911912493407726, 0.0019888831302523613, 0.00298657501116395, 0.003984266892075539, 0.004981958772987127, 0.005979650653898716, 0.006977342534810305, 0.007975034415721893, 0.00897272676229477, 0.00997041817754507, 0.010968109592795372, 0.011965801939368248, 0.012963494285941124, 0.013961185701191425, 0.014958877116441727, 0.015956569463014603, 0.01695426180958748, 0.017951954156160355, 0.01894964464008808, 0.019947336986660957, 0.020945029333233833, 0.02194271981716156, 0.022940412163734436, 0.023938104510307312, 0.024935796856880188, 0.025933489203453064, 0.02693117968738079, 0.027928872033953667, 0.028926564380526543, 0.02992425486445427, 0.030921947211027145, 0.03191963955760002]}, '_timestamp': 1717511830.943682}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.weight': {'_type': 'histogram', 'values': [254, 26, 23, 5, 4, 4, 3, 2, 1, 4, 0, 3, 3, 0, 1, 2, 1, 1, 0, 3, 0, 2, 2, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 6, 4, 1, 2, 1, 3, 0, 0, 2, 2, 1, 0, 0, 2, 0, 1, 2, 1, 1, 0, 1, 3, 3, 4, 2, 9], 'bins': [0.9986231327056885, 0.9986661076545715, 0.9987090826034546, 0.9987519979476929, 0.9987949728965759, 0.998837947845459, 0.998880922794342, 0.9989238381385803, 0.9989668130874634, 0.9990097880363464, 0.9990527629852295, 0.9990957379341125, 0.9991386532783508, 0.9991816282272339, 0.9992246031761169, 0.999267578125, 0.9993104934692383, 0.9993534684181213, 0.9993964433670044, 0.9994394183158875, 0.9994823932647705, 0.9995253086090088, 0.9995682835578918, 0.9996112585067749, 0.999654233455658, 0.999697208404541, 0.9997401237487793, 0.9997830986976624, 0.9998260736465454, 0.9998690485954285, 0.9999119639396667, 0.9999549388885498, 0.9999979138374329, 1.000040888786316, 1.0000838041305542, 1.000126838684082, 1.0001697540283203, 1.0002126693725586, 1.0002557039260864, 1.0002986192703247, 1.0003416538238525, 1.0003845691680908, 1.000427484512329, 1.000470519065857, 1.0005134344100952, 1.000556468963623, 1.0005993843078613, 1.0006422996520996, 1.0006853342056274, 1.0007282495498657, 1.0007712841033936, 1.0008141994476318, 1.0008571147918701, 1.000900149345398, 1.0009430646896362, 1.0009859800338745, 1.0010290145874023, 1.0010719299316406, 1.0011149644851685, 1.0011578798294067, 1.001200795173645, 1.0012438297271729, 1.0012867450714111, 1.001329779624939, 1.0013726949691772]}, '_timestamp': 1717511830.943864}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.bias': {'_type': 'histogram', 'values': [12, 5, 8, 8, 5, 56, 3, 2, 1, 3, 0, 2, 0, 2, 0, 3, 1, 1, 4, 2, 5, 1, 2, 5, 3, 4, 5, 3, 0, 0, 0, 124, 0, 0, 0, 0, 3, 3, 2, 2, 6, 3, 4, 7, 3, 1, 0, 2, 3, 3, 3, 4, 2, 1, 3, 3, 3, 2, 21, 6, 12, 5, 9, 27], 'bins': [-0.0013763827737420797, -0.0013333655660972, -0.001290348474867642, -0.001247331267222762, -0.0012043141759932041, -0.0011612969683483243, -0.0011182798771187663, -0.0010752626694738865, -0.0010322455782443285, -0.0009892283705994487, -0.0009462112793698907, -0.0009031941299326718, -0.0008601769804954529, -0.000817159831058234, -0.0007741426816210151, -0.0007311255321837962, -0.0006881083827465773, -0.0006450911751016974, -0.0006020740256644785, -0.0005590568762272596, -0.0005160397267900407, -0.0004730226064566523, -0.0004300054570194334, -0.0003869883075822145, -0.0003439711290411651, -0.0003009539796039462, -0.0002579368301667273, -0.00021491969528142363, -0.0001719025312922895, -0.0001288853818550706, -8.586823241785169e-05, -4.285108298063278e-05, 1.660664565861225e-07, 4.318321589380503e-05, 8.620036533102393e-05, 0.00012921751476824284, 0.00017223466420546174, 0.00021525182819459587, 0.00025826896307989955, 0.00030128611251711845, 0.00034430326195433736, 0.0003873204404953867, 0.0004303375899326056, 0.00047335473936982453, 0.000516371859703213, 0.0005593890091404319, 0.0006024061585776508, 0.0006454233080148697, 0.0006884405156597495, 0.0007314576650969684, 0.0007744748145341873, 0.0008174919639714062, 0.0008605091134086251, 0.000903526262845844, 0.0009465434122830629, 0.000989560503512621, 0.0010325777111575007, 0.0010755948023870587, 0.0011186120100319386, 0.0011616291012614965, 0.0012046463089063764, 0.0012476634001359344, 0.0012906806077808142, 0.0013336976990103722, 0.001376714906655252]}, '_timestamp': 1717511830.944026}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.weight': {'_type': 'histogram', 'values': [1239, 2786, 2737, 2719, 2683, 2738, 2562, 2740, 2653, 2660, 2669, 2742, 2601, 2694, 2707, 2669, 2636, 2641, 2677, 2648, 2775, 2713, 2711, 2698, 2757, 2694, 2750, 2626, 2673, 2640, 2758, 3146, 2944, 2758, 2712, 2692, 2712, 2664, 2731, 2715, 2739, 2632, 2718, 2695, 2697, 2761, 2628, 2736, 2796, 2751, 2653, 2826, 2683, 2678, 2609, 2774, 2708, 2694, 2795, 2710, 2777, 2584, 2673, 1182], 'bins': [-0.12186765670776367, -0.11805922538042068, -0.1142508015036583, -0.11044237017631531, -0.10663394629955292, -0.10282551497220993, -0.09901709109544754, -0.09520865976810455, -0.09140022844076157, -0.08759180456399918, -0.08378337323665619, -0.0799749493598938, -0.07616651803255081, -0.07235808670520782, -0.06854966282844543, -0.06474123150110245, -0.06093280762434006, -0.05712437629699707, -0.05331594869494438, -0.04950752109289169, -0.045699093490839005, -0.041890665888786316, -0.03808223828673363, -0.03427380695939064, -0.03046537935733795, -0.026656951755285263, -0.022848524153232574, -0.019040096551179886, -0.015231667086482048, -0.01142323948442936, -0.007614810951054096, -0.0038063828833401203, 2.045184373855591e-06, 0.0038104732520878315, 0.007618901319801807, 0.01142732985317707, 0.01523575745522976, 0.019044186919927597, 0.022852614521980286, 0.026661042124032974, 0.030469469726085663, 0.03427789732813835, 0.03808632865548134, 0.04189475625753403, 0.045703183859586716, 0.049511611461639404, 0.05332003906369209, 0.05712846666574478, 0.06093689799308777, 0.06474532186985016, 0.06855375319719315, 0.07236217707395554, 0.07617060840129852, 0.07997903972864151, 0.0837874636054039, 0.08759589493274689, 0.09140431880950928, 0.09521275013685226, 0.09902118146419525, 0.10282960534095764, 0.10663803666830063, 0.11044646054506302, 0.114254891872406, 0.1180633157491684, 0.12187174707651138]}, '_timestamp': 1717511830.945121}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.bias': {'_type': 'histogram', 'values': [2, 4, 7, 6, 4, 4, 7, 7, 10, 4, 10, 6, 6, 8, 8, 7, 7, 5, 5, 4, 2, 8, 6, 5, 4, 4, 7, 7, 6, 6, 11, 9, 5, 7, 8, 7, 6, 4, 3, 8, 5, 7, 9, 6, 9, 8, 7, 4, 6, 7, 9, 7, 4, 6, 5, 9, 5, 8, 13, 7, 10, 2, 11, 5], 'bins': [-0.049847107380628586, -0.04831307753920555, -0.046779047697782516, -0.04524501785635948, -0.04371098801493645, -0.04217695817351341, -0.04064292833209038, -0.03910889849066734, -0.03757486864924431, -0.036040838807821274, -0.03450680896639824, -0.0329727828502655, -0.03143875300884247, -0.029904721304774284, -0.02837069146335125, -0.026836661621928215, -0.02530263364315033, -0.023768603801727295, -0.02223457396030426, -0.020700544118881226, -0.01916651427745819, -0.017632484436035156, -0.01609845459461212, -0.014564424753189087, -0.013030394911766052, -0.011496366001665592, -0.009962336160242558, -0.008428306318819523, -0.006894276477396488, -0.005360247101634741, -0.003826217260211706, -0.002292187651619315, -0.0007581580430269241, 0.0007758716237731278, 0.002309901174157858, 0.0038439310155808926, 0.00537796039134264, 0.006911990232765675, 0.00844602007418871, 0.009980049915611744, 0.011514078825712204, 0.013048108667135239, 0.014582138508558273, 0.016116168349981308, 0.017650198191404343, 0.019184228032827377, 0.020718257874250412, 0.022252287715673447, 0.02378631755709648, 0.025320345535874367, 0.0268543753772974, 0.028388405218720436, 0.02992243506014347, 0.031456466764211655, 0.03299049288034439, 0.034524522721767426, 0.03605855256319046, 0.037592582404613495, 0.03912661224603653, 0.040660642087459564, 0.0421946719288826, 0.043728701770305634, 0.04526273161172867, 0.0467967614531517, 0.04833079129457474]}, '_timestamp': 1717511830.945282}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.weight': {'_type': 'histogram', 'values': [174, 38, 26, 21, 5, 5, 4, 4, 3, 2, 5, 3, 5, 4, 1, 3, 0, 3, 2, 1, 1, 1, 6, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 11, 1, 8, 6, 1, 3, 1, 4, 1, 4, 1, 2, 1, 3, 0, 4, 5, 2, 2, 3, 1, 2, 3, 0, 5, 5, 2, 9], 'bins': [0.9986231327056885, 0.9986661672592163, 0.9987092018127441, 0.998752236366272, 0.9987952709197998, 0.9988382458686829, 0.9988812804222107, 0.9989243149757385, 0.9989673495292664, 0.9990103840827942, 0.999053418636322, 0.9990964531898499, 0.9991394281387329, 0.9991824626922607, 0.9992254972457886, 0.9992685317993164, 0.9993115663528442, 0.9993546009063721, 0.9993976354598999, 0.9994406700134277, 0.9994837045669556, 0.9995266795158386, 0.9995697140693665, 0.9996127486228943, 0.9996557831764221, 0.99969881772995, 0.9997418522834778, 0.9997848868370056, 0.9998278617858887, 0.9998708963394165, 0.9999139308929443, 0.9999569654464722, 1.0, 1.0000430345535278, 1.0000860691070557, 1.0001291036605835, 1.0001721382141113, 1.0002151727676392, 1.000258207321167, 1.0003012418746948, 1.0003442764282227, 1.000387191772461, 1.0004302263259888, 1.0004732608795166, 1.0005162954330444, 1.0005593299865723, 1.0006023645401, 1.000645399093628, 1.0006884336471558, 1.0007314682006836, 1.0007745027542114, 1.0008175373077393, 1.000860571861267, 1.000903606414795, 1.0009466409683228, 1.0009896755218506, 1.0010325908660889, 1.0010756254196167, 1.0011186599731445, 1.0011616945266724, 1.0012047290802002, 1.001247763633728, 1.0012907981872559, 1.0013338327407837, 1.0013768672943115]}, '_timestamp': 1717511830.945441}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.bias': {'_type': 'histogram', 'values': [35, 17, 12, 12, 6, 29, 4, 6, 0, 4, 4, 2, 4, 2, 5, 3, 2, 3, 4, 2, 7, 3, 5, 5, 6, 4, 4, 5, 1, 0, 0, 14, 0, 0, 0, 0, 2, 6, 10, 3, 4, 2, 3, 4, 6, 2, 4, 7, 3, 4, 4, 1, 4, 3, 6, 5, 9, 6, 22, 1, 15, 17, 19, 31], 'bins': [-0.0013766420306637883, -0.0013336194679141045, -0.0012905970215797424, -0.0012475744588300586, -0.0012045520124956965, -0.0011615294497460127, -0.0011185070034116507, -0.0010754844406619668, -0.0010324619943276048, -0.000989439431577921, -0.000946416927035898, -0.000903394422493875, -0.0008603719179518521, -0.0008173494134098291, -0.0007743269088678062, -0.0007313044043257833, -0.0006882818997837603, -0.0006452593952417374, -0.0006022368906997144, -0.0005592143861576915, -0.0005161918816156685, -0.00047316934796981514, -0.0004301468434277922, -0.00038712433888576925, -0.0003441018343437463, -0.00030107932980172336, -0.0002580568252597004, -0.00021503430616576225, -0.0001720118016237393, -0.00012898929708171636, -8.59667852637358e-05, -4.294427708373405e-05, 7.82310962677002e-08, 4.310073927626945e-05, 8.61232474562712e-05, 0.00012914575927425176, 0.0001721682638162747, 0.00021519076835829765, 0.0002582132874522358, 0.00030123579199425876, 0.0003442582965362817, 0.00038728080107830465, 0.0004303033056203276, 0.00047332581016235054, 0.0005163483438082039, 0.0005593708483502269, 0.0006023933528922498, 0.0006454158574342728, 0.0006884383619762957, 0.0007314608665183187, 0.0007744833710603416, 0.0008175058756023645, 0.0008605283801443875, 0.0009035508846864104, 0.0009465733892284334, 0.0009895958937704563, 0.0010326184565201402, 0.0010756409028545022, 0.001118663465604186, 0.001161685911938548, 0.001204708474688232, 0.001247730921022594, 0.0012907534837722778, 0.0013337759301066399, 0.0013767984928563237]}, '_timestamp': 1717511830.94559}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.weight': {'_type': 'histogram', 'values': [1460, 2712, 2680, 2696, 2681, 2739, 2675, 2619, 2699, 2745, 2710, 2571, 2729, 2622, 2646, 2755, 2629, 2754, 2762, 2602, 2734, 2766, 2680, 2725, 2697, 2802, 2728, 2755, 2724, 2681, 2707, 2803, 2851, 2687, 2644, 2696, 2722, 2720, 2682, 2731, 2748, 2687, 2661, 2789, 2763, 2632, 2668, 2638, 2654, 2682, 2707, 2658, 2727, 2714, 2675, 2824, 2707, 2627, 2676, 2698, 2703, 2744, 2688, 1478], 'bins': [-0.12182217091321945, -0.11801442503929138, -0.11420667916536331, -0.11039893329143524, -0.10659118741750717, -0.1027834415435791, -0.09897569566965103, -0.09516795724630356, -0.09136021137237549, -0.08755246549844742, -0.08374471962451935, -0.07993697375059128, -0.07612922787666321, -0.07232148200273514, -0.06851373612880707, -0.064705990254879, -0.06089824438095093, -0.057090502232313156, -0.053282756358385086, -0.049475010484457016, -0.045667264610528946, -0.041859518736600876, -0.038051772862672806, -0.034244030714035034, -0.030436284840106964, -0.026628538966178894, -0.022820793092250824, -0.019013047218322754, -0.015205303207039833, -0.011397557333111763, -0.0075898123905062675, -0.003782066982239485, 2.5678426027297974e-05, 0.0038334238342940807, 0.0076411692425608635, 0.011448914185166359, 0.015256660059094429, 0.01906440407037735, 0.02287214994430542, 0.02667989581823349, 0.03048764169216156, 0.03429538756608963, 0.0381031297147274, 0.04191087558865547, 0.04571862146258354, 0.04952636733651161, 0.05333411321043968, 0.05714185908436775, 0.060949601233005524, 0.0647573471069336, 0.06856509298086166, 0.07237283885478973, 0.0761805847287178, 0.07998833060264587, 0.08379607647657394, 0.08760382235050201, 0.09141156822443008, 0.09521931409835815, 0.09902705252170563, 0.1028347983956337, 0.10664254426956177, 0.11045029014348984, 0.11425803601741791, 0.11806578189134598, 0.12187352776527405]}, '_timestamp': 1717511830.94669}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.bias': {'_type': 'histogram', 'values': [6, 5, 6, 5, 7, 5, 4, 11, 13, 6, 9, 5, 8, 2, 6, 7, 3, 5, 8, 8, 4, 3, 6, 4, 8, 5, 6, 6, 7, 13, 8, 6, 8, 5, 7, 7, 9, 6, 5, 6, 13, 14, 5, 5, 4, 6, 3, 7, 6, 3, 7, 5, 8, 9, 7, 4, 6, 5, 4, 7, 8, 10, 4, 5], 'bins': [-0.049488380551338196, -0.04794016480445862, -0.04639195278286934, -0.04484373703598976, -0.043295521289110184, -0.041747305542230606, -0.04019909352064133, -0.03865087777376175, -0.03710266202688217, -0.03555445000529289, -0.034006234258413315, -0.03245801851153374, -0.03090980462729931, -0.02936158888041973, -0.027813374996185303, -0.026265159249305725, -0.024716945365071297, -0.02316873148083687, -0.02162051573395729, -0.020072301849722862, -0.018524086102843285, -0.016975872218608856, -0.015427657403051853, -0.01387944258749485, -0.012331227771937847, -0.010783012956380844, -0.009234798140823841, -0.007686583790928125, -0.006138368975371122, -0.004590154159814119, -0.00304193957708776, -0.0014937248779460788, 5.448982119560242e-05, 0.0016027045203372836, 0.003150919219478965, 0.004699133802205324, 0.006247348617762327, 0.00779556343331933, 0.009343777783215046, 0.010891992598772049, 0.012440207414329052, 0.013988422229886055, 0.015536637045443058, 0.01708485186100006, 0.01863306574523449, 0.020181281492114067, 0.021729495376348495, 0.023277711123228073, 0.0248259250074625, 0.02637413889169693, 0.027922354638576508, 0.029470568522810936, 0.031018784269690514, 0.03256699815392494, 0.03411521390080452, 0.0356634296476841, 0.037211641669273376, 0.038759857416152954, 0.04030807316303253, 0.04185628518462181, 0.04340450093150139, 0.044952716678380966, 0.046500932425260544, 0.04804914444684982, 0.0495973601937294]}, '_timestamp': 1717511830.9468439}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.weight': {'_type': 'histogram', 'values': [171, 39, 44, 21, 9, 3, 7, 1, 4, 4, 0, 2, 4, 1, 3, 1, 0, 3, 3, 0, 2, 0, 2, 2, 4, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 8, 5, 5, 3, 4, 2, 4, 3, 2, 3, 1, 5, 1, 1, 2, 1, 0, 1, 2, 2, 2, 1, 5, 1, 4, 5, 3], 'bins': [0.9986231327056885, 0.9986661672592163, 0.9987091422080994, 0.9987521767616272, 0.998795211315155, 0.9988381862640381, 0.9988812208175659, 0.9989242553710938, 0.9989672899246216, 0.9990102648735046, 0.9990532994270325, 0.9990963339805603, 0.9991393089294434, 0.9991823434829712, 0.999225378036499, 0.9992683529853821, 0.9993113875389099, 0.9993544220924377, 0.9993973970413208, 0.9994404315948486, 0.9994834661483765, 0.9995264410972595, 0.9995694756507874, 0.9996125102043152, 0.9996554851531982, 0.9996985197067261, 0.9997415542602539, 0.9997845888137817, 0.9998275637626648, 0.9998705983161926, 0.9999136328697205, 0.9999566078186035, 0.9999996423721313, 1.0000426769256592, 1.000085711479187, 1.0001287460327148, 1.0001716613769531, 1.000214695930481, 1.0002577304840088, 1.0003007650375366, 1.0003437995910645, 1.0003868341445923, 1.0004297494888306, 1.0004727840423584, 1.0005158185958862, 1.000558853149414, 1.000601887702942, 1.0006449222564697, 1.000687837600708, 1.0007308721542358, 1.0007739067077637, 1.0008169412612915, 1.0008599758148193, 1.0009030103683472, 1.000946044921875, 1.0009889602661133, 1.0010319948196411, 1.001075029373169, 1.0011180639266968, 1.0011610984802246, 1.0012041330337524, 1.0012470483779907, 1.0012900829315186, 1.0013331174850464, 1.0013761520385742]}, '_timestamp': 1717511830.9470022}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.bias': {'_type': 'histogram', 'values': [44, 22, 15, 8, 7, 20, 7, 7, 2, 3, 4, 1, 3, 1, 3, 5, 2, 4, 5, 4, 7, 7, 5, 2, 6, 5, 4, 2, 0, 0, 0, 0, 1, 0, 0, 1, 8, 3, 5, 3, 4, 4, 2, 10, 6, 6, 5, 4, 5, 6, 1, 3, 1, 4, 3, 3, 3, 11, 23, 7, 14, 20, 12, 30], 'bins': [-0.0013768444769084454, -0.0013338187709450722, -0.001290793064981699, -0.0012477673590183258, -0.0012047416530549526, -0.0011617159470915794, -0.0011186902411282063, -0.001075664535164833, -0.0010326388292014599, -0.0009896131232380867, -0.0009465874172747135, -0.0009035617113113403, -0.0008605360053479671, -0.000817510299384594, -0.0007744845934212208, -0.0007314588874578476, -0.0006884331814944744, -0.0006454074755311012, -0.000602381769567728, -0.0005593560636043549, -0.0005163303576409817, -0.0004733046516776085, -0.0004302789457142353, -0.0003872532397508621, -0.00034422753378748894, -0.00030120182782411575, -0.00025817612186074257, -0.00021515041589736938, -0.0001721247099339962, -0.00012909900397062302, -8.607329800724983e-05, -4.304759204387665e-05, -2.1886080503463745e-08, 4.300381988286972e-05, 8.60295258462429e-05, 0.0001290552318096161, 0.00017208093777298927, 0.00021510664373636246, 0.00025813234969973564, 0.0003011580556631088, 0.000344183761626482, 0.0003872094675898552, 0.0004302351735532284, 0.00047326087951660156, 0.0005162865854799747, 0.0005593122914433479, 0.0006023379974067211, 0.0006453637033700943, 0.0006883894093334675, 0.0007314151152968407, 0.0007744408212602139, 0.000817466527223587, 0.0008604922331869602, 0.0009035179391503334, 0.0009465436451137066, 0.0009895693510770798, 0.001032595057040453, 0.0010756207630038261, 0.0011186464689671993, 0.0011616721749305725, 0.0012046978808939457, 0.0012477235868573189, 0.001290749292820692, 0.0013337749987840652, 0.0013768007047474384]}, '_timestamp': 1717511830.947148}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.weight': {'_type': 'histogram', 'values': [6, 10, 8, 8, 2, 4, 9, 8, 8, 10, 8, 8, 11, 4, 4, 7, 6, 8, 6, 8, 3, 3, 9, 3, 6, 6, 7, 5, 7, 7, 6, 4, 2, 3, 5, 10, 3, 8, 6, 6, 8, 7, 8, 7, 3, 6, 3, 9, 7, 7, 5, 7, 4, 4, 10, 5, 4, 6, 6, 9, 8, 10, 6, 12], 'bins': [-0.17123930156230927, -0.16591501235961914, -0.16059072315692902, -0.1552664339542389, -0.14994215965270996, -0.14461787045001984, -0.1392935812473297, -0.1339692920446396, -0.12864500284194946, -0.12332071363925934, -0.11799642443656921, -0.11267214268445969, -0.10734785348176956, -0.10202356427907944, -0.09669928252696991, -0.09137499332427979, -0.08605070412158966, -0.08072641491889954, -0.07540212571620941, -0.07007784396409988, -0.06475355476140976, -0.059429265558719635, -0.05410498008131981, -0.04878069460391998, -0.04345640540122986, -0.038132116198539734, -0.03280783072113991, -0.027483543381094933, -0.022159256041049957, -0.016834968701004982, -0.011510681360960007, -0.006186394020915031, -0.0008621066808700562, 0.004462180659174919, 0.009786467999219894, 0.01511075533926487, 0.020435042679309845, 0.02575933001935482, 0.031083617359399796, 0.03640790283679962, 0.041732192039489746, 0.04705648124217987, 0.0523807667195797, 0.05770505219697952, 0.06302934139966965, 0.06835363060235977, 0.0736779123544693, 0.07900220155715942, 0.08432649075984955, 0.08965077996253967, 0.0949750691652298, 0.10029935091733932, 0.10562364012002945, 0.11094792932271957, 0.1162722110748291, 0.12159650027751923, 0.12692078948020935, 0.13224507868289948, 0.1375693678855896, 0.14289365708827972, 0.14821794629096985, 0.15354222059249878, 0.1588665097951889, 0.16419079899787903, 0.16951508820056915]}, '_timestamp': 1717511830.9473028}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.bias': {'_type': 'histogram', 'values': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'bins': [-0.45560795068740845, -0.43998295068740845, -0.42435795068740845, -0.40873295068740845, -0.39310795068740845, -0.37748295068740845, -0.36185795068740845, -0.34623295068740845, -0.33060795068740845, -0.31498295068740845, -0.29935795068740845, -0.28373295068740845, -0.26810795068740845, -0.25248295068740845, -0.23685795068740845, -0.22123295068740845, -0.20560795068740845, -0.18998295068740845, -0.17435795068740845, -0.15873295068740845, -0.14310795068740845, -0.12748295068740845, -0.11185795068740845, -0.09623295068740845, -0.08060795068740845, -0.06498295068740845, -0.04935794696211815, -0.03373294696211815, -0.01810794696211815, -0.002482946962118149, 0.013142053037881851, 0.02876705303788185, 0.04439205303788185, 0.06001705303788185, 0.07564204931259155, 0.09126704931259155, 0.10689204931259155, 0.12251704931259155, 0.13814204931259155, 0.15376704931259155, 0.16939204931259155, 0.18501704931259155, 0.20064204931259155, 0.21626704931259155, 0.23189204931259155, 0.24751704931259155, 0.26314204931259155, 0.27876704931259155, 0.29439204931259155, 0.31001704931259155, 0.32564204931259155, 0.34126704931259155, 0.35689204931259155, 0.37251704931259155, 0.38814204931259155, 0.40376704931259155, 0.41939204931259155, 0.43501704931259155, 0.45064204931259155, 0.46626704931259155, 0.48189204931259155, 0.49751704931259155, 0.5131420493125916, 0.5287670493125916, 0.5443920493125916]}, '_timestamp': 1717511830.9474702}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TP Reward Distribution': {'_type': 'histogram', 'values': [1, 1, 1, 1, 0, 3, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [2.6666666666666665, 3.048958333333333, 3.43125, 3.8135416666666666, 4.195833333333333, 4.578125, 4.960416666666666, 5.3427083333333325, 5.725, 6.107291666666667, 6.489583333333333, 6.871874999999999, 7.254166666666666, 7.636458333333334, 8.018749999999999, 8.401041666666666, 8.783333333333333, 9.165624999999999, 9.547916666666666, 9.930208333333333, 10.3125, 10.694791666666665, 11.077083333333333, 11.459375, 11.841666666666665, 12.223958333333332, 12.60625, 12.988541666666665, 13.370833333333332, 13.753124999999999, 14.135416666666666, 14.517708333333331, 14.899999999999999, 15.282291666666666, 15.664583333333331, 16.046875, 16.429166666666667, 16.811458333333334, 17.193749999999998, 17.576041666666665, 17.958333333333332, 18.340625, 18.722916666666666, 19.105208333333334, 19.4875, 19.869791666666668, 20.252083333333335, 20.634375, 21.016666666666666, 21.398958333333333, 21.78125, 22.163541666666667, 22.545833333333334, 22.928125, 23.310416666666665, 23.692708333333332, 24.075, 24.457291666666666, 24.839583333333334, 25.221875, 25.604166666666668, 25.98645833333333, 26.36875, 26.751041666666666, 27.133333333333333]}, '_timestamp': 1717511830.947637}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TN Reward Distribution': {'_type': 'histogram', 'values': [5, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [3.3333333333333335, 7.96875, 12.604166666666668, 17.239583333333332, 21.875, 26.510416666666668, 31.145833333333332, 35.78125000000001, 40.41666666666667, 45.052083333333336, 49.68750000000001, 54.32291666666667, 58.958333333333336, 63.59375000000001, 68.22916666666667, 72.86458333333333, 77.5, 82.13541666666667, 86.77083333333333, 91.40625, 96.04166666666667, 100.67708333333333, 105.3125, 109.94791666666667, 114.58333333333333, 119.21875, 123.85416666666667, 128.48958333333334, 133.12500000000003, 137.76041666666669, 142.39583333333334, 147.03125000000003, 151.66666666666669, 156.30208333333334, 160.93750000000003, 165.57291666666669, 170.20833333333334, 174.84375000000003, 179.47916666666669, 184.11458333333334, 188.75000000000003, 193.38541666666669, 198.02083333333334, 202.65625000000003, 207.29166666666669, 211.92708333333334, 216.56250000000003, 221.19791666666669, 225.83333333333334, 230.46875000000003, 235.10416666666669, 239.73958333333337, 244.37500000000003, 249.01041666666669, 253.64583333333337, 258.28125, 262.9166666666667, 267.5520833333333, 272.1875, 276.8229166666667, 281.4583333333333, 286.09375, 290.7291666666667, 295.3645833333333, 300.0]}, '_timestamp': 1717511830.9477792}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FP Reward Distribution': {'_type': 'histogram', 'values': [1, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [2.6666666666666665, 2.9479166666666665, 3.2291666666666665, 3.5104166666666665, 3.7916666666666665, 4.072916666666666, 4.354166666666666, 4.635416666666666, 4.916666666666666, 5.197916666666666, 5.479166666666666, 5.760416666666666, 6.041666666666666, 6.322916666666666, 6.604166666666666, 6.885416666666666, 7.166666666666666, 7.447916666666666, 7.729166666666666, 8.010416666666666, 8.291666666666666, 8.572916666666666, 8.854166666666666, 9.135416666666666, 9.416666666666666, 9.697916666666666, 9.979166666666666, 10.260416666666666, 10.541666666666666, 10.822916666666666, 11.104166666666666, 11.385416666666666, 11.666666666666666, 11.947916666666666, 12.229166666666666, 12.510416666666666, 12.791666666666666, 13.072916666666666, 13.354166666666666, 13.635416666666666, 13.916666666666666, 14.197916666666666, 14.479166666666666, 14.760416666666666, 15.041666666666666, 15.322916666666666, 15.604166666666666, 15.885416666666666, 16.166666666666668, 16.447916666666668, 16.729166666666668, 17.010416666666668, 17.291666666666668, 17.572916666666668, 17.854166666666668, 18.135416666666668, 18.416666666666668, 18.697916666666668, 18.979166666666668, 19.260416666666668, 19.541666666666668, 19.822916666666668, 20.104166666666668, 20.385416666666668, 20.666666666666668]}, '_timestamp': 1717511830.9479191}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FN Reward Distribution': {'_type': 'histogram', 'values': [3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [3.3333333333333335, 4.93125, 6.529166666666667, 8.127083333333333, 9.725, 11.322916666666666, 12.920833333333334, 14.51875, 16.116666666666667, 17.714583333333334, 19.3125, 20.910416666666666, 22.508333333333333, 24.10625, 25.704166666666666, 27.302083333333332, 28.9, 30.497916666666665, 32.09583333333333, 33.69375, 35.291666666666664, 36.889583333333334, 38.487500000000004, 40.08541666666667, 41.68333333333334, 43.28125, 44.87916666666667, 46.47708333333333, 48.075, 49.672916666666666, 51.270833333333336, 52.86875, 54.46666666666667, 56.06458333333334, 57.6625, 59.26041666666667, 60.858333333333334, 62.456250000000004, 64.05416666666666, 65.65208333333334, 67.25, 68.84791666666666, 70.44583333333333, 72.04374999999999, 73.64166666666667, 75.23958333333333, 76.83749999999999, 78.43541666666665, 80.03333333333333, 81.63125, 83.22916666666666, 84.82708333333333, 86.425, 88.02291666666666, 89.62083333333332, 91.21875, 92.81666666666666, 94.41458333333333, 96.01249999999999, 97.61041666666667, 99.20833333333333, 100.80624999999999, 102.40416666666665, 104.00208333333333, 105.6]}, '_timestamp': 1717511830.948053}).
Epoch 690/1000, Train Loss: 0.8048592209815979, Val Loss: 0.38963496685028076, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 700/1000, Train Loss: 0.8622627556324005, Val Loss: 0.39004167914390564, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 710/1000, Train Loss: 0.6344445943832397, Val Loss: 0.38878726959228516, Train Acc: 0.875, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 720/1000, Train Loss: 0.7614547610282898, Val Loss: 0.3888269364833832, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 730/1000, Train Loss: 0.7335151731967926, Val Loss: 0.38888460397720337, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 740/1000, Train Loss: 0.7625204026699066, Val Loss: 0.3893839120864868, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 750/1000, Train Loss: 0.7159042656421661, Val Loss: 0.3902762532234192, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 760/1000, Train Loss: 0.6482031345367432, Val Loss: 0.39033275842666626, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 770/1000, Train Loss: 0.6548172533512115, Val Loss: 0.3909842371940613, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 780/1000, Train Loss: 0.5076458007097244, Val Loss: 0.3911758065223694, Train Acc: 0.925, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 790/1000, Train Loss: 0.6931783556938171, Val Loss: 0.3917572498321533, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 800/1000, Train Loss: 0.6674177944660187, Val Loss: 0.3909395635128021, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 810/1000, Train Loss: 0.5709247589111328, Val Loss: 0.39117228984832764, Train Acc: 0.875, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 820/1000, Train Loss: 0.7757863104343414, Val Loss: 0.3917084336280823, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 830/1000, Train Loss: 0.5960864275693893, Val Loss: 0.39152535796165466, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 840/1000, Train Loss: 0.6661438345909119, Val Loss: 0.390907347202301, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 850/1000, Train Loss: 0.7410233914852142, Val Loss: 0.3919863700866699, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 860/1000, Train Loss: 0.80354443192482, Val Loss: 0.39180028438568115, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 870/1000, Train Loss: 0.818538248538971, Val Loss: 0.39158958196640015, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 880/1000, Train Loss: 0.7939775288105011, Val Loss: 0.39195147156715393, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 890/1000, Train Loss: 0.8304140567779541, Val Loss: 0.39160221815109253, Train Acc: 0.65, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 900/1000, Train Loss: 0.7912816405296326, Val Loss: 0.39111676812171936, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 910/1000, Train Loss: 0.5581198334693909, Val Loss: 0.39179471135139465, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 920/1000, Train Loss: 0.6139668077230453, Val Loss: 0.39189445972442627, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 930/1000, Train Loss: 0.6695793271064758, Val Loss: 0.3918415606021881, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 940/1000, Train Loss: 0.7241289913654327, Val Loss: 0.39205968379974365, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 950/1000, Train Loss: 0.8843565881252289, Val Loss: 0.3921341001987457, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 960/1000, Train Loss: 0.6153680086135864, Val Loss: 0.3925844728946686, Train Acc: 0.875, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 970/1000, Train Loss: 0.6280870884656906, Val Loss: 0.3923155963420868, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 980/1000, Train Loss: 0.6467393934726715, Val Loss: 0.39345669746398926, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
Epoch 990/1000, Train Loss: 0.8250807523727417, Val Loss: 0.39265260100364685, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 6.8829921141761235e-06
[33m[W 2024-06-04 10:37:34,527][39m Trial 0 failed with parameters: {'hidden_size': 413, 'learning_rate': 0.0006882992114176076, 'weight_decay': 0.0007003030754512387} because of the following error: The value None could not be cast to float..
[33m[W 2024-06-04 10:37:34,528][39m Trial 0 failed with value None.