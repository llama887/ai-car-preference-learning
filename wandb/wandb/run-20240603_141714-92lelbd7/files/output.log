Epoch 0/1000, Train Loss: 2.915551543235779, Val Loss: 0.7001800537109375, Train Acc: 0.5, Val Acc: 0.6000000238418579, LR: 0.0004287594260002101
Epoch 10/1000, Train Loss: 2.7863270044326782, Val Loss: 0.5733785629272461, Train Acc: 0.5, Val Acc: 0.8999999761581421, LR: 0.0004202531469328526
Epoch 20/1000, Train Loss: 1.4748428463935852, Val Loss: 0.5454829931259155, Train Acc: 0.675, Val Acc: 1.0, LR: 0.000411746867865495
Epoch 30/1000, Train Loss: 1.5618830919265747, Val Loss: 0.5230691432952881, Train Acc: 0.5, Val Acc: 0.8999999761581421, LR: 0.0004032405887981374
Epoch 40/1000, Train Loss: 1.3662687540054321, Val Loss: 0.4870643615722656, Train Acc: 0.55, Val Acc: 0.8999999761581421, LR: 0.0003947343097307797
Epoch 50/1000, Train Loss: 2.4850047826766968, Val Loss: 0.4585390090942383, Train Acc: 0.5, Val Acc: 0.8999999761581421, LR: 0.00038622803066342207
Epoch 60/1000, Train Loss: 1.7065733671188354, Val Loss: 0.4295150339603424, Train Acc: 0.475, Val Acc: 0.8999999761581421, LR: 0.00037772175159606444
Epoch 70/1000, Train Loss: 1.7319238781929016, Val Loss: 0.45293498039245605, Train Acc: 0.5, Val Acc: 0.8999999761581421, LR: 0.0003692154725287068
Epoch 80/1000, Train Loss: 1.642593502998352, Val Loss: 0.41483139991760254, Train Acc: 0.55, Val Acc: 0.8999999761581421, LR: 0.00036070919346134917
Epoch 90/1000, Train Loss: 1.3993170261383057, Val Loss: 0.39402657747268677, Train Acc: 0.6, Val Acc: 0.8999999761581421, LR: 0.00035220291439399154
Epoch 100/1000, Train Loss: 1.321510910987854, Val Loss: 0.369089275598526, Train Acc: 0.625, Val Acc: 0.8999999761581421, LR: 0.0003436966353266339
Epoch 110/1000, Train Loss: 1.5981979370117188, Val Loss: 0.33968350291252136, Train Acc: 0.55, Val Acc: 0.8999999761581421, LR: 0.0003351903562592763
Epoch 120/1000, Train Loss: 1.1285305619239807, Val Loss: 0.34528857469558716, Train Acc: 0.675, Val Acc: 0.8999999761581421, LR: 0.0003266840771919186
Epoch 130/1000, Train Loss: 0.8968794941902161, Val Loss: 0.31948649883270264, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.0003181777981245609
Epoch 140/1000, Train Loss: 1.1976436376571655, Val Loss: 0.3200693726539612, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.0003096715190572032
Epoch 150/1000, Train Loss: 1.3839370012283325, Val Loss: 0.32137972116470337, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.00030116523998984554
Epoch 160/1000, Train Loss: 1.1602283120155334, Val Loss: 0.31310638785362244, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 0.00029265896092248785
Epoch 170/1000, Train Loss: 1.3938711881637573, Val Loss: 0.28892141580581665, Train Acc: 0.675, Val Acc: 0.8999999761581421, LR: 0.00028415268185513016
Epoch 180/1000, Train Loss: 0.8511540591716766, Val Loss: 0.32256823778152466, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 0.0002756464027877725
Epoch 190/1000, Train Loss: 0.7464031279087067, Val Loss: 0.3152182996273041, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 0.0002671401237204148
Epoch 200/1000, Train Loss: 0.8610334992408752, Val Loss: 0.29181864857673645, Train Acc: 0.9, Val Acc: 0.8999999761581421, LR: 0.0002586338446530571
Epoch 210/1000, Train Loss: 0.9946928024291992, Val Loss: 0.27523699402809143, Train Acc: 0.7, Val Acc: 0.8999999761581421, LR: 0.0002501275655856994
Epoch 220/1000, Train Loss: 1.4672616124153137, Val Loss: 0.27842721343040466, Train Acc: 0.675, Val Acc: 0.8999999761581421, LR: 0.00024162128651834184
Epoch 230/1000, Train Loss: 0.9443500339984894, Val Loss: 0.29336971044540405, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.0002331150074509844
Epoch 240/1000, Train Loss: 0.8098130524158478, Val Loss: 0.30400699377059937, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 0.00022460872838362696
Epoch 250/1000, Train Loss: 1.0996394753456116, Val Loss: 0.29664286971092224, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 0.0002161024493162695
Epoch 260/1000, Train Loss: 1.0287712812423706, Val Loss: 0.2963513135910034, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 0.0002075961702489121
Epoch 270/1000, Train Loss: 0.7816774547100067, Val Loss: 0.27326175570487976, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.00019908989118155466
Epoch 280/1000, Train Loss: 0.7083452343940735, Val Loss: 0.2836119532585144, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 0.0001905836121141973
Epoch 290/1000, Train Loss: 1.0378077924251556, Val Loss: 0.2941768169403076, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.00018207733304683986
Epoch 300/1000, Train Loss: 0.7280041575431824, Val Loss: 0.2812991738319397, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.0001735710539794824
Epoch 310/1000, Train Loss: 0.7800836563110352, Val Loss: 0.2830207049846649, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 0.0001650647749121249
Epoch 320/1000, Train Loss: 0.9664523601531982, Val Loss: 0.28315794467926025, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.00015655849584476748
Epoch 330/1000, Train Loss: 0.6912986934185028, Val Loss: 0.27064651250839233, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.00014805221677741007
Epoch 340/1000, Train Loss: 1.0929099321365356, Val Loss: 0.2692457139492035, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.00013954593771005268
Epoch 350/1000, Train Loss: 1.0240717828273773, Val Loss: 0.28646573424339294, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.00013103965864269526
Epoch 360/1000, Train Loss: 1.1465742588043213, Val Loss: 0.3012664318084717, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.0001225333795753378
Epoch 370/1000, Train Loss: 0.8514918982982635, Val Loss: 0.26799649000167847, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.00011402710050798034
Epoch 380/1000, Train Loss: 0.8977012038230896, Val Loss: 0.2762702703475952, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.00010552082144062291
Epoch 390/1000, Train Loss: 1.150009036064148, Val Loss: 0.28769296407699585, Train Acc: 0.7, Val Acc: 0.8999999761581421, LR: 9.70145423732655e-05
Epoch 400/1000, Train Loss: 1.1970264315605164, Val Loss: 0.2801392674446106, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 8.850826330590808e-05
Epoch 410/1000, Train Loss: 1.0603509545326233, Val Loss: 0.27002397179603577, Train Acc: 0.7, Val Acc: 0.8999999761581421, LR: 8.000198423855067e-05
Epoch 420/1000, Train Loss: 0.9714377522468567, Val Loss: 0.2730640769004822, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 7.149570517119325e-05
Epoch 430/1000, Train Loss: 0.8522457480430603, Val Loss: 0.26131734251976013, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 6.298942610383584e-05
Epoch 440/1000, Train Loss: 1.0073769092559814, Val Loss: 0.28042247891426086, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 5.44831470364784e-05
Epoch 450/1000, Train Loss: 1.0658271610736847, Val Loss: 0.28032171726226807, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 4.5976867969120956e-05
Epoch 460/1000, Train Loss: 0.9524554312229156, Val Loss: 0.2811245322227478, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 3.7470588901763514e-05
Epoch 470/1000, Train Loss: 0.8643363416194916, Val Loss: 0.27685490250587463, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 2.896430983440607e-05
Epoch 480/1000, Train Loss: 0.8853981494903564, Val Loss: 0.27656790614128113, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 2.0458030767048607e-05
Epoch 490/1000, Train Loss: 0.6112783253192902, Val Loss: 0.27800244092941284, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 1.1951751699691145e-05
Epoch 500/1000, Train Loss: 0.8221187889575958, Val Loss: 0.2781105935573578, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 510/1000, Train Loss: 1.107021987438202, Val Loss: 0.27849429845809937, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 520/1000, Train Loss: 0.6709302663803101, Val Loss: 0.278543621301651, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 530/1000, Train Loss: 0.6904578804969788, Val Loss: 0.2793174386024475, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 540/1000, Train Loss: 0.8595116138458252, Val Loss: 0.2810109257698059, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 550/1000, Train Loss: 0.6068630218505859, Val Loss: 0.2825746238231659, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 560/1000, Train Loss: 0.7355352640151978, Val Loss: 0.2827746272087097, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 570/1000, Train Loss: 0.7046772837638855, Val Loss: 0.28222328424453735, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 580/1000, Train Loss: 0.7510801255702972, Val Loss: 0.2809637188911438, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 590/1000, Train Loss: 0.6504290252923965, Val Loss: 0.2805310785770416, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 600/1000, Train Loss: 1.0189887881278992, Val Loss: 0.2805643379688263, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 610/1000, Train Loss: 0.7736024260520935, Val Loss: 0.280470073223114, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 620/1000, Train Loss: 1.0456530451774597, Val Loss: 0.280434250831604, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 630/1000, Train Loss: 1.039320021867752, Val Loss: 0.28108668327331543, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 640/1000, Train Loss: 0.5580682158470154, Val Loss: 0.2816913425922394, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 650/1000, Train Loss: 0.7574399411678314, Val Loss: 0.28143179416656494, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 660/1000, Train Loss: 0.9083563089370728, Val Loss: 0.28124478459358215, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 670/1000, Train Loss: 1.026898354291916, Val Loss: 0.2808837890625, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 680/1000, Train Loss: 0.6299977898597717, Val Loss: 0.28088662028312683, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 690/1000, Train Loss: 0.9426038265228271, Val Loss: 0.280962198972702, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 700/1000, Train Loss: 0.7922897934913635, Val Loss: 0.28101029992103577, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 710/1000, Train Loss: 0.8250456154346466, Val Loss: 0.2808089852333069, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 720/1000, Train Loss: 0.7414208352565765, Val Loss: 0.2806209921836853, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 730/1000, Train Loss: 1.0615196526050568, Val Loss: 0.2808384299278259, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 740/1000, Train Loss: 0.785196527838707, Val Loss: 0.28087499737739563, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 750/1000, Train Loss: 0.7049701809883118, Val Loss: 0.2808525562286377, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 760/1000, Train Loss: 0.858215868473053, Val Loss: 0.2808479070663452, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 770/1000, Train Loss: 0.7620483040809631, Val Loss: 0.2805793881416321, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 780/1000, Train Loss: 0.8730558156967163, Val Loss: 0.2804572582244873, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 790/1000, Train Loss: 0.7413354367017746, Val Loss: 0.2809502184391022, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 800/1000, Train Loss: 0.9493656754493713, Val Loss: 0.2810271382331848, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 810/1000, Train Loss: 0.9156646132469177, Val Loss: 0.2807311415672302, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 820/1000, Train Loss: 0.8551628887653351, Val Loss: 0.2805250585079193, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 830/1000, Train Loss: 0.6963693499565125, Val Loss: 0.28075748682022095, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 840/1000, Train Loss: 0.7276915907859802, Val Loss: 0.2814193069934845, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 850/1000, Train Loss: 0.8454046845436096, Val Loss: 0.28149232268333435, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 860/1000, Train Loss: 0.9394748210906982, Val Loss: 0.2814877927303314, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 870/1000, Train Loss: 0.8771063089370728, Val Loss: 0.2816619277000427, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 880/1000, Train Loss: 0.8194786608219147, Val Loss: 0.28151005506515503, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 890/1000, Train Loss: 1.0000665485858917, Val Loss: 0.28124549984931946, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 900/1000, Train Loss: 0.8812392950057983, Val Loss: 0.28050607442855835, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 910/1000, Train Loss: 1.076599657535553, Val Loss: 0.2804681360721588, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 920/1000, Train Loss: 0.70411017537117, Val Loss: 0.2804580628871918, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 930/1000, Train Loss: 0.8704506456851959, Val Loss: 0.28031307458877563, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 940/1000, Train Loss: 0.7085179835557938, Val Loss: 0.28072628378868103, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 950/1000, Train Loss: 0.7958614230155945, Val Loss: 0.28070324659347534, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 960/1000, Train Loss: 0.5158464759588242, Val Loss: 0.28084495663642883, Train Acc: 0.9, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 970/1000, Train Loss: 0.6627135276794434, Val Loss: 0.28033265471458435, Train Acc: 0.9, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 980/1000, Train Loss: 0.8912919461727142, Val Loss: 0.2798421084880829, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
Epoch 990/1000, Train Loss: 0.726294994354248, Val Loss: 0.27931004762649536, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.296100539069429e-06
[34m[1mwandb[39m[22m: [33mWARNING[39m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'Train Loss': 2.915551543235779, 'Validation Loss': 0.7001800537109375, 'Train Accuracy': 0.5, 'Validation Accuracy': 0.6000000238418579, '_timestamp': 1717438635.244514}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.weight': {'_type': 'histogram', 'values': [2068, 3168, 3198, 3293, 3275, 3323, 3338, 3202, 3224, 3291, 3258, 3226, 3158, 3224, 3236, 3304, 3137, 3206, 3258, 3257, 3235, 3315, 3258, 3227, 3276, 3105, 3190, 3225, 3259, 3234, 3173, 3212, 3158, 3365, 3137, 3313, 3299, 3221, 3094, 3250, 3338, 3355, 3262, 3185, 3199, 3182, 3258, 3200, 3145, 3278, 3157, 3245, 3220, 3313, 3189, 3372, 3293, 3249, 3267, 3216, 3383, 3268, 3264, 2172], 'bins': [-0.1039760559797287, -0.10072646290063858, -0.09747687727212906, -0.09422728419303894, -0.09097769856452942, -0.0877281054854393, -0.08447851985692978, -0.08122892677783966, -0.07797934114933014, -0.07472974807024002, -0.0714801549911499, -0.06823056936264038, -0.06498097628355026, -0.06173138692975044, -0.05848179757595062, -0.0552322082221508, -0.05198261886835098, -0.04873302951455116, -0.04548344016075134, -0.04223385080695152, -0.0389842614531517, -0.035734668374061584, -0.032485079020261765, -0.029235489666461945, -0.025985900312662125, -0.022736310958862305, -0.019486721605062485, -0.016237130388617516, -0.012987541034817696, -0.009737951681017876, -0.006488361395895481, -0.003238771576434374, 1.0818243026733398e-05, 0.0032604080624878407, 0.006509997881948948, 0.009759588167071342, 0.013009177520871162, 0.016258766874670982, 0.01950835809111595, 0.02275794744491577, 0.02600753679871559, 0.02925712615251541, 0.03250671550631523, 0.03575630486011505, 0.03900589793920517, 0.04225548729300499, 0.04550507664680481, 0.04875466600060463, 0.05200425535440445, 0.05525384470820427, 0.05850343406200409, 0.06175302341580391, 0.06500261276960373, 0.06825220584869385, 0.07150179147720337, 0.07475138455629349, 0.0780009776353836, 0.08125056326389313, 0.08450015634298325, 0.08774974197149277, 0.09099933505058289, 0.09424892067909241, 0.09749851375818253, 0.10074809938669205, 0.10399769246578217]}, '_timestamp': 1717438635.2469}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.bias': {'_type': 'histogram', 'values': [6, 4, 1, 3, 3, 1, 4, 5, 6, 4, 1, 6, 3, 2, 4, 1, 9, 4, 1, 3, 6, 3, 3, 5, 1, 3, 2, 7, 5, 2, 2, 7, 7, 4, 1, 3, 3, 1, 6, 7, 4, 3, 3, 2, 2, 3, 3, 3, 4, 3, 6, 3, 5, 4, 1, 2, 2, 4, 2, 0, 2, 5, 6, 7], 'bins': [-0.03177684172987938, -0.03077300265431404, -0.029769161716103554, -0.028765322640538216, -0.02776148170232773, -0.02675764262676239, -0.025753803551197052, -0.024749962612986565, -0.023746123537421227, -0.02274228259921074, -0.0217384435236454, -0.020734604448080063, -0.019730763509869576, -0.018726924434304237, -0.01772308349609375, -0.016719244420528412, -0.015715405344963074, -0.014711564406752586, -0.013707724399864674, -0.012703885324299335, -0.011700045317411423, -0.01069620531052351, -0.009692365303635597, -0.008688525296747684, -0.007684685755521059, -0.006680845748633146, -0.005677006207406521, -0.004673166200518608, -0.003669326426461339, -0.00266548665240407, -0.001661646761931479, -0.000657806929666549, 0.00034603290259838104, 0.0013498726766556501, 0.0023537124507129192, 0.003357552457600832, 0.004361391998827457, 0.00536523200571537, 0.006369072012603283, 0.007372911553829908, 0.008376751095056534, 0.009380591101944447, 0.01038443110883236, 0.011388271115720272, 0.012392111122608185, 0.013395951129496098, 0.014399790205061436, 0.015403630211949348, 0.016407471150159836, 0.017411310225725174, 0.018415149301290512, 0.019418990239501, 0.020422829315066338, 0.021426670253276825, 0.022430509328842163, 0.0234343484044075, 0.02443818934261799, 0.025442028418183327, 0.026445869356393814, 0.027449708431959152, 0.02845354750752449, 0.029457388445734978, 0.030461227521300316, 0.0314650684595108, 0.03246890753507614]}, '_timestamp': 1717438635.2471502}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.weight': {'_type': 'histogram', 'values': [120, 5, 12, 11, 3, 5, 2, 0, 1, 2, 1, 1, 0, 2, 2, 0, 0, 1, 1, 0, 1, 1, 2, 2, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 1, 1, 4, 0, 3, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 2, 1, 1, 2, 1, 3, 4, 6, 8], 'bins': [0.9991406798362732, 0.9991675615310669, 0.9991943836212158, 0.9992212653160095, 0.9992480874061584, 0.9992749691009521, 0.9993017911911011, 0.9993286728858948, 0.9993554949760437, 0.9993823766708374, 0.9994091987609863, 0.99943608045578, 0.9994629621505737, 0.9994897842407227, 0.9995166659355164, 0.9995434880256653, 0.999570369720459, 0.9995971918106079, 0.9996240735054016, 0.9996508955955505, 0.9996777772903442, 0.9997045993804932, 0.9997314810752869, 0.9997583627700806, 0.9997851848602295, 0.9998120665550232, 0.9998388886451721, 0.9998657703399658, 0.9998925924301147, 0.9999194741249084, 0.9999462962150574, 0.9999731779098511, 1.0, 1.0000269412994385, 1.0000537633895874, 1.0000805854797363, 1.0001074075698853, 1.0001343488693237, 1.0001611709594727, 1.0001879930496216, 1.0002148151397705, 1.000241756439209, 1.000268578529358, 1.0002954006195068, 1.0003223419189453, 1.0003491640090942, 1.0003759860992432, 1.000402808189392, 1.0004297494888306, 1.0004565715789795, 1.0004833936691284, 1.0005102157592773, 1.0005371570587158, 1.0005639791488647, 1.0005908012390137, 1.0006177425384521, 1.000644564628601, 1.00067138671875, 1.000698208808899, 1.0007251501083374, 1.0007519721984863, 1.0007787942886353, 1.0008056163787842, 1.0008325576782227, 1.0008593797683716]}, '_timestamp': 1717438635.247339}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.bias': {'_type': 'histogram', 'values': [15, 2, 8, 1, 4, 7, 4, 1, 3, 2, 4, 0, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0, 3, 1, 4, 0, 3, 1, 0, 0, 0, 0, 84, 0, 0, 0, 3, 0, 1, 2, 4, 1, 1, 5, 2, 2, 2, 0, 0, 1, 1, 2, 0, 0, 1, 1, 2, 2, 4, 2, 2, 3, 8, 21], 'bins': [-0.0008593710954301059, -0.000832516117952764, -0.0008056610822677612, -0.0007788060465827584, -0.0007519510691054165, -0.0007250960916280746, -0.0006982410559430718, -0.000671386020258069, -0.0006445310427807271, -0.0006176760653033853, -0.0005908210296183825, -0.0005639659939333797, -0.0005371110164560378, -0.0005102560389786959, -0.00048340100329369307, -0.0004565459967125207, -0.00042969099013134837, -0.000402835983550176, -0.0003759809769690037, -0.00034912597038783133, -0.000322270963806659, -0.00029541595722548664, -0.0002685609506443143, -0.00024170594406314194, -0.0002148509374819696, -0.00018799593090079725, -0.0001611409243196249, -0.00013428591773845255, -0.0001074309111572802, -8.057590457610786e-05, -5.372089799493551e-05, -2.6865891413763165e-05, -1.0884832590818405e-08, 2.684412174858153e-05, 5.3699128329753876e-05, 8.055413491092622e-05, 0.00010740914149209857, 0.00013426414807327092, 0.00016111915465444326, 0.0001879741612356156, 0.00021482916781678796, 0.0002416841743979603, 0.00026853918097913265, 0.000295394187560305, 0.00032224919414147735, 0.0003491042007226497, 0.00037595920730382204, 0.0004028142138849944, 0.00042966922046616673, 0.0004565242270473391, 0.00048337923362851143, 0.0005102342693135142, 0.0005370892467908561, 0.000563944224268198, 0.0005907992599532008, 0.0006176542956382036, 0.0006445092731155455, 0.0006713642505928874, 0.0006982192862778902, 0.000725074321962893, 0.0007519292994402349, 0.0007787842769175768, 0.0008056393126025796, 0.0008324943482875824, 0.0008593493257649243]}, '_timestamp': 1717438635.247502}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.weight': {'_type': 'histogram', 'values': [631, 849, 749, 827, 774, 841, 821, 833, 838, 838, 849, 764, 786, 833, 775, 808, 827, 796, 787, 855, 800, 839, 861, 810, 839, 862, 806, 810, 801, 830, 852, 872, 852, 835, 833, 804, 791, 799, 862, 878, 847, 797, 857, 822, 850, 753, 759, 815, 807, 832, 814, 769, 792, 805, 853, 824, 790, 766, 836, 843, 825, 780, 813, 618], 'bins': [-0.16297537088394165, -0.15788328647613525, -0.15279118716716766, -0.14769910275936127, -0.14260701835155487, -0.13751493394374847, -0.13242283463478088, -0.1273307502269745, -0.12223866581916809, -0.1171465739607811, -0.1120544895529747, -0.10696239769458771, -0.10187031328678131, -0.09677822142839432, -0.09168613702058792, -0.08659404516220093, -0.08150196075439453, -0.07640986889600754, -0.07131777703762054, -0.06622569262981415, -0.06113360449671745, -0.05604151636362076, -0.05094942823052406, -0.04585734009742737, -0.040765248239040375, -0.03567316010594368, -0.030581073835492134, -0.02548898570239544, -0.020396895706653595, -0.015304808504879475, -0.010212719440460205, -0.00512063130736351, -2.8543174266815186e-05, 0.00506354495882988, 0.010155633091926575, 0.015247722156345844, 0.020339809358119965, 0.02543189935386181, 0.030523987486958504, 0.03561607375741005, 0.040708161890506744, 0.04580025374889374, 0.05089234188199043, 0.05598443001508713, 0.06107651814818382, 0.06616860628128052, 0.07126069068908691, 0.07635278254747391, 0.0814448744058609, 0.0865369588136673, 0.09162905067205429, 0.09672113507986069, 0.10181322693824768, 0.10690531134605408, 0.11199740320444107, 0.11708948761224747, 0.12218157947063446, 0.12727366387844086, 0.13236574828624725, 0.13745784759521484, 0.14254993200302124, 0.14764201641082764, 0.15273410081863403, 0.15782620012760162, 0.16291828453540802]}, '_timestamp': 1717438635.24795}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.bias': {'_type': 'histogram', 'values': [8, 7, 2, 2, 4, 6, 2, 3, 1, 5, 4, 5, 3, 2, 5, 2, 4, 2, 6, 4, 3, 1, 1, 6, 5, 2, 1, 5, 3, 3, 4, 6, 1, 3, 3, 2, 4, 2, 6, 2, 5, 6, 3, 4, 4, 1, 4, 2, 2, 6, 2, 9, 1, 5, 2, 4, 3, 6, 3, 2, 5, 3, 1, 5], 'bins': [-0.0655336081981659, -0.0634852796792984, -0.06143695116043091, -0.059388622641563416, -0.05734029412269592, -0.05529196560382843, -0.05324363708496094, -0.051195308566093445, -0.04914698004722595, -0.04709865152835846, -0.04505032300949097, -0.043001994490623474, -0.04095366597175598, -0.03890533745288849, -0.036857008934020996, -0.0348086804151535, -0.03276035189628601, -0.030712025240063667, -0.028663696721196175, -0.026615368202328682, -0.02456703968346119, -0.022518711164593697, -0.020470382645726204, -0.01842205412685871, -0.016373727470636368, -0.0143253980204463, -0.012277069501578808, -0.010228740982711315, -0.008180413395166397, -0.006132084410637617, -0.004083756357431412, -0.002035427838563919, 1.2900680303573608e-05, 0.0020612291991710663, 0.004109557718038559, 0.006157885771244764, 0.008206214755773544, 0.010254542343318462, 0.012302870862185955, 0.014351199381053448, 0.016399528831243515, 0.01844785548746586, 0.02049618400633335, 0.022544512525200844, 0.024592841044068336, 0.02664116956293583, 0.028689498081803322, 0.030737826600670815, 0.03278615325689316, 0.03483448177576065, 0.03688281029462814, 0.038931138813495636, 0.04097946733236313, 0.04302779585123062, 0.045076124370098114, 0.04712445288896561, 0.0491727814078331, 0.05122110992670059, 0.053269438445568085, 0.05531776696443558, 0.05736609548330307, 0.05941442400217056, 0.061462752521038055, 0.06351108103990555, 0.06555940955877304]}, '_timestamp': 1717438635.248115}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.weight': {'_type': 'histogram', 'values': [58, 15, 14, 10, 6, 3, 4, 1, 1, 0, 5, 1, 4, 2, 0, 4, 0, 2, 0, 1, 1, 1, 1, 0, 4, 3, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 4, 3, 1, 2, 2, 1, 1, 4, 2, 2, 0, 1, 0, 2, 3, 5, 1, 4, 1, 1, 3, 4, 4, 4, 4, 19], 'bins': [0.9991406202316284, 0.9991674423217773, 0.999194324016571, 0.9992212057113647, 0.9992480278015137, 0.9992748498916626, 0.9993017315864563, 0.99932861328125, 0.9993554353713989, 0.9993822574615479, 0.9994091391563416, 0.9994360208511353, 0.9994628429412842, 0.9994896650314331, 0.9995165467262268, 0.9995434284210205, 0.9995702505111694, 0.9995970726013184, 0.9996239542961121, 0.9996508359909058, 0.9996776580810547, 0.9997044801712036, 0.9997313618659973, 0.999758243560791, 0.9997850656509399, 0.9998118877410889, 0.9998387694358826, 0.9998656511306763, 0.9998924732208252, 0.9999192953109741, 0.9999461770057678, 0.9999730587005615, 0.9999998807907104, 1.0000267028808594, 1.0000536441802979, 1.0000804662704468, 1.0001072883605957, 1.0001341104507446, 1.0001609325408936, 1.000187873840332, 1.000214695930481, 1.0002415180206299, 1.0002684593200684, 1.0002952814102173, 1.0003221035003662, 1.0003489255905151, 1.000375747680664, 1.0004026889801025, 1.0004295110702515, 1.0004563331604004, 1.0004832744598389, 1.0005100965499878, 1.0005369186401367, 1.0005637407302856, 1.0005905628204346, 1.000617504119873, 1.000644326210022, 1.000671148300171, 1.0006980895996094, 1.0007249116897583, 1.0007517337799072, 1.0007785558700562, 1.000805377960205, 1.0008323192596436, 1.0008591413497925]}, '_timestamp': 1717438635.248273}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.bias': {'_type': 'histogram', 'values': [25, 8, 4, 7, 4, 16, 3, 5, 4, 2, 2, 2, 2, 1, 0, 2, 1, 2, 3, 2, 3, 2, 1, 3, 1, 4, 3, 2, 0, 0, 0, 1, 0, 0, 0, 0, 4, 4, 3, 2, 4, 4, 3, 2, 1, 0, 2, 0, 1, 3, 1, 0, 2, 2, 1, 1, 1, 3, 13, 2, 11, 7, 6, 30], 'bins': [-0.0008593074162490666, -0.0008324530790559947, -0.0008055988000705838, -0.000778744462877512, -0.0007518901256844401, -0.0007250358466990292, -0.0006981815095059574, -0.0006713271723128855, -0.0006444728933274746, -0.0006176185561344028, -0.0005907642189413309, -0.00056390993995592, -0.0005370556027628481, -0.0005102012655697763, -0.0004833469574805349, -0.0004564926493912935, -0.0004296383121982217, -0.0004027840041089803, -0.0003759296960197389, -0.00034907535882666707, -0.0003222210507374257, -0.0002953667426481843, -0.00026851240545511246, -0.00024165809736587107, -0.00021480378927662969, -0.00018794946663547307, -0.00016109514399431646, -0.00013424082135315984, -0.00010738651326391846, -8.053219062276185e-05, -5.3677875257562846e-05, -2.682355625438504e-05, 3.0762748792767525e-08, 2.6885081751970574e-05, 5.373940075514838e-05, 8.059371612034738e-05, 0.000107448038761504, 0.00013430234685074538, 0.000161156669491902, 0.0001880109921330586, 0.00021486531477421522, 0.0002417196228634566, 0.000268573930952698, 0.00029542826814576983, 0.0003222825762350112, 0.0003491368843242526, 0.00037599122151732445, 0.00040284552960656583, 0.0004296998376958072, 0.00045655417488887906, 0.00048340848297812045, 0.0005102627910673618, 0.0005371171282604337, 0.0005639714654535055, 0.0005908257444389164, 0.0006176800816319883, 0.0006445344188250601, 0.0006713886978104711, 0.0006982430350035429, 0.0007250973721966147, 0.0007519516511820257, 0.0007788059883750975, 0.0008056603255681694, 0.0008325146045535803, 0.0008593689417466521]}, '_timestamp': 1717438635.248427}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.weight': {'_type': 'histogram', 'values': [672, 833, 841, 839, 808, 807, 837, 861, 778, 844, 823, 812, 804, 825, 844, 797, 823, 796, 792, 853, 849, 847, 811, 820, 828, 794, 831, 802, 837, 820, 797, 846, 862, 781, 828, 830, 821, 830, 823, 785, 813, 787, 777, 838, 812, 803, 831, 790, 799, 785, 818, 797, 797, 803, 789, 822, 837, 836, 783, 792, 848, 838, 786, 672], 'bins': [-0.16286486387252808, -0.15777236223220825, -0.15267987549304962, -0.1475873738527298, -0.14249488711357117, -0.13740238547325134, -0.13230988383293152, -0.1272173970937729, -0.12212489545345306, -0.11703240126371384, -0.11193990707397461, -0.10684741288423538, -0.10175491869449615, -0.09666242450475693, -0.0915699228644371, -0.08647742867469788, -0.08138493448495865, -0.07629244029521942, -0.0711999461054802, -0.06610744446516037, -0.06101495027542114, -0.055922456085681915, -0.05082996189594269, -0.04573746398091316, -0.040644969791173935, -0.03555247560143471, -0.030459977686405182, -0.025367483496665955, -0.020274987444281578, -0.015182491391897202, -0.0100899962708354, -0.00499750068411231, 9.499490261077881e-05, 0.005187490489333868, 0.010279986076056957, 0.01537248119711876, 0.020464977249503136, 0.025557473301887512, 0.03064996749162674, 0.035742465406656265, 0.04083495959639549, 0.04592745378613472, 0.051019951701164246, 0.05611244589090347, 0.0612049400806427, 0.06629743427038193, 0.07138993591070175, 0.07648243010044098, 0.0815749242901802, 0.08666741847991943, 0.09175991266965866, 0.09685241430997849, 0.10194490849971771, 0.10703740268945694, 0.11212989687919617, 0.1172223910689354, 0.12231488525867462, 0.12740738689899445, 0.13249987363815308, 0.1375923752784729, 0.14268487691879272, 0.14777736365795135, 0.15286986529827118, 0.1579623520374298, 0.16305485367774963]}, '_timestamp': 1717438635.248863}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.bias': {'_type': 'histogram', 'values': [2, 6, 6, 3, 1, 3, 5, 2, 5, 2, 7, 9, 6, 5, 0, 8, 4, 2, 4, 1, 3, 5, 2, 2, 1, 1, 2, 6, 4, 2, 5, 2, 4, 8, 4, 1, 3, 6, 3, 6, 3, 2, 4, 7, 2, 4, 5, 1, 5, 4, 2, 3, 2, 2, 7, 2, 4, 2, 2, 2, 3, 3, 4, 2], 'bins': [-0.06633032113313675, -0.06425946205854416, -0.06218860298395157, -0.06011774390935898, -0.05804688483476639, -0.0559760257601738, -0.05390516296029091, -0.05183430388569832, -0.04976344481110573, -0.04769258573651314, -0.04562172666192055, -0.04355086758732796, -0.04148000851273537, -0.039409149438142776, -0.037338290363550186, -0.035267431288957596, -0.03319656848907471, -0.031125711277127266, -0.029054852202534676, -0.026983991265296936, -0.024913132190704346, -0.022842273116111755, -0.020771414041519165, -0.018700554966926575, -0.016629695892333984, -0.01455883588641882, -0.012487975880503654, -0.010417116805911064, -0.008346257731318474, -0.006275397725403309, -0.0042045386508107185, -0.002133679110556841, -6.281957030296326e-05, 0.0020080399699509144, 0.004078899510204792, 0.006149758584797382, 0.008220618590712547, 0.010291477665305138, 0.012362336739897728, 0.014433196745812893, 0.016504056751728058, 0.018574915826320648, 0.02064577490091324, 0.02271663397550583, 0.02478749305009842, 0.02685835212469101, 0.02892921306192875, 0.03100007213652134, 0.03307092934846878, 0.03514179214835167, 0.03721265122294426, 0.03928351029753685, 0.04135436937212944, 0.04342522844672203, 0.04549608752131462, 0.04756694659590721, 0.0496378056704998, 0.05170866474509239, 0.05377952381968498, 0.05585038661956787, 0.05792124569416046, 0.05999210476875305, 0.06206296384334564, 0.06413382291793823, 0.06620468199253082]}, '_timestamp': 1717438635.249016}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.weight': {'_type': 'histogram', 'values': [70, 16, 15, 11, 7, 7, 1, 5, 4, 1, 2, 2, 5, 3, 1, 0, 0, 2, 1, 0, 1, 2, 0, 2, 1, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 5, 3, 3, 4, 3, 2, 0, 1, 1, 1, 0, 3, 4, 1, 2, 0, 1, 2, 1, 0, 0, 1, 2, 3, 1, 7, 7], 'bins': [0.9991406798362732, 0.9991675615310669, 0.9991943836212158, 0.9992212653160095, 0.9992480874061584, 0.9992749691009521, 0.9993017911911011, 0.9993286728858948, 0.9993554949760437, 0.9993823766708374, 0.9994091987609863, 0.99943608045578, 0.9994629621505737, 0.9994897842407227, 0.9995166659355164, 0.9995434880256653, 0.999570369720459, 0.9995971918106079, 0.9996240735054016, 0.9996508955955505, 0.9996777772903442, 0.9997045993804932, 0.9997314810752869, 0.9997583627700806, 0.9997851848602295, 0.9998120665550232, 0.9998388886451721, 0.9998657703399658, 0.9998925924301147, 0.9999194741249084, 0.9999462962150574, 0.9999731779098511, 1.0, 1.0000269412994385, 1.0000537633895874, 1.0000805854797363, 1.0001074075698853, 1.0001343488693237, 1.0001611709594727, 1.0001879930496216, 1.0002148151397705, 1.000241756439209, 1.000268578529358, 1.0002954006195068, 1.0003223419189453, 1.0003491640090942, 1.0003759860992432, 1.000402808189392, 1.0004297494888306, 1.0004565715789795, 1.0004833936691284, 1.0005102157592773, 1.0005371570587158, 1.0005639791488647, 1.0005908012390137, 1.0006177425384521, 1.000644564628601, 1.00067138671875, 1.000698208808899, 1.0007251501083374, 1.0007519721984863, 1.0007787942886353, 1.0008056163787842, 1.0008325576782227, 1.0008593797683716]}, '_timestamp': 1717438635.249166}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.bias': {'_type': 'histogram', 'values': [31, 8, 11, 8, 5, 14, 2, 6, 3, 1, 3, 2, 2, 4, 1, 3, 0, 0, 4, 0, 1, 3, 1, 3, 3, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 3, 3, 7, 2, 2, 5, 2, 1, 0, 1, 1, 0, 2, 3, 1, 0, 4, 0, 7, 5, 3, 4, 10, 11, 21], 'bins': [-0.0008593659149482846, -0.0008325118687935174, -0.0008056577644310892, -0.0007788037182763219, -0.0007519496721215546, -0.0007250955677591264, -0.0006982415216043591, -0.0006713874754495919, -0.0006445333710871637, -0.0006176793249323964, -0.0005908252787776291, -0.000563971174415201, -0.0005371171282604337, -0.0005102630821056664, -0.0004834089777432382, -0.00045655493158847094, -0.0004297008563298732, -0.00040284678107127547, -0.0003759927349165082, -0.00034913865965791047, -0.00032228458439931273, -0.00029543053824454546, -0.00026857646298594773, -0.00024172238772735, -0.0002148683270206675, -0.000188014266313985, -0.00016116019105538726, -0.00013430613034870476, -0.00010745206236606464, -8.059799438342452e-05, -5.374393003876321e-05, -2.6889863875112496e-05, -3.5797711461782455e-08, 2.681826845218893e-05, 5.3672334615839645e-05, 8.052639896050096e-05, 0.00010738046694314107, 0.0001342345349257812, 0.0001610885956324637, 0.00018794267089106143, 0.00021479673159774393, 0.00024165079230442643, 0.00026850486756302416, 0.0002953589428216219, 0.00032221298897638917, 0.0003490670642349869, 0.00037592113949358463, 0.0004027751856483519, 0.00042962926090694964, 0.00045648333616554737, 0.00048333738232031465, 0.0005101914866827428, 0.0005370455328375101, 0.0005638995789922774, 0.0005907536833547056, 0.0006176077295094728, 0.0006444617756642401, 0.0006713158800266683, 0.0006981699261814356, 0.0007250239723362029, 0.000751878076698631, 0.0007787321228533983, 0.0008055861690081656, 0.0008324402733705938, 0.0008592943195253611]}, '_timestamp': 1717438635.249313}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.weight': {'_type': 'histogram', 'values': [6, 2, 4, 2, 5, 5, 4, 1, 3, 1, 5, 0, 5, 4, 6, 4, 2, 3, 3, 3, 1, 3, 0, 5, 3, 8, 2, 4, 4, 4, 7, 5, 3, 2, 4, 4, 3, 4, 7, 3, 5, 2, 2, 1, 2, 4, 7, 4, 7, 7, 5, 1, 5, 5, 0, 2, 4, 5, 1, 4, 0, 6, 2, 3], 'bins': [-0.22672607004642487, -0.21961157023906708, -0.2124970555305481, -0.2053825557231903, -0.19826804101467133, -0.19115354120731354, -0.18403902649879456, -0.17692452669143677, -0.16981001198291779, -0.16269551217556, -0.15558099746704102, -0.14846649765968323, -0.14135199785232544, -0.13423748314380646, -0.12712298333644867, -0.12000846862792969, -0.1128939688205719, -0.10577946156263351, -0.09866495430469513, -0.09155044704675674, -0.08443593978881836, -0.07732143253087997, -0.07020692527294159, -0.0630924180150032, -0.05597791075706482, -0.04886340722441673, -0.04174889996647835, -0.03463439270853996, -0.027519885450601578, -0.020405380055308342, -0.013290872797369957, -0.006176366470754147, 0.0009381398558616638, 0.008052646182477474, 0.015167152509093285, 0.02228165976703167, 0.029396165162324905, 0.03651067242026329, 0.043625179678201675, 0.05073968693614006, 0.05785419046878815, 0.06496869772672653, 0.07208320498466492, 0.0791977122426033, 0.08631221950054169, 0.09342672675848007, 0.10054123401641846, 0.10765574127435684, 0.11477024853229523, 0.12188474833965302, 0.128999263048172, 0.13611376285552979, 0.14322827756404877, 0.15034277737140656, 0.15745727717876434, 0.16457179188728333, 0.1716862916946411, 0.1788008064031601, 0.18591530621051788, 0.19302982091903687, 0.20014432072639465, 0.20725883543491364, 0.21437333524227142, 0.2214878499507904, 0.2286023497581482]}, '_timestamp': 1717438635.249467}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.bias': {'_type': 'histogram', 'values': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'bins': [-0.5500773787498474, -0.5344523787498474, -0.5188273787498474, -0.5032023787498474, -0.4875773787498474, -0.4719523787498474, -0.4563273787498474, -0.4407023787498474, -0.4250773787498474, -0.4094523787498474, -0.3938273787498474, -0.3782023787498474, -0.3625773787498474, -0.3469523787498474, -0.3313273787498474, -0.3157023787498474, -0.3000773787498474, -0.2844523787498474, -0.2688273787498474, -0.2532023787498474, -0.23757736384868622, -0.22195236384868622, -0.20632736384868622, -0.19070236384868622, -0.17507736384868622, -0.15945236384868622, -0.14382736384868622, -0.12820236384868622, -0.11257736384868622, -0.09695236384868622, -0.08132736384868622, -0.06570236384868622, -0.05007736384868622, -0.03445236384868622, -0.018827363848686218, -0.0032023638486862183, 0.012422636151313782, 0.028047636151313782, 0.04367263615131378, 0.05929763615131378, 0.07492263615131378, 0.09054763615131378, 0.10617263615131378, 0.12179763615131378, 0.13742263615131378, 0.15304763615131378, 0.16867263615131378, 0.18429763615131378, 0.19992263615131378, 0.21554763615131378, 0.23117263615131378, 0.24679763615131378, 0.2624226212501526, 0.2780476212501526, 0.2936726212501526, 0.3092976212501526, 0.3249226212501526, 0.3405476212501526, 0.3561726212501526, 0.3717976212501526, 0.3874226212501526, 0.4030476212501526, 0.4186726212501526, 0.4342976212501526, 0.4499226212501526]}, '_timestamp': 1717438635.249634}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TP Reward Distribution': {'_type': 'histogram', 'values': [8, 4, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [2.6666666666666665, 5.503125, 8.339583333333334, 11.176041666666666, 14.0125, 16.848958333333332, 19.68541666666667, 22.521875, 25.358333333333334, 28.194791666666667, 31.03125, 33.86770833333333, 36.704166666666666, 39.540625, 42.37708333333333, 45.213541666666664, 48.05, 50.88645833333333, 53.72291666666666, 56.559374999999996, 59.39583333333333, 62.23229166666666, 65.06875, 67.90520833333333, 70.74166666666667, 73.578125, 76.41458333333334, 79.25104166666667, 82.0875, 84.92395833333333, 87.76041666666667, 90.596875, 93.43333333333334, 96.26979166666668, 99.10625, 101.94270833333334, 104.77916666666667, 107.61562500000001, 110.45208333333333, 113.28854166666667, 116.125, 118.96145833333334, 121.79791666666667, 124.634375, 127.47083333333333, 130.30729166666666, 133.14374999999998, 135.9802083333333, 138.81666666666666, 141.653125, 144.48958333333331, 147.32604166666667, 150.1625, 152.99895833333332, 155.83541666666665, 158.671875, 161.50833333333333, 164.34479166666665, 167.18124999999998, 170.01770833333333, 172.85416666666666, 175.69062499999998, 178.5270833333333, 181.36354166666666, 184.2]}, '_timestamp': 1717438635.2498071}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TN Reward Distribution': {'_type': 'histogram', 'values': [4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [3.6, 6.366666666666667, 9.133333333333333, 11.9, 14.666666666666666, 17.433333333333334, 20.200000000000003, 22.96666666666667, 25.733333333333334, 28.5, 31.266666666666666, 34.03333333333333, 36.800000000000004, 39.56666666666667, 42.333333333333336, 45.1, 47.86666666666667, 50.63333333333333, 53.4, 56.166666666666664, 58.93333333333333, 61.7, 64.46666666666667, 67.23333333333333, 70.0, 72.76666666666667, 75.53333333333333, 78.3, 81.06666666666666, 83.83333333333333, 86.6, 89.36666666666666, 92.13333333333333, 94.89999999999999, 97.66666666666666, 100.43333333333332, 103.19999999999999, 105.96666666666665, 108.73333333333332, 111.49999999999999, 114.26666666666665, 117.03333333333333, 119.8, 122.56666666666666, 125.33333333333333, 128.1, 130.86666666666667, 133.63333333333333, 136.4, 139.16666666666666, 141.93333333333334, 144.7, 147.46666666666667, 150.23333333333332, 153.0, 155.76666666666665, 158.53333333333333, 161.29999999999998, 164.06666666666666, 166.83333333333331, 169.6, 172.36666666666665, 175.13333333333333, 177.89999999999998, 180.66666666666666]}, '_timestamp': 1717438635.249949}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FP Reward Distribution': {'_type': 'histogram', 'values': [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1], 'bins': [3.533333333333333, 3.8249999999999997, 4.116666666666666, 4.408333333333333, 4.699999999999999, 4.991666666666666, 5.283333333333333, 5.574999999999999, 5.866666666666666, 6.158333333333333, 6.449999999999999, 6.741666666666666, 7.033333333333333, 7.324999999999999, 7.616666666666666, 7.908333333333332, 8.2, 8.491666666666667, 8.783333333333331, 9.075, 9.366666666666665, 9.658333333333331, 9.95, 10.241666666666665, 10.533333333333331, 10.825, 11.116666666666665, 11.408333333333331, 11.7, 11.991666666666665, 12.283333333333331, 12.575, 12.866666666666665, 13.158333333333331, 13.45, 13.741666666666665, 14.033333333333331, 14.325, 14.616666666666665, 14.908333333333331, 15.199999999999998, 15.491666666666665, 15.783333333333331, 16.074999999999996, 16.366666666666667, 16.65833333333333, 16.949999999999996, 17.241666666666667, 17.53333333333333, 17.824999999999996, 18.116666666666667, 18.40833333333333, 18.699999999999996, 18.991666666666667, 19.28333333333333, 19.574999999999996, 19.866666666666667, 20.15833333333333, 20.449999999999996, 20.741666666666667, 21.03333333333333, 21.324999999999996, 21.616666666666667, 21.90833333333333, 22.2]}, '_timestamp': 1717438635.25009}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FN Reward Distribution': {'_type': 'histogram', 'values': [2, 3, 1, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [3.3333333333333335, 3.7177083333333334, 4.102083333333334, 4.486458333333333, 4.870833333333334, 5.255208333333334, 5.639583333333334, 6.023958333333334, 6.408333333333333, 6.792708333333334, 7.177083333333334, 7.561458333333334, 7.945833333333335, 8.330208333333333, 8.714583333333334, 9.098958333333334, 9.483333333333334, 9.867708333333335, 10.252083333333333, 10.636458333333334, 11.020833333333334, 11.405208333333334, 11.789583333333335, 12.173958333333335, 12.558333333333335, 12.942708333333334, 13.327083333333334, 13.711458333333335, 14.095833333333335, 14.480208333333335, 14.864583333333334, 15.248958333333334, 15.633333333333335, 16.017708333333335, 16.402083333333334, 16.786458333333332, 17.170833333333334, 17.555208333333333, 17.939583333333335, 18.323958333333334, 18.708333333333332, 19.092708333333334, 19.477083333333333, 19.86145833333333, 20.245833333333334, 20.630208333333332, 21.014583333333334, 21.398958333333333, 21.783333333333335, 22.167708333333334, 22.552083333333332, 22.936458333333334, 23.320833333333333, 23.705208333333335, 24.089583333333334, 24.473958333333332, 24.858333333333334, 25.242708333333333, 25.627083333333335, 26.011458333333334, 26.395833333333332, 26.780208333333334, 27.164583333333333, 27.548958333333335, 27.933333333333334]}, '_timestamp': 1717438635.250228}).
[32m[I 2024-06-03 14:17:31,338][39m Trial 0 finished with value: 0.13808099925518036 and parameters: {'hidden_size': 228, 'learning_rate': 0.00042961005390694583, 'weight_decay': 0.00031863628766595645}. Best is trial 0 with value: 0.13808099925518036.