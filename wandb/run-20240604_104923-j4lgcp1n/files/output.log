Epoch 0/1000, Train Loss: 1.5434736609458923, Val Loss: 0.634831964969635, Train Acc: 0.625, Val Acc: 0.800000011920929, LR: 0.00035928797169287646
Epoch 10/1000, Train Loss: 2.0324464440345764, Val Loss: 0.5847230553627014, Train Acc: 0.55, Val Acc: 0.800000011920929, LR: 0.00035215995638304416
Epoch 20/1000, Train Loss: 1.5883890390396118, Val Loss: 0.5794661045074463, Train Acc: 0.575, Val Acc: 0.800000011920929, LR: 0.00034503194107321185
Epoch 30/1000, Train Loss: 1.9512585401535034, Val Loss: 0.5627415180206299, Train Acc: 0.55, Val Acc: 0.800000011920929, LR: 0.00033790392576337955
Epoch 40/1000, Train Loss: 2.5618202686309814, Val Loss: 0.5212455987930298, Train Acc: 0.525, Val Acc: 0.800000011920929, LR: 0.00033077591045354725
Epoch 50/1000, Train Loss: 1.4723577499389648, Val Loss: 0.5472697019577026, Train Acc: 0.625, Val Acc: 0.800000011920929, LR: 0.00032364789514371494
Epoch 60/1000, Train Loss: 2.3441526889801025, Val Loss: 0.5026235580444336, Train Acc: 0.5, Val Acc: 0.800000011920929, LR: 0.00031651987983388264
Epoch 70/1000, Train Loss: 1.9332326650619507, Val Loss: 0.469035804271698, Train Acc: 0.6, Val Acc: 0.800000011920929, LR: 0.00030939186452405034
Epoch 80/1000, Train Loss: 2.0639808177948, Val Loss: 0.4443529546260834, Train Acc: 0.525, Val Acc: 0.800000011920929, LR: 0.00030226384921421803
Epoch 90/1000, Train Loss: 1.1439504325389862, Val Loss: 0.44595369696617126, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 0.00029513583390438573
Epoch 100/1000, Train Loss: 1.1680209040641785, Val Loss: 0.43487539887428284, Train Acc: 0.625, Val Acc: 0.800000011920929, LR: 0.0002880078185945534
Epoch 110/1000, Train Loss: 1.1338390707969666, Val Loss: 0.4137335419654846, Train Acc: 0.65, Val Acc: 0.800000011920929, LR: 0.0002808798032847211
Epoch 120/1000, Train Loss: 0.992162823677063, Val Loss: 0.40094366669654846, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 0.0002737517879748888
Epoch 130/1000, Train Loss: 1.123076856136322, Val Loss: 0.3854414224624634, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 0.0002666237726650565
Epoch 140/1000, Train Loss: 1.062010020017624, Val Loss: 0.3906901180744171, Train Acc: 0.65, Val Acc: 0.800000011920929, LR: 0.0002594957573552242
Epoch 150/1000, Train Loss: 1.3428919315338135, Val Loss: 0.3951970934867859, Train Acc: 0.575, Val Acc: 0.800000011920929, LR: 0.0002523677420453919
Epoch 160/1000, Train Loss: 1.3512545228004456, Val Loss: 0.39432671666145325, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 0.0002452397267355596
Epoch 170/1000, Train Loss: 1.183865487575531, Val Loss: 0.3946938216686249, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 0.00023811171142572716
Epoch 180/1000, Train Loss: 1.0920365452766418, Val Loss: 0.38478606939315796, Train Acc: 0.65, Val Acc: 0.800000011920929, LR: 0.00023098369611589456
Epoch 190/1000, Train Loss: 0.966979444026947, Val Loss: 0.3891788125038147, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 0.00022385568080606207
Epoch 200/1000, Train Loss: 1.2319204807281494, Val Loss: 0.38190317153930664, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 0.00021672766549622952
Epoch 210/1000, Train Loss: 1.0771637260913849, Val Loss: 0.3894589841365814, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 0.00020959965018639703
Epoch 220/1000, Train Loss: 0.6552203893661499, Val Loss: 0.39583608508110046, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 0.00020247163487656462
Epoch 230/1000, Train Loss: 0.9849490523338318, Val Loss: 0.39369940757751465, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 0.00019534361956673215
Epoch 240/1000, Train Loss: 1.1298004984855652, Val Loss: 0.3803047835826874, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 0.00018821560425689968
Epoch 250/1000, Train Loss: 0.7618255317211151, Val Loss: 0.38417258858680725, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 0.0001810875889470672
Epoch 260/1000, Train Loss: 1.0496169924736023, Val Loss: 0.3925764858722687, Train Acc: 0.65, Val Acc: 0.800000011920929, LR: 0.00017395957363723483
Epoch 270/1000, Train Loss: 1.047225534915924, Val Loss: 0.3931313455104828, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 0.00016683155832740242
Epoch 280/1000, Train Loss: 1.0508153438568115, Val Loss: 0.40356436371803284, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 0.00015970354301757004
Epoch 290/1000, Train Loss: 1.1775749921798706, Val Loss: 0.3974025845527649, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 0.00015257552770773762
Epoch 300/1000, Train Loss: 1.3432554602622986, Val Loss: 0.39615243673324585, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 0.00014544751239790516
Epoch 310/1000, Train Loss: 0.6840027868747711, Val Loss: 0.39612579345703125, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 0.0001383194970880727
Epoch 320/1000, Train Loss: 0.7038061916828156, Val Loss: 0.3933282494544983, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 0.00013119148177824025
Epoch 330/1000, Train Loss: 0.8305083513259888, Val Loss: 0.3929460942745209, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 0.00012406346646840784
Epoch 340/1000, Train Loss: 0.9940389692783356, Val Loss: 0.4016285836696625, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 0.00011693545115857539
Epoch 350/1000, Train Loss: 0.8965498805046082, Val Loss: 0.39669471979141235, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 0.00010980743584874291
Epoch 360/1000, Train Loss: 0.8013522028923035, Val Loss: 0.39802971482276917, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 0.00010267942053891036
Epoch 370/1000, Train Loss: 0.6856057047843933, Val Loss: 0.3999179005622864, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 9.555140522907791e-05
Epoch 380/1000, Train Loss: 0.8299757540225983, Val Loss: 0.3991850018501282, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 8.842338991924541e-05
Epoch 390/1000, Train Loss: 0.7814585268497467, Val Loss: 0.4088660776615143, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 8.129537460941287e-05
Epoch 400/1000, Train Loss: 1.0152596533298492, Val Loss: 0.3994794487953186, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 7.416735929958033e-05
Epoch 410/1000, Train Loss: 0.8025674521923065, Val Loss: 0.3904423117637634, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 6.703934398974781e-05
Epoch 420/1000, Train Loss: 0.9049704968929291, Val Loss: 0.39519819617271423, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 5.9911328679915265e-05
Epoch 430/1000, Train Loss: 0.9326815605163574, Val Loss: 0.40097475051879883, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 5.278331337008279e-05
Epoch 440/1000, Train Loss: 0.9698323607444763, Val Loss: 0.40019720792770386, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 4.565529806025031e-05
Epoch 450/1000, Train Loss: 0.908026248216629, Val Loss: 0.3988357484340668, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 3.852728275041784e-05
Epoch 460/1000, Train Loss: 1.113720864057541, Val Loss: 0.397112101316452, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 3.1399267440585366e-05
Epoch 470/1000, Train Loss: 0.8853073418140411, Val Loss: 0.39643043279647827, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 2.4271252130752872e-05
Epoch 480/1000, Train Loss: 0.724980890750885, Val Loss: 0.3963521122932434, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 1.714323682092037e-05
Epoch 490/1000, Train Loss: 0.8479355573654175, Val Loss: 0.3954162001609802, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 1.0015221511087877e-05
Epoch 500/1000, Train Loss: 0.7653760015964508, Val Loss: 0.39649826288223267, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 510/1000, Train Loss: 1.1650490760803223, Val Loss: 0.3969765305519104, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 520/1000, Train Loss: 0.7578672617673874, Val Loss: 0.39740651845932007, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 530/1000, Train Loss: 1.1591986417770386, Val Loss: 0.397533655166626, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 540/1000, Train Loss: 1.1434321999549866, Val Loss: 0.39758166670799255, Train Acc: 0.625, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 550/1000, Train Loss: 0.8173064291477203, Val Loss: 0.3977630138397217, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 560/1000, Train Loss: 0.9379837512969971, Val Loss: 0.39767685532569885, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 570/1000, Train Loss: 0.7762390673160553, Val Loss: 0.39754217863082886, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 580/1000, Train Loss: 0.7055235505104065, Val Loss: 0.39735403656959534, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 590/1000, Train Loss: 0.9409556686878204, Val Loss: 0.39716798067092896, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 600/1000, Train Loss: 0.898400604724884, Val Loss: 0.39735347032546997, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 610/1000, Train Loss: 0.927385002374649, Val Loss: 0.39769047498703003, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 620/1000, Train Loss: 1.0293720960617065, Val Loss: 0.3978884816169739, Train Acc: 0.65, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 630/1000, Train Loss: 0.9018294513225555, Val Loss: 0.397651344537735, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 640/1000, Train Loss: 0.9310714900493622, Val Loss: 0.39722394943237305, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 650/1000, Train Loss: 0.6999861896038055, Val Loss: 0.39678362011909485, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 660/1000, Train Loss: 0.8826491236686707, Val Loss: 0.39679622650146484, Train Acc: 0.625, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 670/1000, Train Loss: 0.8977913558483124, Val Loss: 0.396266371011734, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 680/1000, Train Loss: 0.754058375954628, Val Loss: 0.39601776003837585, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 690/1000, Train Loss: 0.8627014458179474, Val Loss: 0.3959798216819763, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 700/1000, Train Loss: 0.7356811165809631, Val Loss: 0.39579877257347107, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 710/1000, Train Loss: 0.8432924449443817, Val Loss: 0.395725816488266, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 720/1000, Train Loss: 0.8566689342260361, Val Loss: 0.39551177620887756, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 730/1000, Train Loss: 0.9847763776779175, Val Loss: 0.39581063389778137, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 740/1000, Train Loss: 0.992253303527832, Val Loss: 0.39620083570480347, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 750/1000, Train Loss: 1.1969799399375916, Val Loss: 0.39595067501068115, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 760/1000, Train Loss: 0.8047408759593964, Val Loss: 0.3965068757534027, Train Acc: 0.875, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 770/1000, Train Loss: 0.6701579838991165, Val Loss: 0.39757025241851807, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 780/1000, Train Loss: 0.9355890154838562, Val Loss: 0.3978540301322937, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 790/1000, Train Loss: 0.5847630053758621, Val Loss: 0.3981862962245941, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 800/1000, Train Loss: 0.6791087090969086, Val Loss: 0.39801734685897827, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 810/1000, Train Loss: 0.8821548521518707, Val Loss: 0.39794689416885376, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 820/1000, Train Loss: 0.6989718675613403, Val Loss: 0.3981923460960388, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 830/1000, Train Loss: 0.7349344342947006, Val Loss: 0.398682177066803, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 840/1000, Train Loss: 0.8322502672672272, Val Loss: 0.39873257279396057, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 850/1000, Train Loss: 0.9640899300575256, Val Loss: 0.3983718156814575, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 860/1000, Train Loss: 0.6360094547271729, Val Loss: 0.39827239513397217, Train Acc: 0.875, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 870/1000, Train Loss: 0.7762909829616547, Val Loss: 0.3983832597732544, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 880/1000, Train Loss: 0.9096868932247162, Val Loss: 0.3982696831226349, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 890/1000, Train Loss: 0.9256493151187897, Val Loss: 0.39866405725479126, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 900/1000, Train Loss: 0.8259519040584564, Val Loss: 0.3988017439842224, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 910/1000, Train Loss: 0.7541799545288086, Val Loss: 0.39884287118911743, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 920/1000, Train Loss: 0.7689076364040375, Val Loss: 0.3986889719963074, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 930/1000, Train Loss: 1.1746986210346222, Val Loss: 0.3985440135002136, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 940/1000, Train Loss: 0.924688458442688, Val Loss: 0.39859408140182495, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 950/1000, Train Loss: 0.6752539277076721, Val Loss: 0.3983392119407654, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 960/1000, Train Loss: 0.5604501366615295, Val Loss: 0.39812368154525757, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 970/1000, Train Loss: 0.7903344035148621, Val Loss: 0.3983730375766754, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 980/1000, Train Loss: 0.9296762943267822, Val Loss: 0.3983679413795471, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
Epoch 990/1000, Train Loss: 0.9571476578712463, Val Loss: 0.39837461709976196, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 3.6000077322386366e-06
[34m[1mwandb[39m[22m: [33mWARNING[39m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'Train Loss': 1.5434736609458923, 'Validation Loss': 0.634831964969635, 'Train Accuracy': 0.625, 'Validation Accuracy': 0.800000011920929, '_timestamp': 1717512565.141363}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.weight': {'_type': 'histogram', 'values': [1748, 2348, 2379, 2372, 2362, 2487, 2379, 2444, 2461, 2433, 2504, 2478, 2370, 2421, 2404, 2423, 2466, 2435, 2405, 2467, 2495, 2429, 2494, 2368, 2439, 2374, 2429, 2465, 2444, 2396, 2461, 2340, 2403, 2428, 2388, 2448, 2321, 2418, 2454, 2401, 2359, 2447, 2458, 2484, 2425, 2460, 2363, 2402, 2400, 2396, 2435, 2368, 2411, 2530, 2429, 2341, 2488, 2371, 2396, 2449, 2438, 2410, 2388, 2071], 'bins': [-0.10655984282493591, -0.10322979092597961, -0.09989973157644272, -0.09656967967748642, -0.09323962032794952, -0.08990956842899323, -0.08657950907945633, -0.08324945718050003, -0.07991939783096313, -0.07658934593200684, -0.07325928658246994, -0.06992923468351364, -0.06659917533397675, -0.06326912343502045, -0.05993906408548355, -0.056609008461236954, -0.053278952836990356, -0.04994890093803406, -0.04661884531378746, -0.04328878968954086, -0.039958734065294266, -0.03662867844104767, -0.03329862281680107, -0.029968565329909325, -0.026638511568307877, -0.02330845594406128, -0.019978400319814682, -0.016648344695568085, -0.013318289071321487, -0.00998823344707489, -0.006658177822828293, -0.0033281221985816956, 1.9334256649017334e-06, 0.003331989049911499, 0.006662044674158096, 0.009992100298404694, 0.013322155922651291, 0.016652211546897888, 0.019982267171144485, 0.023312322795391083, 0.02664237841963768, 0.029972432181239128, 0.033302489668130875, 0.03663254529237747, 0.03996260091662407, 0.043292656540870667, 0.046622712165117264, 0.04995276778936386, 0.05328281968832016, 0.05661287531256676, 0.059942930936813354, 0.06327299028635025, 0.06660304218530655, 0.06993310153484344, 0.07326315343379974, 0.07659321278333664, 0.07992326468229294, 0.08325332403182983, 0.08658337593078613, 0.08991343528032303, 0.09324348717927933, 0.09657354652881622, 0.09990359842777252, 0.10323365777730942, 0.10656370967626572]}, '_timestamp': 1717512565.1460989}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.bias': {'_type': 'histogram', 'values': [2, 2, 5, 1, 0, 4, 2, 7, 6, 2, 2, 1, 3, 4, 3, 1, 1, 2, 7, 1, 2, 3, 2, 2, 3, 2, 1, 3, 3, 5, 2, 2, 5, 3, 3, 6, 2, 3, 4, 2, 3, 1, 4, 3, 3, 1, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 1, 2, 2, 4, 4, 0, 2], 'bins': [-0.032502319663763046, -0.03148460388183594, -0.030466891825199127, -0.02944917604327202, -0.02843146212399006, -0.0274137482047081, -0.02639603242278099, -0.02537831850349903, -0.02436060458421707, -0.023342890664935112, -0.022325176745653152, -0.021307460963726044, -0.020289747044444084, -0.019272033125162125, -0.018254317343235016, -0.017236603423953056, -0.016218889504671097, -0.015201175585389137, -0.014183460734784603, -0.013165745884180069, -0.01214803196489811, -0.01113031804561615, -0.010112603195011616, -0.009094888344407082, -0.008077174425125122, -0.007059460040181875, -0.006041745655238628, -0.0050240312702953815, -0.004006316885352135, -0.002988602500408888, -0.001970888115465641, -0.0009531737305223942, 6.454065442085266e-05, 0.0010822550393640995, 0.0020999694243073463, 0.003117683809250593, 0.00413539819419384, 0.005153112579137087, 0.006170826964080334, 0.0071885413490235806, 0.008206255733966827, 0.009223969653248787, 0.010241684503853321, 0.011259399354457855, 0.012277113273739815, 0.013294827193021774, 0.014312542043626308, 0.015330256894230843, 0.016347970813512802, 0.01736568473279476, 0.01838339865207672, 0.01940111443400383, 0.02041882835328579, 0.02143654227256775, 0.022454258054494858, 0.023471971973776817, 0.024489685893058777, 0.025507399812340736, 0.026525113731622696, 0.027542829513549805, 0.028560543432831764, 0.029578257352113724, 0.030595973134040833, 0.03161368519067764, 0.03263140097260475]}, '_timestamp': 1717512565.146379}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.weight': {'_type': 'histogram', 'values': [79, 8, 9, 5, 5, 3, 2, 1, 1, 0, 2, 0, 1, 1, 0, 3, 2, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 2, 0, 2, 3, 0, 1, 1, 1, 2, 0, 1, 2, 2, 2, 3, 0, 1, 0, 0, 0, 5, 12], 'bins': [0.9992799758911133, 0.9993025064468384, 0.9993249773979187, 0.9993475079536438, 0.9993699789047241, 0.9993925094604492, 0.9994149804115295, 0.9994375109672546, 0.999459981918335, 0.9994825124740601, 0.9995049834251404, 0.9995275139808655, 0.9995499849319458, 0.9995725154876709, 0.9995949864387512, 0.9996175169944763, 0.9996399879455566, 0.9996625185012817, 0.9996850490570068, 0.9997075200080872, 0.9997300505638123, 0.9997525215148926, 0.9997750520706177, 0.999797523021698, 0.9998200535774231, 0.9998425245285034, 0.9998650550842285, 0.9998875260353088, 0.9999100565910339, 0.9999325275421143, 0.9999550580978394, 0.9999775290489197, 1.0, 1.000022530555725, 1.0000450611114502, 1.0000675916671753, 1.0000901222229004, 1.000112533569336, 1.000135064125061, 1.0001575946807861, 1.0001801252365112, 1.0002025365829468, 1.0002250671386719, 1.000247597694397, 1.000270128250122, 1.0002925395965576, 1.0003150701522827, 1.0003376007080078, 1.000360131263733, 1.000382661819458, 1.0004050731658936, 1.0004276037216187, 1.0004501342773438, 1.0004726648330688, 1.0004950761795044, 1.0005176067352295, 1.0005401372909546, 1.0005626678466797, 1.0005850791931152, 1.0006076097488403, 1.0006301403045654, 1.0006526708602905, 1.000675082206726, 1.0006976127624512, 1.0007201433181763]}, '_timestamp': 1717512565.146577}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.bias': {'_type': 'histogram', 'values': [14, 3, 1, 2, 3, 18, 1, 2, 0, 2, 0, 3, 1, 1, 1, 0, 0, 0, 0, 1, 3, 0, 0, 0, 2, 1, 2, 1, 0, 0, 0, 0, 47, 0, 0, 0, 1, 0, 1, 2, 0, 1, 0, 3, 1, 0, 4, 0, 1, 1, 1, 1, 2, 0, 0, 1, 2, 2, 13, 1, 1, 1, 6, 16], 'bins': [-0.0007201287662610412, -0.0006976251024752855, -0.000675121380481869, -0.0006526177166961133, -0.0006301139947026968, -0.0006076103309169412, -0.0005851066089235246, -0.000562602945137769, -0.0005400992231443524, -0.0005175955593585968, -0.0004950918373651803, -0.00047258814447559416, -0.00045008445158600807, -0.000427580758696422, -0.0004050770658068359, -0.0003825733729172498, -0.0003600696800276637, -0.0003375660162419081, -0.000315062323352322, -0.0002925586304627359, -0.0002700549375731498, -0.0002475512446835637, -0.0002250475372420624, -0.0002025438443524763, -0.00018004016601480544, -0.00015753647312521935, -0.00013503278023563325, -0.00011252908007008955, -9.002539445646107e-05, -6.752170156687498e-05, -4.501800867728889e-05, -2.25143157877028e-05, -1.0622898116707802e-08, 2.2493069991469383e-05, 4.4996762881055474e-05, 6.750045577064157e-05, 9.000414866022766e-05, 0.00011250783427385613, 0.00013501153443939984, 0.00015751522732898593, 0.00018001892021857202, 0.00020252259855624288, 0.00022502629144582897, 0.0002475299988873303, 0.0002700336917769164, 0.0002925373846665025, 0.00031504107755608857, 0.00033754477044567466, 0.0003600484342314303, 0.0003825521271210164, 0.0004050558200106025, 0.00042755951290018857, 0.00045006320578977466, 0.00047256689867936075, 0.0004950705915689468, 0.0005175743135623634, 0.000540077977348119, 0.0005625816993415356, 0.0005850853631272912, 0.0006075890851207078, 0.0006300927489064634, 0.0006525964708998799, 0.0006751001346856356, 0.0006976038566790521, 0.0007201075204648077]}, '_timestamp': 1717512565.1467462}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.weight': {'_type': 'histogram', 'values': [379, 479, 492, 491, 446, 464, 454, 481, 434, 484, 440, 487, 434, 440, 452, 451, 461, 440, 488, 488, 450, 478, 465, 490, 445, 445, 445, 458, 439, 482, 481, 452, 436, 423, 437, 442, 440, 449, 438, 459, 482, 450, 464, 471, 469, 476, 459, 447, 447, 482, 424, 461, 476, 473, 455, 507, 476, 489, 449, 437, 422, 447, 467, 372], 'bins': [-0.18789789080619812, -0.18202617764472961, -0.1761544644832611, -0.1702827513217926, -0.1644110381603241, -0.1585393249988556, -0.15266761183738708, -0.14679589867591858, -0.14092418551445007, -0.13505247235298157, -0.12918075919151306, -0.12330905348062515, -0.11743734031915665, -0.11156562715768814, -0.10569391399621964, -0.09982220083475113, -0.09395048767328262, -0.08807877451181412, -0.08220706135034561, -0.0763353481888771, -0.0704636350274086, -0.0645919218659401, -0.05872021242976189, -0.05284849926829338, -0.046976786106824875, -0.04110507294535637, -0.03523335978388786, -0.029361648485064507, -0.023489935323596, -0.017618222162127495, -0.011746509931981564, -0.005874797236174345, -3.084540367126465e-06, 0.005868628155440092, 0.01174034085124731, 0.017612053081393242, 0.023483766242861748, 0.029355479404330254, 0.03522719070315361, 0.041098903864622116, 0.04697061702609062, 0.05284233018755913, 0.058714043349027634, 0.06458575278520584, 0.07045746594667435, 0.07632917910814285, 0.08220089226961136, 0.08807260543107986, 0.09394431859254837, 0.09981603175401688, 0.10568774491548538, 0.11155945807695389, 0.1174311712384224, 0.1233028843998909, 0.1291745901107788, 0.13504630327224731, 0.14091801643371582, 0.14678972959518433, 0.15266144275665283, 0.15853315591812134, 0.16440486907958984, 0.17027658224105835, 0.17614829540252686, 0.18202000856399536, 0.18789172172546387]}, '_timestamp': 1717512565.147098}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.bias': {'_type': 'histogram', 'values': [4, 3, 3, 1, 1, 2, 2, 3, 7, 2, 6, 0, 0, 2, 4, 2, 3, 1, 3, 2, 2, 4, 5, 3, 4, 6, 0, 2, 2, 3, 4, 4, 1, 2, 4, 3, 2, 3, 1, 0, 5, 1, 6, 3, 4, 3, 3, 4, 0, 4, 2, 6, 2, 1, 2, 2, 1, 2, 3, 2, 2, 0, 3, 4], 'bins': [-0.0737152174115181, -0.07136039435863495, -0.0690055787563324, -0.06665075570344925, -0.0642959326505661, -0.06194111332297325, -0.0595862902700901, -0.05723147094249725, -0.054876647889614105, -0.052521828562021255, -0.050167009234428406, -0.04781218618154526, -0.04545736685395241, -0.04310254380106926, -0.04074772447347641, -0.03839290142059326, -0.03603808209300041, -0.03368326276540756, -0.031328439712524414, -0.028973618522286415, -0.026618797332048416, -0.024263978004455566, -0.021909156814217567, -0.01955433562397957, -0.01719951443374157, -0.01484469324350357, -0.012489872053265572, -0.010135051794350147, -0.007780230604112148, -0.005425409413874149, -0.0030705886892974377, -0.0007157677318900824, 0.001639053225517273, 0.003993874415755272, 0.0063486951403319836, 0.008703515864908695, 0.011058337055146694, 0.013413158245384693, 0.015767978504300117, 0.018122799694538116, 0.020477620884776115, 0.022832442075014114, 0.025187263265252113, 0.027542084455490112, 0.029896903783082962, 0.03225172683596611, 0.03460654616355896, 0.03696136921644211, 0.03931618854403496, 0.04167100787162781, 0.044025830924510956, 0.046380650252103806, 0.048735473304986954, 0.0510902926325798, 0.05344511568546295, 0.0557999350130558, 0.05815475434064865, 0.0605095773935318, 0.06286440044641495, 0.0652192160487175, 0.06757403910160065, 0.0699288621544838, 0.07228368520736694, 0.0746385008096695, 0.07699332386255264]}, '_timestamp': 1717512565.147264}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.weight': {'_type': 'histogram', 'values': [29, 5, 17, 10, 3, 4, 5, 4, 2, 2, 2, 1, 2, 0, 0, 0, 2, 2, 0, 1, 1, 0, 1, 1, 1, 3, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 5, 1, 1, 1, 2, 0, 5, 1, 0, 5, 1, 1, 0, 0, 0, 3, 1, 6, 5, 1, 2, 3, 3, 5, 13], 'bins': [0.9992799162864685, 0.9993023872375488, 0.9993249177932739, 0.9993473887443542, 0.9993698596954346, 0.9993923902511597, 0.99941486120224, 0.9994373917579651, 0.9994598627090454, 0.9994823336601257, 0.9995048642158508, 0.9995273351669312, 0.9995498061180115, 0.9995723366737366, 0.9995948076248169, 0.9996172785758972, 0.9996398091316223, 0.9996622800827026, 0.999684751033783, 0.9997072815895081, 0.9997297525405884, 0.9997522830963135, 0.9997747540473938, 0.9997972249984741, 0.9998197555541992, 0.9998422265052795, 0.9998646974563599, 0.999887228012085, 0.9999096989631653, 0.9999321699142456, 0.9999547004699707, 0.999977171421051, 0.9999996423721313, 1.0000221729278564, 1.0000447034835815, 1.000067114830017, 1.0000896453857422, 1.0001121759414673, 1.0001345872879028, 1.000157117843628, 1.000179648399353, 1.0002020597457886, 1.0002245903015137, 1.0002471208572388, 1.0002695322036743, 1.0002920627593994, 1.0003145933151245, 1.00033700466156, 1.0003595352172852, 1.0003820657730103, 1.0004044771194458, 1.000427007675171, 1.000449538230896, 1.000472068786621, 1.0004944801330566, 1.0005170106887817, 1.0005395412445068, 1.0005619525909424, 1.0005844831466675, 1.0006070137023926, 1.0006294250488281, 1.0006519556045532, 1.0006744861602783, 1.0006968975067139, 1.000719428062439]}, '_timestamp': 1717512565.147421}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.bias': {'_type': 'histogram', 'values': [16, 5, 4, 6, 2, 13, 4, 3, 0, 1, 2, 0, 2, 3, 1, 1, 2, 0, 0, 3, 1, 1, 1, 1, 1, 0, 1, 2, 0, 0, 0, 5, 0, 0, 0, 0, 1, 3, 1, 3, 3, 2, 1, 1, 2, 0, 1, 1, 0, 4, 4, 2, 4, 0, 1, 0, 1, 1, 10, 3, 4, 8, 8, 21], 'bins': [-0.000719627074431628, -0.0006971380207687616, -0.0006746489088982344, -0.000652159855235368, -0.0006296708015725017, -0.0006071816897019744, -0.000584692636039108, -0.0005622035241685808, -0.0005397144705057144, -0.0005172254168428481, -0.0004947363049723208, -0.00047224725130945444, -0.00044975816854275763, -0.0004272691148798913, -0.00040478003211319447, -0.00038229094934649765, -0.00035980186657980084, -0.0003373128129169345, -0.0003148237301502377, -0.00029233464738354087, -0.0002698455937206745, -0.0002473565109539777, -0.0002248674281872809, -0.0002023783599724993, -0.00017988929175771773, -0.00015740020899102092, -0.00013491114077623934, -0.00011242205800954252, -8.993298979476094e-05, -6.744391430402175e-05, -4.495483881328255e-05, -2.2465763322543353e-05, 2.3312168195843697e-08, 2.251238765893504e-05, 4.500146314967424e-05, 6.749053864041343e-05, 8.997961413115263e-05, 0.00011246868234593421, 0.00013495776511263102, 0.0001574468333274126, 0.00017993591609410942, 0.000202424984308891, 0.00022491405252367258, 0.0002474031352903694, 0.0002698922180570662, 0.00029238127171993256, 0.00031487035448662937, 0.0003373594372533262, 0.00035984849091619253, 0.00038233757368288934, 0.00040482665644958615, 0.00042731573921628296, 0.0004498047928791493, 0.00047229387564584613, 0.0004947829293087125, 0.0005172720411792397, 0.0005397610948421061, 0.0005622501485049725, 0.0005847392603754997, 0.0006072283140383661, 0.0006297174259088933, 0.0006522064795717597, 0.0006746955332346261, 0.0006971846451051533, 0.0007196736987680197]}, '_timestamp': 1717512565.147692}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.weight': {'_type': 'histogram', 'values': [361, 479, 475, 469, 462, 486, 504, 503, 474, 411, 449, 447, 440, 459, 458, 494, 436, 478, 451, 474, 440, 476, 430, 489, 462, 437, 436, 419, 471, 425, 472, 471, 451, 478, 474, 446, 414, 465, 465, 473, 465, 454, 447, 457, 462, 467, 414, 507, 455, 446, 412, 434, 429, 466, 492, 483, 431, 482, 421, 485, 487, 463, 454, 424], 'bins': [-0.1878998577594757, -0.18203012645244598, -0.17616038024425507, -0.17029064893722534, -0.16442090272903442, -0.1585511714220047, -0.15268142521381378, -0.14681169390678406, -0.14094194769859314, -0.13507221639156342, -0.1292024701833725, -0.12333273887634277, -0.11746300011873245, -0.11159326136112213, -0.10572352260351181, -0.09985378384590149, -0.09398404508829117, -0.08811430633068085, -0.08224456757307053, -0.0763748288154602, -0.07050509005784988, -0.06463535130023956, -0.05876561626791954, -0.05289587751030922, -0.0470261387526989, -0.04115639999508858, -0.035286661237478256, -0.029416924342513084, -0.023547185584902763, -0.017677446827292442, -0.011807709001004696, -0.005937970709055662, -6.823241710662842e-05, 0.005801505874842405, 0.011671244166791439, 0.017540981993079185, 0.023410720750689507, 0.029280459508299828, 0.035150196403265, 0.04101993516087532, 0.04688967391848564, 0.05275941267609596, 0.058629151433706284, 0.0644988864660263, 0.07036862522363663, 0.07623836398124695, 0.08210810273885727, 0.08797784149646759, 0.09384758025407791, 0.09971731901168823, 0.10558705776929855, 0.11145679652690887, 0.1173265352845192, 0.12319627404212952, 0.12906600534915924, 0.13493575155735016, 0.14080548286437988, 0.1466752290725708, 0.15254496037960052, 0.15841470658779144, 0.16428443789482117, 0.17015418410301208, 0.1760239154100418, 0.18189366161823273, 0.18776339292526245]}, '_timestamp': 1717512565.148143}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.bias': {'_type': 'histogram', 'values': [1, 3, 2, 2, 2, 7, 4, 3, 2, 1, 1, 0, 1, 4, 4, 5, 4, 3, 0, 3, 3, 3, 2, 3, 2, 3, 4, 1, 3, 3, 5, 1, 2, 2, 4, 3, 2, 4, 3, 4, 3, 0, 0, 2, 4, 2, 0, 4, 4, 3, 4, 3, 0, 2, 3, 5, 4, 3, 3, 3, 3, 2, 1, 4], 'bins': [-0.07667624950408936, -0.07429748773574829, -0.07191872596740723, -0.06953996419906616, -0.0671612024307251, -0.06478244066238403, -0.06240367889404297, -0.060024917125701904, -0.05764615535736084, -0.055267393589019775, -0.05288863182067871, -0.050509870052337646, -0.04813110828399658, -0.04575234651565552, -0.04337358474731445, -0.04099482297897339, -0.038616061210632324, -0.03623729571700096, -0.0338585339486599, -0.03147977218031883, -0.029101012274622917, -0.026722250506281853, -0.02434348873794079, -0.021964726969599724, -0.01958596333861351, -0.017207201570272446, -0.014828440733253956, -0.012449678964912891, -0.010070916265249252, -0.007692154962569475, -0.005313392728567123, -0.002934630960226059, -0.0005558691918849945, 0.0018228926928713918, 0.004201654344797134, 0.006580416578799486, 0.008959177881479263, 0.011337940581142902, 0.013716702349483967, 0.016095463186502457, 0.01847422495484352, 0.020852988585829735, 0.0232317503541708, 0.025610512122511864, 0.027989273890852928, 0.030368035659193993, 0.03274679556488991, 0.03512555733323097, 0.037504322826862335, 0.0398830845952034, 0.042261846363544464, 0.04464060813188553, 0.04701936990022659, 0.04939813166856766, 0.05177689343690872, 0.054155655205249786, 0.05653441697359085, 0.058913178741931915, 0.06129194051027298, 0.06367070227861404, 0.06604946404695511, 0.06842822581529617, 0.07080698758363724, 0.0731857493519783, 0.07556451112031937]}, '_timestamp': 1717512565.14833}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.weight': {'_type': 'histogram', 'values': [34, 15, 7, 8, 7, 9, 5, 2, 1, 1, 3, 4, 0, 1, 2, 0, 2, 1, 2, 2, 0, 0, 3, 1, 2, 4, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 3, 2, 1, 0, 1, 1, 0, 2, 1, 0, 1, 0, 0, 0, 1, 5, 2, 0, 2, 4, 3, 2, 4, 3, 4, 4], 'bins': [0.9992799162864685, 0.9993023872375488, 0.9993249177932739, 0.9993473887443542, 0.9993699193000793, 0.9993923902511597, 0.9994149208068848, 0.9994373917579651, 0.9994599223136902, 0.9994823932647705, 0.9995049238204956, 0.9995273947715759, 0.999549925327301, 0.9995723962783813, 0.9995949268341064, 0.9996173977851868, 0.9996399283409119, 0.9996623992919922, 0.9996849298477173, 0.9997074007987976, 0.9997299313545227, 0.999752402305603, 0.9997749328613281, 0.9997974038124084, 0.9998199343681335, 0.9998424053192139, 0.999864935874939, 0.9998874068260193, 0.9999099373817444, 0.9999324083328247, 0.9999549388885498, 0.9999774098396301, 0.9999998807907104, 1.0000224113464355, 1.0000449419021606, 1.0000673532485962, 1.0000898838043213, 1.0001124143600464, 1.0001349449157715, 1.000157356262207, 1.0001798868179321, 1.0002024173736572, 1.0002249479293823, 1.0002473592758179, 1.000269889831543, 1.000292420387268, 1.0003149509429932, 1.0003373622894287, 1.0003598928451538, 1.000382423400879, 1.000404953956604, 1.0004273653030396, 1.0004498958587646, 1.0004724264144897, 1.0004949569702148, 1.0005173683166504, 1.0005398988723755, 1.0005624294281006, 1.0005849599838257, 1.0006073713302612, 1.0006299018859863, 1.0006524324417114, 1.0006749629974365, 1.000697374343872, 1.0007199048995972]}, '_timestamp': 1717512565.1484969}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.bias': {'_type': 'histogram', 'values': [21, 8, 7, 7, 2, 9, 8, 3, 3, 4, 4, 2, 1, 0, 2, 0, 1, 0, 2, 4, 2, 0, 1, 2, 3, 3, 2, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 2, 0, 0, 1, 0, 2, 3, 1, 1, 2, 1, 1, 0, 2, 0, 0, 4, 3, 3, 1, 11, 0, 7, 8, 3, 8], 'bins': [-0.0007200082181952894, -0.0006975087453611195, -0.0006750092725269496, -0.0006525097414851189, -0.000630010268650949, -0.0006075107958167791, -0.0005850113229826093, -0.0005625118501484394, -0.0005400123191066086, -0.0005175128462724388, -0.0004950133734382689, -0.00047251390060409904, -0.0004500143986660987, -0.00042751492583192885, -0.00040501542389392853, -0.00038251595105975866, -0.0003600164782255888, -0.0003375169762875885, -0.0003150175034534186, -0.0002925180015154183, -0.0002700185286812484, -0.0002475190267432481, -0.00022501955390907824, -0.00020252006652299315, -0.00018002057913690805, -0.0001575211063027382, -0.0001350216189166531, -0.000112522131530568, -9.002264414448291e-05, -6.752316403435543e-05, -4.502367664827034e-05, -2.2524192900164053e-05, -2.4709152057766914e-08, 2.247477459604852e-05, 4.4974258344154805e-05, 6.74737457302399e-05, 8.997322584036738e-05, 0.00011247271322645247, 0.00013497220061253756, 0.00015747168799862266, 0.00017997116083279252, 0.0002024706482188776, 0.0002249701356049627, 0.00024746960843913257, 0.0002699691103771329, 0.00029246858321130276, 0.0003149680851493031, 0.00033746755798347294, 0.00035996705992147326, 0.00038246653275564313, 0.000404966005589813, 0.0004274655075278133, 0.0004499649803619832, 0.0004724644822999835, 0.0004949639551341534, 0.0005174634279683232, 0.0005399629008024931, 0.0005624624318443239, 0.0005849619046784937, 0.0006074613775126636, 0.0006299608503468335, 0.0006524603231810033, 0.0006749598542228341, 0.000697459327057004, 0.0007199587998911738]}, '_timestamp': 1717512565.148652}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.weight': {'_type': 'histogram', 'values': [4, 5, 2, 1, 3, 3, 4, 2, 3, 7, 1, 2, 5, 1, 3, 1, 4, 5, 1, 1, 4, 0, 3, 3, 2, 1, 2, 1, 3, 1, 1, 4, 3, 2, 2, 1, 3, 1, 4, 3, 2, 1, 5, 4, 1, 3, 5, 1, 6, 0, 2, 5, 2, 2, 0, 4, 1, 7, 2, 2, 6, 1, 4, 3], 'bins': [-0.2630681097507477, -0.2548379898071289, -0.24660785496234894, -0.23837772011756897, -0.2301476001739502, -0.22191748023033142, -0.21368734538555145, -0.20545721054077148, -0.1972270905971527, -0.18899697065353394, -0.18076683580875397, -0.172536700963974, -0.16430658102035522, -0.15607646107673645, -0.14784632623195648, -0.1396161913871765, -0.13138607144355774, -0.12315594404935837, -0.114925816655159, -0.10669568926095963, -0.09846556186676025, -0.09023543447256088, -0.08200530707836151, -0.07377517968416214, -0.06554505228996277, -0.0573149248957634, -0.049084797501564026, -0.040854670107364655, -0.03262454271316528, -0.024394415318965912, -0.01616428792476654, -0.00793416053056717, 0.00029596686363220215, 0.008526094257831573, 0.016756221652030945, 0.024986349046230316, 0.03321647644042969, 0.04144660383462906, 0.04967673122882843, 0.0579068586230278, 0.06613698601722717, 0.07436711341142654, 0.08259724080562592, 0.09082736819982529, 0.09905749559402466, 0.10728762298822403, 0.1155177503824234, 0.12374787777662277, 0.13197800517082214, 0.14020812511444092, 0.1484382599592209, 0.15666839480400085, 0.16489851474761963, 0.1731286346912384, 0.18135876953601837, 0.18958890438079834, 0.19781902432441711, 0.2060491442680359, 0.21427927911281586, 0.22250941395759583, 0.2307395339012146, 0.23896965384483337, 0.24719978868961334, 0.2554299235343933, 0.2636600434780121]}, '_timestamp': 1717512565.1488159}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.bias': {'_type': 'histogram', 'values': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'bins': [-0.5431917905807495, -0.5275667905807495, -0.5119417905807495, -0.4963167905807495, -0.4806917905807495, -0.4650667905807495, -0.4494417905807495, -0.4338167905807495, -0.4181917905807495, -0.4025667905807495, -0.3869417905807495, -0.3713167905807495, -0.3556917905807495, -0.3400667905807495, -0.3244417905807495, -0.3088167905807495, -0.2931917905807495, -0.2775667905807495, -0.2619417905807495, -0.2463167905807495, -0.2306917905807495, -0.2150667905807495, -0.1994417905807495, -0.1838167905807495, -0.1681917905807495, -0.1525667905807495, -0.1369417905807495, -0.12131679803133011, -0.10569179803133011, -0.09006679803133011, -0.07444179803133011, -0.05881679803133011, -0.04319179803133011, -0.02756679803133011, -0.011941798031330109, 0.0036832019686698914, 0.01930820196866989, 0.03493320196866989, 0.05055820196866989, 0.06618320196866989, 0.08180820196866989, 0.09743320196866989, 0.11305820196866989, 0.1286832094192505, 0.1443082094192505, 0.1599332094192505, 0.1755582094192505, 0.1911832094192505, 0.2068082094192505, 0.2224332094192505, 0.2380582094192505, 0.2536832094192505, 0.2693082094192505, 0.2849332094192505, 0.3005582094192505, 0.3161832094192505, 0.3318082094192505, 0.3474332094192505, 0.3630582094192505, 0.3786832094192505, 0.3943082094192505, 0.4099332094192505, 0.4255582094192505, 0.4411832094192505, 0.4568082094192505]}, '_timestamp': 1717512565.148994}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TP Reward Distribution': {'_type': 'histogram', 'values': [6, 5, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [3.3333333333333335, 4.93125, 6.529166666666667, 8.127083333333333, 9.725, 11.322916666666666, 12.920833333333334, 14.51875, 16.116666666666667, 17.714583333333334, 19.3125, 20.910416666666666, 22.508333333333333, 24.10625, 25.704166666666666, 27.302083333333332, 28.9, 30.497916666666665, 32.09583333333333, 33.69375, 35.291666666666664, 36.889583333333334, 38.487500000000004, 40.08541666666667, 41.68333333333334, 43.28125, 44.87916666666667, 46.47708333333333, 48.075, 49.672916666666666, 51.270833333333336, 52.86875, 54.46666666666667, 56.06458333333334, 57.6625, 59.26041666666667, 60.858333333333334, 62.456250000000004, 64.05416666666666, 65.65208333333334, 67.25, 68.84791666666666, 70.44583333333333, 72.04374999999999, 73.64166666666667, 75.23958333333333, 76.83749999999999, 78.43541666666665, 80.03333333333333, 81.63125, 83.22916666666666, 84.82708333333333, 86.425, 88.02291666666666, 89.62083333333332, 91.21875, 92.81666666666666, 94.41458333333333, 96.01249999999999, 97.61041666666667, 99.20833333333333, 100.80624999999999, 102.40416666666665, 104.00208333333333, 105.6]}, '_timestamp': 1717512565.149324}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TN Reward Distribution': {'_type': 'histogram', 'values': [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1], 'bins': [3.3333333333333335, 3.379166666666667, 3.4250000000000003, 3.4708333333333337, 3.5166666666666666, 3.5625, 3.6083333333333334, 3.654166666666667, 3.7, 3.7458333333333336, 3.791666666666667, 3.8375000000000004, 3.8833333333333333, 3.9291666666666667, 3.975, 4.020833333333334, 4.066666666666666, 4.1125, 4.158333333333333, 4.204166666666667, 4.25, 4.295833333333333, 4.341666666666667, 4.3875, 4.433333333333334, 4.479166666666667, 4.525, 4.570833333333333, 4.616666666666667, 4.6625, 4.708333333333334, 4.754166666666666, 4.8, 4.845833333333333, 4.891666666666667, 4.9375, 4.983333333333333, 5.029166666666667, 5.075, 5.120833333333334, 5.166666666666667, 5.2125, 5.258333333333333, 5.304166666666667, 5.35, 5.395833333333334, 5.441666666666666, 5.4875, 5.533333333333333, 5.579166666666667, 5.625, 5.670833333333333, 5.716666666666667, 5.7625, 5.808333333333334, 5.854166666666666, 5.9, 5.945833333333333, 5.991666666666667, 6.0375, 6.083333333333334, 6.129166666666666, 6.175, 6.220833333333333, 6.266666666666667]}, '_timestamp': 1717512565.1494741}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FP Reward Distribution': {'_type': 'histogram', 'values': [2, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [2.6666666666666665, 7.3125, 11.958333333333332, 16.604166666666668, 21.25, 25.895833333333332, 30.541666666666668, 35.18749999999999, 39.83333333333333, 44.479166666666664, 49.12499999999999, 53.77083333333333, 58.416666666666664, 63.06249999999999, 67.70833333333333, 72.35416666666667, 77.0, 81.64583333333333, 86.29166666666667, 90.9375, 95.58333333333333, 100.22916666666667, 104.875, 109.52083333333333, 114.16666666666667, 118.8125, 123.45833333333333, 128.10416666666666, 132.74999999999997, 137.39583333333331, 142.04166666666666, 146.68749999999997, 151.33333333333331, 155.97916666666666, 160.62499999999997, 165.27083333333331, 169.91666666666666, 174.56249999999997, 179.20833333333331, 183.85416666666666, 188.49999999999997, 193.14583333333331, 197.79166666666666, 202.43749999999997, 207.08333333333331, 211.72916666666666, 216.37499999999997, 221.02083333333331, 225.66666666666666, 230.31249999999997, 234.95833333333331, 239.60416666666663, 244.24999999999997, 248.89583333333331, 253.54166666666663, 258.1875, 262.8333333333333, 267.4791666666667, 272.125, 276.7708333333333, 281.4166666666667, 286.0625, 290.7083333333333, 295.3541666666667, 300.0]}, '_timestamp': 1717512565.149614}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FN Reward Distribution': {'_type': 'histogram', 'values': [1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [2.6666666666666665, 3.048958333333333, 3.43125, 3.8135416666666666, 4.195833333333333, 4.578125, 4.960416666666666, 5.3427083333333325, 5.725, 6.107291666666667, 6.489583333333333, 6.871874999999999, 7.254166666666666, 7.636458333333334, 8.018749999999999, 8.401041666666666, 8.783333333333333, 9.165624999999999, 9.547916666666666, 9.930208333333333, 10.3125, 10.694791666666665, 11.077083333333333, 11.459375, 11.841666666666665, 12.223958333333332, 12.60625, 12.988541666666665, 13.370833333333332, 13.753124999999999, 14.135416666666666, 14.517708333333331, 14.899999999999999, 15.282291666666666, 15.664583333333331, 16.046875, 16.429166666666667, 16.811458333333334, 17.193749999999998, 17.576041666666665, 17.958333333333332, 18.340625, 18.722916666666666, 19.105208333333334, 19.4875, 19.869791666666668, 20.252083333333335, 20.634375, 21.016666666666666, 21.398958333333333, 21.78125, 22.163541666666667, 22.545833333333334, 22.928125, 23.310416666666665, 23.692708333333332, 24.075, 24.457291666666666, 24.839583333333334, 25.221875, 25.604166666666668, 25.98645833333333, 26.36875, 26.751041666666666, 27.133333333333333]}, '_timestamp': 1717512565.149759}).
[33m[W 2024-06-04 10:49:39,850][39m Trial 0 failed with parameters: {'hidden_size': 171, 'learning_rate': 0.0003600007732238597, 'weight_decay': 0.00017767448934505762} because of the following error: The value None could not be cast to float..
[33m[W 2024-06-04 10:49:39,851][39m Trial 0 failed with value None.