Epoch 0/1000, Train Loss: 2.841714859008789, Val Loss: 0.6748658418655396, Train Acc: 0.325, Val Acc: 0.8999999761581421, LR: 0.0007179551902064997
Epoch 10/1000, Train Loss: 2.478912591934204, Val Loss: 0.6096727252006531, Train Acc: 0.5, Val Acc: 0.8999999761581421, LR: 0.0007037114748840726
Epoch 20/1000, Train Loss: 1.5162206888198853, Val Loss: 0.606255054473877, Train Acc: 0.55, Val Acc: 0.8999999761581421, LR: 0.0006894677595616455
Epoch 30/1000, Train Loss: 1.921895980834961, Val Loss: 0.6105492115020752, Train Acc: 0.475, Val Acc: 0.8999999761581421, LR: 0.0006752240442392181
Epoch 40/1000, Train Loss: 1.9188251495361328, Val Loss: 0.5974339842796326, Train Acc: 0.475, Val Acc: 0.8999999761581421, LR: 0.0006609803289167909
Epoch 50/1000, Train Loss: 1.3386875987052917, Val Loss: 0.575722336769104, Train Acc: 0.6, Val Acc: 0.8999999761581421, LR: 0.0006467366135943635
Epoch 60/1000, Train Loss: 1.288757562637329, Val Loss: 0.5661036372184753, Train Acc: 0.625, Val Acc: 0.8999999761581421, LR: 0.000632492898271936
Epoch 70/1000, Train Loss: 1.2396512031555176, Val Loss: 0.5220905542373657, Train Acc: 0.6, Val Acc: 0.8999999761581421, LR: 0.0006182491829495086
Epoch 80/1000, Train Loss: 1.1336175203323364, Val Loss: 0.43865737318992615, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 0.0006040054676270812
Epoch 90/1000, Train Loss: 1.3663760423660278, Val Loss: 0.40958061814308167, Train Acc: 0.575, Val Acc: 0.8999999761581421, LR: 0.000589761752304654
Epoch 100/1000, Train Loss: 1.4802168011665344, Val Loss: 0.3922301232814789, Train Acc: 0.6, Val Acc: 0.8999999761581421, LR: 0.0005755180369822266
Epoch 110/1000, Train Loss: 1.2165838479995728, Val Loss: 0.3520239293575287, Train Acc: 0.625, Val Acc: 0.8999999761581421, LR: 0.0005612743216597996
Epoch 120/1000, Train Loss: 0.9143255949020386, Val Loss: 0.3317928910255432, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 0.0005470306063373721
Epoch 130/1000, Train Loss: 0.7196091860532761, Val Loss: 0.33146801590919495, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 0.000532786891014945
Epoch 140/1000, Train Loss: 0.8768872320652008, Val Loss: 0.32238954305648804, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.0005185431756925178
Epoch 150/1000, Train Loss: 0.8878235816955566, Val Loss: 0.3135256767272949, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 0.0005042994603700903
Epoch 160/1000, Train Loss: 1.0525302588939667, Val Loss: 0.29857054352760315, Train Acc: 0.65, Val Acc: 0.8999999761581421, LR: 0.000490055745047663
Epoch 170/1000, Train Loss: 0.7500316798686981, Val Loss: 0.29381251335144043, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 0.000475812029725236
Epoch 180/1000, Train Loss: 1.2026947736740112, Val Loss: 0.28519731760025024, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.00046156831440280873
Epoch 190/1000, Train Loss: 1.151152342557907, Val Loss: 0.2889750599861145, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.0004473245990803817
Epoch 200/1000, Train Loss: 1.1485430002212524, Val Loss: 0.2897869348526001, Train Acc: 0.7, Val Acc: 0.8999999761581421, LR: 0.00043308088375795453
Epoch 210/1000, Train Loss: 1.056419849395752, Val Loss: 0.2756800055503845, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 0.0004188371684355275
Epoch 220/1000, Train Loss: 0.9441900849342346, Val Loss: 0.2823360860347748, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 0.0004045934531131006
Epoch 230/1000, Train Loss: 0.9915547370910645, Val Loss: 0.29966193437576294, Train Acc: 0.625, Val Acc: 0.8999999761581421, LR: 0.0003903497377906736
Epoch 240/1000, Train Loss: 1.0748609900474548, Val Loss: 0.27642956376075745, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 0.00037610602246824663
Epoch 250/1000, Train Loss: 1.1199372112751007, Val Loss: 0.284128338098526, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.0003618623071458196
Epoch 260/1000, Train Loss: 0.9928120374679565, Val Loss: 0.30609720945358276, Train Acc: 0.675, Val Acc: 0.8999999761581421, LR: 0.0003476185918233928
Epoch 270/1000, Train Loss: 0.9009795188903809, Val Loss: 0.2614819407463074, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.00033337487650096593
Epoch 280/1000, Train Loss: 0.8535357713699341, Val Loss: 0.2932524085044861, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.0003191311611785391
Epoch 290/1000, Train Loss: 0.8625304400920868, Val Loss: 0.28354424238204956, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 0.0003048874458561122
Epoch 300/1000, Train Loss: 0.5565976947546005, Val Loss: 0.2774616777896881, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 0.00029064373053368524
Epoch 310/1000, Train Loss: 0.8609411716461182, Val Loss: 0.29398131370544434, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.00027640001521125825
Epoch 320/1000, Train Loss: 1.0770383775234222, Val Loss: 0.28331896662712097, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.0002621562998888313
Epoch 330/1000, Train Loss: 0.8460873961448669, Val Loss: 0.2777983248233795, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.00024791258456640443
Epoch 340/1000, Train Loss: 1.0771133303642273, Val Loss: 0.2710971534252167, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 0.00023366886924397747
Epoch 350/1000, Train Loss: 0.8059109449386597, Val Loss: 0.29163485765457153, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 0.00021942515392155045
Epoch 360/1000, Train Loss: 0.8087536096572876, Val Loss: 0.2728695869445801, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 0.00020518143859912333
Epoch 370/1000, Train Loss: 0.8650439977645874, Val Loss: 0.2720748782157898, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.0001909377232766964
Epoch 380/1000, Train Loss: 1.0685233771800995, Val Loss: 0.2705681324005127, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 0.00017669400795426935
Epoch 390/1000, Train Loss: 0.6644872725009918, Val Loss: 0.27288421988487244, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 0.0001624502926318422
Epoch 400/1000, Train Loss: 0.7867764532566071, Val Loss: 0.2732391059398651, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 0.00014820657730941507
Epoch 410/1000, Train Loss: 1.0897707045078278, Val Loss: 0.28017115592956543, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.000133962861986988
Epoch 420/1000, Train Loss: 0.8008897006511688, Val Loss: 0.28699031472206116, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 0.00011971914666456085
Epoch 430/1000, Train Loss: 0.7883657217025757, Val Loss: 0.2818393111228943, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.00010547543134213387
Epoch 440/1000, Train Loss: 0.9604303538799286, Val Loss: 0.28404754400253296, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 9.123171601970686e-05
Epoch 450/1000, Train Loss: 0.8860898911952972, Val Loss: 0.27896249294281006, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 7.698800069727986e-05
Epoch 460/1000, Train Loss: 0.9036564081907272, Val Loss: 0.272372305393219, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 6.274428537485287e-05
Epoch 470/1000, Train Loss: 0.8222445845603943, Val Loss: 0.2709803879261017, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 4.850057005242583e-05
Epoch 480/1000, Train Loss: 0.7239130139350891, Val Loss: 0.27776193618774414, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 3.425685472999877e-05
Epoch 490/1000, Train Loss: 0.8996328115463257, Val Loss: 0.2790652811527252, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 2.0013139407571732e-05
Epoch 500/1000, Train Loss: 0.7043393850326538, Val Loss: 0.2792828679084778, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 510/1000, Train Loss: 0.5763228982686996, Val Loss: 0.2780180871486664, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 520/1000, Train Loss: 0.9078940749168396, Val Loss: 0.2772214412689209, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 530/1000, Train Loss: 0.7859804034233093, Val Loss: 0.27741190791130066, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 540/1000, Train Loss: 0.8239282667636871, Val Loss: 0.2773398160934448, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 550/1000, Train Loss: 0.6141999363899231, Val Loss: 0.27708134055137634, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 560/1000, Train Loss: 0.8782444596290588, Val Loss: 0.27576422691345215, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 570/1000, Train Loss: 0.8763274848461151, Val Loss: 0.27535420656204224, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 580/1000, Train Loss: 0.7866550385951996, Val Loss: 0.27604129910469055, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 590/1000, Train Loss: 0.9170635342597961, Val Loss: 0.2759379744529724, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 600/1000, Train Loss: 1.0716473758220673, Val Loss: 0.2768173813819885, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 610/1000, Train Loss: 0.6726854741573334, Val Loss: 0.27753502130508423, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 620/1000, Train Loss: 0.8975315093994141, Val Loss: 0.27656108140945435, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 630/1000, Train Loss: 0.5720811784267426, Val Loss: 0.27585333585739136, Train Acc: 0.9, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 640/1000, Train Loss: 0.8750890493392944, Val Loss: 0.27563515305519104, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 650/1000, Train Loss: 0.9699333906173706, Val Loss: 0.27532288432121277, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 660/1000, Train Loss: 0.745454728603363, Val Loss: 0.27542611956596375, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 670/1000, Train Loss: 0.703035980463028, Val Loss: 0.2752763628959656, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 680/1000, Train Loss: 0.771321177482605, Val Loss: 0.2749970853328705, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 690/1000, Train Loss: 0.5687054395675659, Val Loss: 0.27484020590782166, Train Acc: 0.9, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 700/1000, Train Loss: 0.5615300536155701, Val Loss: 0.2753130793571472, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 710/1000, Train Loss: 0.8818529844284058, Val Loss: 0.2747113108634949, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 720/1000, Train Loss: 0.811494767665863, Val Loss: 0.2747950851917267, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 730/1000, Train Loss: 0.6914394497871399, Val Loss: 0.2751845717430115, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 740/1000, Train Loss: 0.8746364414691925, Val Loss: 0.2754763662815094, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 750/1000, Train Loss: 0.7774298787117004, Val Loss: 0.275723397731781, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 760/1000, Train Loss: 0.7420947551727295, Val Loss: 0.275718629360199, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 770/1000, Train Loss: 0.8085335493087769, Val Loss: 0.27623242139816284, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 780/1000, Train Loss: 0.7867838591337204, Val Loss: 0.27625584602355957, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 790/1000, Train Loss: 1.015926569700241, Val Loss: 0.2756829857826233, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 800/1000, Train Loss: 0.974385678768158, Val Loss: 0.2750861644744873, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 810/1000, Train Loss: 0.6835860311985016, Val Loss: 0.2739895284175873, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 820/1000, Train Loss: 0.6782542616128922, Val Loss: 0.2739909291267395, Train Acc: 0.9, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 830/1000, Train Loss: 0.7153491079807281, Val Loss: 0.27387094497680664, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 840/1000, Train Loss: 1.153635025024414, Val Loss: 0.27392643690109253, Train Acc: 0.675, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 850/1000, Train Loss: 0.8709507584571838, Val Loss: 0.2749180197715759, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 860/1000, Train Loss: 0.761111319065094, Val Loss: 0.27595940232276917, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 870/1000, Train Loss: 1.0717774331569672, Val Loss: 0.2759648859500885, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 880/1000, Train Loss: 0.7324835956096649, Val Loss: 0.2754645347595215, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 890/1000, Train Loss: 0.8358672857284546, Val Loss: 0.2753947973251343, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 900/1000, Train Loss: 0.7304109930992126, Val Loss: 0.2749597430229187, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 910/1000, Train Loss: 0.6343823671340942, Val Loss: 0.2751184105873108, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 920/1000, Train Loss: 0.7348747253417969, Val Loss: 0.2742580473423004, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
[34m[1mwandb[39m[22m: [33mWARNING[39m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'Train Loss': 2.841714859008789, 'Validation Loss': 0.6748658418655396, 'Train Accuracy': 0.325, 'Validation Accuracy': 0.8999999761581421, '_timestamp': 1717438767.576881}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.weight': {'_type': 'histogram', 'values': [2576, 3808, 3799, 3686, 3742, 3758, 3680, 3666, 3737, 3827, 3705, 3923, 3745, 3704, 3785, 3800, 3770, 3827, 3693, 3796, 3734, 3796, 3704, 3639, 3764, 3805, 3750, 3767, 3872, 3816, 3851, 3787, 3714, 3769, 3851, 3784, 3753, 3812, 3738, 3835, 3726, 3811, 3780, 3825, 3777, 3761, 3842, 3748, 3888, 3759, 3768, 3692, 3679, 3670, 3817, 3810, 3718, 3747, 3719, 3737, 3672, 3804, 3747, 1535], 'bins': [-0.10295218229293823, -0.09973514825105667, -0.0965181216597557, -0.09330108761787415, -0.09008405357599258, -0.08686702698469162, -0.08364999294281006, -0.0804329589009285, -0.07721592485904694, -0.07399889826774597, -0.07078186422586441, -0.06756483018398285, -0.06434780359268188, -0.061130769550800323, -0.05791373923420906, -0.0546967051923275, -0.05147967487573624, -0.048262644559144974, -0.04504561051726341, -0.04182858020067215, -0.03861154615879059, -0.035394515842199326, -0.03217748552560806, -0.02896045334637165, -0.02574342116713524, -0.022526388987898827, -0.019309356808662415, -0.01609232649207115, -0.01287529431283474, -0.009658262133598328, -0.00644123088568449, -0.0032241991721093655, -7.167458534240723e-06, 0.003209864255040884, 0.006426895968616009, 0.009643927216529846, 0.012860959395766258, 0.01607799157500267, 0.019295021891593933, 0.022512054070830345, 0.025729086250066757, 0.02894611842930317, 0.03216315060853958, 0.035380180925130844, 0.03859721124172211, 0.04181424528360367, 0.04503127560019493, 0.04824830964207649, 0.051465339958667755, 0.05468237027525902, 0.05789940431714058, 0.06111643463373184, 0.0643334686756134, 0.06755049526691437, 0.07076752930879593, 0.07398456335067749, 0.07720158994197845, 0.08041862398386002, 0.08363565802574158, 0.08685269206762314, 0.0900697186589241, 0.09328675270080566, 0.09650378674268723, 0.09972081333398819, 0.10293784737586975]}, '_timestamp': 1717438767.579395}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.bias': {'_type': 'histogram', 'values': [1, 3, 3, 4, 3, 3, 4, 3, 2, 8, 2, 3, 7, 5, 2, 4, 6, 6, 4, 4, 4, 4, 6, 2, 4, 3, 3, 4, 6, 2, 4, 8, 2, 1, 4, 5, 3, 4, 4, 9, 3, 4, 4, 10, 4, 3, 8, 1, 7, 7, 3, 4, 3, 6, 3, 6, 4, 5, 5, 8, 5, 1, 0, 1], 'bins': [-0.03369675204157829, -0.03263354301452637, -0.03157033771276474, -0.030507128685712814, -0.029443921521306038, -0.028380712494254112, -0.027317505329847336, -0.02625429816544056, -0.025191091001033783, -0.024127881973981857, -0.02306467480957508, -0.022001467645168304, -0.02093825861811638, -0.019875051453709602, -0.018811844289302826, -0.0177486352622509, -0.016685428097844124, -0.015622220002114773, -0.014559012837707996, -0.013495804741978645, -0.012432597577571869, -0.011369389481842518, -0.010306181386113167, -0.00924297422170639, -0.00817976612597704, -0.007116558495908976, -0.006053350865840912, -0.004990142770111561, -0.003926935140043497, -0.0028637275099754333, -0.001800519647076726, -0.0007373119005933404, 0.00032589584589004517, 0.0013891035923734307, 0.0024523113388568163, 0.0035155192017555237, 0.004578726831823587, 0.005641934461891651, 0.006705142557621002, 0.007768350187689066, 0.00883155781775713, 0.00989476591348648, 0.010957973077893257, 0.012021181173622608, 0.01308438926935196, 0.014147596433758736, 0.015210804529488087, 0.016274012625217438, 0.017337219789624214, 0.01840042695403099, 0.019463635981082916, 0.020526843145489693, 0.02159005030989647, 0.022653259336948395, 0.02371646650135517, 0.024779673665761948, 0.025842882692813873, 0.02690608985722065, 0.027969297021627426, 0.029032504186034203, 0.030095713213086128, 0.031158920377492905, 0.03222212940454483, 0.03328533470630646, 0.03434854373335838]}, '_timestamp': 1717438767.579616}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.weight': {'_type': 'histogram', 'values': [135, 8, 9, 13, 5, 0, 3, 4, 1, 3, 1, 0, 0, 0, 1, 0, 2, 1, 1, 1, 0, 0, 1, 2, 1, 1, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 4, 2, 2, 0, 2, 1, 2, 3, 1, 1, 3, 1, 0, 2, 3, 2, 0, 1, 1, 0, 0, 1, 0, 1, 8, 3, 4, 19], 'bins': [0.9985610246658325, 0.9986059665679932, 0.9986509680747986, 0.9986959099769592, 0.9987409114837646, 0.9987858533859253, 0.9988308548927307, 0.9988757967948914, 0.9989207983016968, 0.9989657402038574, 0.9990107417106628, 0.9990556836128235, 0.9991006851196289, 0.9991456270217896, 0.999190628528595, 0.9992355704307556, 0.999280571937561, 0.9993255138397217, 0.9993704557418823, 0.9994154572486877, 0.9994603991508484, 0.9995054006576538, 0.9995503425598145, 0.9995953440666199, 0.9996402859687805, 0.9996852874755859, 0.9997302293777466, 0.999775230884552, 0.9998201727867126, 0.9998651742935181, 0.9999101161956787, 0.9999551177024841, 1.0, 1.0000450611114502, 1.0000900030136108, 1.0001349449157715, 1.0001798868179321, 1.0002249479293823, 1.000269889831543, 1.0003148317337036, 1.0003597736358643, 1.0004048347473145, 1.000449776649475, 1.0004947185516357, 1.0005396604537964, 1.0005847215652466, 1.0006296634674072, 1.0006746053695679, 1.0007195472717285, 1.0007644891738892, 1.0008095502853394, 1.0008544921875, 1.0008994340896606, 1.0009443759918213, 1.0009894371032715, 1.0010343790054321, 1.0010793209075928, 1.0011242628097534, 1.0011693239212036, 1.0012142658233643, 1.001259207725525, 1.0013041496276855, 1.0013492107391357, 1.0013941526412964, 1.001439094543457]}, '_timestamp': 1717438767.579797}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.bias': {'_type': 'histogram', 'values': [17, 6, 6, 2, 6, 12, 3, 0, 2, 0, 1, 3, 1, 1, 0, 0, 1, 1, 0, 1, 7, 0, 1, 0, 3, 2, 1, 2, 0, 0, 0, 106, 0, 0, 0, 0, 3, 0, 3, 0, 2, 1, 0, 5, 2, 2, 0, 0, 0, 2, 5, 2, 1, 2, 3, 2, 0, 1, 3, 1, 8, 4, 7, 20], 'bins': [-0.0014389483258128166, -0.001393980230204761, -0.0013490121345967054, -0.0013040440389886498, -0.0012590759433805943, -0.0012141078477725387, -0.001169139752164483, -0.0011241716565564275, -0.0010792035609483719, -0.0010342354653403163, -0.0009892673697322607, -0.0009442992741242051, -0.0008993311785161495, -0.0008543630829080939, -0.0008093949873000383, -0.0007644268916919827, -0.0007194587960839272, -0.0006744906422682106, -0.0006295225466601551, -0.0005845544510520995, -0.0005395863554440439, -0.0004946182598359883, -0.00044965019333176315, -0.00040468209772370756, -0.0003597139730118215, -0.0003147458774037659, -0.0002697777817957103, -0.00022480970073956996, -0.00017984159057959914, -0.00013487349497154355, -8.990539936348796e-05, -4.493730375543237e-05, 3.079185262322426e-08, 4.4998887460678816e-05, 8.996698306873441e-05, 0.00013493507867679, 0.0001799031742848456, 0.0002248712844448164, 0.0002698393655009568, 0.00031480746110901237, 0.00035977555671706796, 0.000404743681428954, 0.0004497117770370096, 0.0004946798435412347, 0.0005396479391492903, 0.0005846160347573459, 0.0006295841303654015, 0.0006745522259734571, 0.0007195203797891736, 0.0007644884753972292, 0.0008094565710052848, 0.0008544246666133404, 0.000899392762221396, 0.0009443608578294516, 0.0009893289534375072, 0.0010342970490455627, 0.0010792651446536183, 0.001124233240261674, 0.0011692013358697295, 0.0012141694314777851, 0.0012591375270858407, 0.0013041056226938963, 0.0013490737183019519, 0.0013940418139100075, 0.001439009909518063]}, '_timestamp': 1717438767.5799592}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.weight': {'_type': 'histogram', 'values': [637, 1125, 1120, 1095, 1109, 1082, 1102, 1155, 1076, 1126, 1078, 1083, 1078, 1072, 1075, 1026, 1123, 1097, 1165, 1055, 1075, 1075, 1146, 1099, 1101, 1109, 1088, 1078, 1078, 1021, 1087, 1248, 1249, 1147, 1083, 1129, 1047, 1140, 1102, 1152, 1132, 1111, 1113, 1098, 1121, 1058, 1061, 1140, 1102, 1085, 1051, 1092, 1106, 1087, 1111, 1122, 1083, 1140, 1087, 1137, 1074, 1083, 1140, 629], 'bins': [-0.1519978791475296, -0.14724791049957275, -0.1424979269504547, -0.13774795830249786, -0.13299798965454102, -0.12824800610542297, -0.12349803745746613, -0.11874806880950928, -0.11399809271097183, -0.10924811661243439, -0.10449814796447754, -0.0997481718659401, -0.09499819576740265, -0.0902482271194458, -0.08549825102090836, -0.08074828237295151, -0.07599830627441406, -0.07124833017587662, -0.06649836152791977, -0.061748385429382324, -0.05699841305613518, -0.05224844068288803, -0.047498464584350586, -0.04274849221110344, -0.03799851983785629, -0.033248547464609146, -0.02849857322871685, -0.023748598992824554, -0.018998626619577408, -0.014248653315007687, -0.009498680010437965, -0.004748706705868244, 1.2665987014770508e-06, 0.004751239903271198, 0.00950121320784092, 0.01425118651241064, 0.019001159816980362, 0.02375113219022751, 0.028501106426119804, 0.0332510806620121, 0.03800105303525925, 0.04275102540850639, 0.04750099778175354, 0.052250973880290985, 0.05700094625353813, 0.06175091862678528, 0.06650089472532272, 0.07125086337327957, 0.07600083947181702, 0.08075081557035446, 0.08550078421831131, 0.09025076031684875, 0.0950007289648056, 0.09975070506334305, 0.1045006811618805, 0.10925064980983734, 0.11400062590837479, 0.11875060200691223, 0.12350057065486908, 0.12825053930282593, 0.13300052285194397, 0.13775049149990082, 0.14250046014785767, 0.1472504436969757, 0.15200041234493256]}, '_timestamp': 1717438767.5805209}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.bias': {'_type': 'histogram', 'values': [8, 5, 7, 2, 5, 3, 0, 5, 5, 3, 4, 2, 5, 6, 3, 3, 3, 3, 5, 2, 6, 3, 1, 2, 3, 4, 4, 3, 3, 1, 5, 4, 2, 7, 8, 5, 4, 4, 4, 2, 3, 2, 8, 2, 7, 3, 4, 4, 4, 4, 4, 4, 3, 8, 5, 7, 4, 6, 4, 7, 7, 6, 3, 1], 'bins': [-0.06131995469331741, -0.059397436678409576, -0.05747491866350174, -0.0555524006485939, -0.053629882633686066, -0.05170736461877823, -0.04978484660387039, -0.047862328588962555, -0.04593981057405472, -0.04401729255914688, -0.042094774544239044, -0.040172260254621506, -0.03824974223971367, -0.03632722422480583, -0.034404706209897995, -0.03248218819499016, -0.03055967018008232, -0.028637152165174484, -0.026714634150266647, -0.02479211613535881, -0.022869598120450974, -0.020947080105543137, -0.0190245620906353, -0.017102044075727463, -0.015179526060819626, -0.013257008977234364, -0.011334490962326527, -0.00941197294741869, -0.007489454932510853, -0.005566937383264303, -0.0036444193683564663, -0.001721901586279273, 0.00020061619579792023, 0.0021231339778751135, 0.004045651759952307, 0.005968169774860144, 0.007890687324106693, 0.00981320533901453, 0.011735723353922367, 0.013658241368830204, 0.015580758452415466, 0.017503276467323303, 0.01942579448223114, 0.021348312497138977, 0.023270830512046814, 0.02519334852695465, 0.027115866541862488, 0.029038384556770325, 0.03096090257167816, 0.032883420586586, 0.034805938601493835, 0.03672845661640167, 0.03865097463130951, 0.040573492646217346, 0.042496006935834885, 0.04441852495074272, 0.04634104296565056, 0.048263560980558395, 0.05018607899546623, 0.05210859701037407, 0.054031115025281906, 0.05595363304018974, 0.05787615105509758, 0.05979866907000542, 0.061721187084913254]}, '_timestamp': 1717438767.580684}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.weight': {'_type': 'histogram', 'values': [32, 11, 16, 9, 18, 8, 3, 4, 3, 1, 3, 3, 0, 3, 4, 0, 1, 2, 4, 1, 2, 0, 5, 4, 2, 2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 4, 1, 3, 0, 2, 2, 1, 1, 1, 2, 5, 1, 5, 4, 1, 3, 4, 6, 5, 5, 5, 5, 6, 7, 11, 24], 'bins': [0.9985613226890564, 0.998606264591217, 0.9986512064933777, 0.9986962080001831, 0.9987411499023438, 0.9987860918045044, 0.998831033706665, 0.9988759756088257, 0.9989209175109863, 0.9989659190177917, 0.9990108609199524, 0.999055802822113, 0.9991007447242737, 0.9991456866264343, 0.999190628528595, 0.9992356300354004, 0.999280571937561, 0.9993255138397217, 0.9993704557418823, 0.999415397644043, 0.9994603395462036, 0.999505341053009, 0.9995502829551697, 0.9995952248573303, 0.999640166759491, 0.9996851086616516, 0.9997300505638123, 0.9997750520706177, 0.9998199939727783, 0.999864935874939, 0.9999098777770996, 0.9999548196792603, 0.9999997615814209, 1.0000447034835815, 1.0000896453857422, 1.0001347064971924, 1.000179648399353, 1.0002245903015137, 1.0002695322036743, 1.000314474105835, 1.0003594160079956, 1.0004043579101562, 1.000449299812317, 1.0004942417144775, 1.0005391836166382, 1.0005841255187988, 1.0006290674209595, 1.0006741285324097, 1.0007190704345703, 1.000764012336731, 1.0008089542388916, 1.0008538961410522, 1.000898838043213, 1.0009437799453735, 1.0009887218475342, 1.0010336637496948, 1.0010786056518555, 1.0011235475540161, 1.0011684894561768, 1.001213550567627, 1.0012584924697876, 1.0013034343719482, 1.0013483762741089, 1.0013933181762695, 1.0014382600784302]}, '_timestamp': 1717438767.5808399}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.bias': {'_type': 'histogram', 'values': [19, 10, 5, 5, 4, 35, 1, 5, 2, 1, 1, 1, 0, 2, 0, 6, 3, 5, 1, 4, 1, 3, 0, 3, 8, 2, 3, 1, 0, 0, 0, 6, 0, 0, 0, 0, 3, 0, 5, 1, 2, 2, 3, 5, 2, 3, 3, 4, 2, 1, 4, 1, 1, 2, 5, 2, 3, 2, 14, 5, 9, 8, 12, 23], 'bins': [-0.0014389574062079191, -0.0013939900090917945, -0.0013490226119756699, -0.0013040550984442234, -0.0012590877013280988, -0.0012141203042119741, -0.0011691527906805277, -0.001124185393564403, -0.0010792179964482784, -0.0010342505993321538, -0.0009892832022160292, -0.0009443156886845827, -0.0008993482915684581, -0.0008543808944523335, -0.0008094134391285479, -0.0007644459838047624, -0.0007194785866886377, -0.0006745111895725131, -0.0006295437342487276, -0.000584576278924942, -0.0005396088818088174, -0.0004946414846926928, -0.0004496740293689072, -0.0004047066031489521, -0.00035973917692899704, -0.00031477175070904195, -0.00026980432448908687, -0.00022483689826913178, -0.0001798694720491767, -0.0001349020458292216, -8.993461960926652e-05, -4.496719338931143e-05, 2.3283064365386963e-10, 4.496765905059874e-05, 8.993508527055383e-05, 0.00013490251149050891, 0.000179869937710464, 0.0002248373639304191, 0.0002698047901503742, 0.00031477221637032926, 0.00035973964259028435, 0.00040470706881023943, 0.0004496744950301945, 0.0004946419503539801, 0.0005396093474701047, 0.0005845767445862293, 0.0006295441999100149, 0.0006745116552338004, 0.000719479052349925, 0.0007644464494660497, 0.0008094139047898352, 0.0008543813601136208, 0.0008993487572297454, 0.00094431615434587, 0.0009892836678773165, 0.001034251064993441, 0.0010792184621095657, 0.0011241858592256904, 0.001169153256341815, 0.0012141207698732615, 0.001259088166989386, 0.0013040555641055107, 0.0013490230776369572, 0.0013939904747530818, 0.0014389578718692064]}, '_timestamp': 1717438767.580989}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.weight': {'_type': 'histogram', 'values': [752, 1109, 1090, 1108, 1096, 1087, 1122, 1055, 1050, 1138, 1128, 1106, 1095, 1050, 1083, 1123, 1144, 1077, 1052, 1018, 1059, 1153, 1137, 1134, 1108, 1135, 1109, 1068, 1062, 1150, 1121, 1116, 1127, 1110, 1029, 1108, 1112, 1046, 1108, 1113, 1149, 1085, 1074, 1167, 1066, 1121, 1085, 1100, 1094, 1149, 1137, 1074, 1138, 1067, 1129, 1049, 1153, 1088, 1117, 1101, 1099, 1079, 1038, 749], 'bins': [-0.15211819112300873, -0.1473645269870758, -0.14261086285114288, -0.13785719871520996, -0.13310353457927704, -0.12834987044334412, -0.1235962063074112, -0.11884254217147827, -0.11408887803554535, -0.10933521389961243, -0.1045815497636795, -0.09982788562774658, -0.09507422149181366, -0.09032055735588074, -0.08556689321994781, -0.08081322908401489, -0.07605956494808197, -0.07130589336156845, -0.06655222922563553, -0.061798568814992905, -0.05704490467905998, -0.05229124054312706, -0.04753757640719414, -0.042783912271261215, -0.038030244410037994, -0.03327658027410507, -0.0285229180008173, -0.023769253864884377, -0.019015587866306305, -0.014261924661695957, -0.00950825959444046, -0.004754595458507538, -9.313225746154785e-07, 0.004752732813358307, 0.00950639694929123, 0.014260062016546726, 0.019013725221157074, 0.023767391219735146, 0.028521055355668068, 0.03327471762895584, 0.03802838176488876, 0.042782049626111984, 0.04753571376204491, 0.05228937789797783, 0.05704304203391075, 0.061796706169843674, 0.0665503665804863, 0.07130403071641922, 0.07605770230293274, 0.08081136643886566, 0.08556503057479858, 0.0903186947107315, 0.09507235884666443, 0.09982602298259735, 0.10457968711853027, 0.1093333512544632, 0.11408701539039612, 0.11884067952632904, 0.12359434366226196, 0.12834800779819489, 0.1331016719341278, 0.13785533607006073, 0.14260900020599365, 0.14736266434192657, 0.1521163284778595]}, '_timestamp': 1717438767.581554}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.bias': {'_type': 'histogram', 'values': [2, 2, 3, 4, 3, 3, 3, 3, 2, 3, 4, 3, 5, 8, 4, 7, 4, 5, 3, 8, 3, 3, 2, 4, 6, 2, 7, 5, 4, 4, 5, 8, 3, 2, 2, 4, 5, 3, 5, 8, 6, 0, 5, 3, 1, 7, 3, 6, 3, 5, 4, 3, 7, 3, 3, 4, 3, 5, 5, 2, 7, 5, 2, 8], 'bins': [-0.06188282370567322, -0.059939391911029816, -0.05799596384167671, -0.05605253577232361, -0.054109103977680206, -0.052165672183036804, -0.0502222441136837, -0.0482788160443306, -0.046335384249687195, -0.04439195245504379, -0.04244852438569069, -0.040505096316337585, -0.03856166452169418, -0.03661823272705078, -0.03467480465769768, -0.032731376588344574, -0.030787944793701172, -0.02884451486170292, -0.026901084929704666, -0.024957654997706413, -0.02301422506570816, -0.021070795133709908, -0.019127365201711655, -0.017183935269713402, -0.015240505337715149, -0.013297075405716896, -0.011353645473718643, -0.00941021554172039, -0.0074667856097221375, -0.005523355677723885, -0.0035799257457256317, -0.0016364958137273788, 0.000306934118270874, 0.002250364050269127, 0.00419379398226738, 0.006137223914265633, 0.008080653846263885, 0.010024083778262138, 0.011967513710260391, 0.013910943642258644, 0.015854373574256897, 0.01779780350625515, 0.019741233438253403, 0.021684663370251656, 0.02362809330224991, 0.02557152323424816, 0.027514953166246414, 0.029458383098244667, 0.03140181303024292, 0.03334524482488632, 0.035288672894239426, 0.03723210096359253, 0.03917553275823593, 0.041118964552879333, 0.04306239262223244, 0.04500582069158554, 0.04694925248622894, 0.048892684280872345, 0.05083611235022545, 0.05277954041957855, 0.054722972214221954, 0.056666404008865356, 0.05860983207821846, 0.060553260147571564, 0.062496691942214966]}, '_timestamp': 1717438767.581707}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.weight': {'_type': 'histogram', 'values': [53, 20, 16, 17, 7, 3, 3, 2, 2, 4, 1, 5, 0, 1, 3, 6, 1, 5, 1, 3, 1, 4, 2, 2, 3, 1, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 6, 1, 2, 4, 3, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 0, 4, 2, 1, 2, 5, 3, 7, 7, 8, 16], 'bins': [0.9985610842704773, 0.9986060261726379, 0.9986510276794434, 0.998695969581604, 0.9987409114837646, 0.9987859129905701, 0.9988308548927307, 0.9988757967948914, 0.9989207983016968, 0.9989657402038574, 0.9990106821060181, 0.9990556836128235, 0.9991006255149841, 0.9991455674171448, 0.9991905689239502, 0.9992355108261108, 0.9992804527282715, 0.9993254542350769, 0.9993703961372375, 0.9994153380393982, 0.9994603395462036, 0.9995052814483643, 0.9995502233505249, 0.9995952248573303, 0.999640166759491, 0.9996851086616516, 0.999730110168457, 0.9997750520706177, 0.9998199939727783, 0.9998649954795837, 0.9999099373817444, 0.999954879283905, 0.9999998807907104, 1.000044822692871, 1.0000897645950317, 1.0001347064971924, 1.000179648399353, 1.0002247095108032, 1.0002696514129639, 1.0003145933151245, 1.0003595352172852, 1.0004044771194458, 1.0004494190216064, 1.0004944801330566, 1.0005394220352173, 1.000584363937378, 1.0006293058395386, 1.0006742477416992, 1.0007191896438599, 1.00076425075531, 1.0008091926574707, 1.0008541345596313, 1.000899076461792, 1.0009440183639526, 1.0009889602661133, 1.0010340213775635, 1.0010789632797241, 1.0011239051818848, 1.0011688470840454, 1.001213788986206, 1.0012587308883667, 1.001303791999817, 1.0013487339019775, 1.0013936758041382, 1.0014386177062988]}, '_timestamp': 1717438767.581855}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.bias': {'_type': 'histogram', 'values': [39, 10, 10, 8, 7, 16, 3, 3, 0, 1, 0, 6, 3, 1, 1, 4, 2, 2, 0, 1, 2, 2, 4, 4, 7, 2, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 5, 6, 8, 7, 2, 2, 4, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 3, 2, 1, 2, 12, 3, 8, 9, 10, 19], 'bins': [-0.001438986393623054, -0.0013940188800916076, -0.0013490513665601611, -0.0013040839694440365, -0.00125911645591259, -0.0012141489423811436, -0.0011691814288496971, -0.0011242140317335725, -0.001079246518202126, -0.0010342790046706796, -0.0009893114911392331, -0.0009443440358154476, -0.000899376580491662, -0.0008544090669602156, -0.00080944161163643, -0.0007644740981049836, -0.000719506642781198, -0.0006745391292497516, -0.0006295716157183051, -0.0005846041603945196, -0.0005396366468630731, -0.0004946691915392876, -0.00044970170711167157, -0.00040473422268405557, -0.0003597667091526091, -0.0003147992247249931, -0.0002698317402973771, -0.00022486427042167634, -0.0001798967714421451, -0.0001349292870145291, -8.996180258691311e-05, -4.499431815929711e-05, -2.6833731681108475e-08, 4.494065069593489e-05, 8.990813512355089e-05, 0.0001348756195511669, 0.0001798431039787829, 0.00022481060295831412, 0.0002697780728340149, 0.0003147455572616309, 0.0003597130416892469, 0.00040468055522069335, 0.00044964803964830935, 0.0004946155240759254, 0.0005395829793997109, 0.0005845504929311574, 0.0006295179482549429, 0.0006744854617863894, 0.0007194529753178358, 0.0007644204306416214, 0.0008093879441730678, 0.0008543553994968534, 0.0008993229130282998, 0.0009442903683520854, 0.000989257823675871, 0.0010342253372073174, 0.0010791928507387638, 0.0011241603642702103, 0.001169127761386335, 0.0012140952749177814, 0.0012590627884492278, 0.0013040303019806743, 0.001348997699096799, 0.0013939652126282454, 0.0014389327261596918]}, '_timestamp': 1717438767.582002}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.weight': {'_type': 'histogram', 'values': [3, 7, 3, 5, 1, 8, 1, 2, 1, 4, 2, 2, 2, 1, 3, 3, 5, 4, 3, 5, 5, 4, 4, 3, 6, 4, 3, 5, 5, 0, 5, 3, 7, 2, 5, 3, 5, 6, 7, 4, 2, 3, 4, 7, 9, 3, 6, 5, 3, 4, 3, 7, 4, 5, 5, 5, 3, 6, 5, 2, 3, 5, 6, 8], 'bins': [-0.2094566524028778, -0.20285235345363617, -0.19624806940555573, -0.1896437704563141, -0.18303947150707245, -0.176435187458992, -0.16983088850975037, -0.16322660446166992, -0.15662230551242828, -0.15001800656318665, -0.1434137225151062, -0.13680942356586456, -0.13020512461662292, -0.12360083311796188, -0.11699654161930084, -0.1103922501206398, -0.10378795862197876, -0.09718365967273712, -0.09057936817407608, -0.08397507667541504, -0.0773707777261734, -0.07076648622751236, -0.06416219472885132, -0.05755789950489998, -0.05095360428094864, -0.0443493127822876, -0.03774501755833626, -0.031140726059675217, -0.024536430835723877, -0.017932137474417686, -0.011327844113111496, -0.0047235507518053055, 0.001880742609500885, 0.008485035970807076, 0.015089329332113266, 0.021693622693419456, 0.028297916054725647, 0.03490221127867699, 0.04150650277733803, 0.04811079800128937, 0.05471508949995041, 0.06131938472390175, 0.06792367994785309, 0.07452797144651413, 0.08113226294517517, 0.08773656189441681, 0.09434085339307785, 0.10094514489173889, 0.10754944384098053, 0.11415373533964157, 0.12075802683830261, 0.12736232578754425, 0.1339666098356247, 0.14057090878486633, 0.14717520773410797, 0.15377949178218842, 0.16038379073143005, 0.1669880896806717, 0.17359237372875214, 0.18019667267799377, 0.18680095672607422, 0.19340525567531586, 0.2000095546245575, 0.20661383867263794, 0.21321813762187958]}, '_timestamp': 1717438767.5821528}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.bias': {'_type': 'histogram', 'values': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'bins': [-0.5291435122489929, -0.5135185122489929, -0.4978935122489929, -0.4822685122489929, -0.4666435122489929, -0.4510185122489929, -0.4353935122489929, -0.4197685122489929, -0.4041435122489929, -0.3885185122489929, -0.3728935122489929, -0.3572685122489929, -0.3416435122489929, -0.3260185122489929, -0.3103935122489929, -0.2947685122489929, -0.2791435122489929, -0.2635185122489929, -0.24789351224899292, -0.23226851224899292, -0.21664351224899292, -0.20101851224899292, -0.18539351224899292, -0.16976851224899292, -0.15414351224899292, -0.13851851224899292, -0.12289351224899292, -0.10726851224899292, -0.09164351224899292, -0.07601851224899292, -0.06039351224899292, -0.04476851224899292, -0.02914351411163807, -0.01351851411163807, 0.002106485888361931, 0.01773148588836193, 0.03335648775100708, 0.04898148775100708, 0.06460648775100708, 0.08023148775100708, 0.09585648775100708, 0.11148148775100708, 0.12710648775100708, 0.14273148775100708, 0.15835648775100708, 0.17398148775100708, 0.18960648775100708, 0.20523148775100708, 0.22085648775100708, 0.23648148775100708, 0.2521064877510071, 0.2677314877510071, 0.2833564877510071, 0.2989814877510071, 0.3146064877510071, 0.3302314877510071, 0.3458564877510071, 0.3614814877510071, 0.3771064877510071, 0.3927314877510071, 0.4083564877510071, 0.4239814877510071, 0.4396064877510071, 0.4552314877510071, 0.4708564877510071]}, '_timestamp': 1717438767.582321}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TP Reward Distribution': {'_type': 'histogram', 'values': [6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [2.933333333333333, 5.765625, 8.597916666666666, 11.430208333333333, 14.2625, 17.094791666666666, 19.927083333333332, 22.759375, 25.591666666666665, 28.42395833333333, 31.256249999999998, 34.088541666666664, 36.92083333333333, 39.75312499999999, 42.58541666666666, 45.41770833333333, 48.24999999999999, 51.082291666666656, 53.914583333333326, 56.746874999999996, 59.57916666666666, 62.41145833333332, 65.24374999999999, 68.07604166666667, 70.90833333333333, 73.740625, 76.57291666666666, 79.40520833333333, 82.2375, 85.06979166666666, 87.90208333333334, 90.734375, 93.56666666666666, 96.39895833333333, 99.23124999999999, 102.06354166666667, 104.89583333333333, 107.72812499999999, 110.56041666666667, 113.39270833333333, 116.225, 119.05729166666666, 121.88958333333332, 124.721875, 127.55416666666666, 130.3864583333333, 133.21875, 136.05104166666666, 138.88333333333333, 141.715625, 144.54791666666665, 147.38020833333331, 150.21249999999998, 153.04479166666667, 155.87708333333333, 158.709375, 161.54166666666666, 164.37395833333332, 167.20624999999998, 170.03854166666665, 172.87083333333334, 175.703125, 178.53541666666666, 181.36770833333333, 184.2]}, '_timestamp': 1717438767.582484}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TN Reward Distribution': {'_type': 'histogram', 'values': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1], 'bins': [6.266666666666667, 6.515625, 6.764583333333333, 7.013541666666667, 7.2625, 7.511458333333334, 7.760416666666666, 8.009375, 8.258333333333333, 8.507291666666667, 8.75625, 9.005208333333334, 9.254166666666666, 9.503125, 9.752083333333333, 10.001041666666666, 10.25, 10.498958333333334, 10.747916666666667, 10.996875, 11.245833333333334, 11.494791666666668, 11.74375, 11.992708333333333, 12.241666666666667, 12.490625, 12.739583333333332, 12.988541666666666, 13.2375, 13.486458333333333, 13.735416666666666, 13.984375, 14.233333333333334, 14.482291666666665, 14.73125, 14.980208333333334, 15.229166666666668, 15.478124999999999, 15.727083333333333, 15.976041666666667, 16.225, 16.473958333333332, 16.722916666666666, 16.971875, 17.220833333333335, 17.469791666666666, 17.71875, 17.967708333333334, 18.216666666666665, 18.465625, 18.714583333333334, 18.963541666666668, 19.2125, 19.461458333333333, 19.710416666666667, 19.959375, 20.208333333333332, 20.457291666666666, 20.70625, 20.955208333333335, 21.204166666666666, 21.453125, 21.702083333333334, 21.95104166666667, 22.2]}, '_timestamp': 1717438767.582623}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FP Reward Distribution': {'_type': 'histogram', 'values': [6, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [3.533333333333333, 6.301041666666666, 9.06875, 11.836458333333333, 14.604166666666666, 17.371875, 20.139583333333334, 22.907291666666666, 25.674999999999997, 28.44270833333333, 31.210416666666667, 33.978125, 36.74583333333333, 39.51354166666666, 42.28125, 45.04895833333333, 47.81666666666666, 50.584374999999994, 53.352083333333326, 56.119791666666664, 58.887499999999996, 61.65520833333333, 64.42291666666667, 67.190625, 69.95833333333333, 72.72604166666666, 75.49374999999999, 78.26145833333332, 81.02916666666667, 83.796875, 86.56458333333333, 89.33229166666666, 92.1, 94.86770833333333, 97.63541666666666, 100.40312499999999, 103.17083333333332, 105.93854166666667, 108.70625, 111.47395833333333, 114.24166666666666, 117.00937499999999, 119.77708333333332, 122.54479166666665, 125.3125, 128.08020833333333, 130.84791666666666, 133.615625, 136.38333333333333, 139.15104166666666, 141.91875, 144.68645833333332, 147.45416666666665, 150.22187499999998, 152.98958333333331, 155.75729166666665, 158.525, 161.29270833333334, 164.06041666666667, 166.828125, 169.59583333333333, 172.36354166666666, 175.13125, 177.89895833333333, 180.66666666666666]}, '_timestamp': 1717438767.5827599}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FN Reward Distribution': {'_type': 'histogram', 'values': [2, 1, 2, 4, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [2.6666666666666665, 3.0614583333333334, 3.45625, 3.8510416666666663, 4.245833333333334, 4.640625, 5.035416666666666, 5.430208333333333, 5.824999999999999, 6.219791666666666, 6.614583333333333, 7.009375, 7.404166666666667, 7.798958333333333, 8.19375, 8.588541666666666, 8.983333333333333, 9.378124999999999, 9.772916666666665, 10.167708333333334, 10.5625, 10.957291666666666, 11.352083333333333, 11.746875, 12.141666666666666, 12.536458333333332, 12.931249999999999, 13.326041666666665, 13.720833333333333, 14.115625, 14.510416666666666, 14.905208333333333, 15.299999999999999, 15.694791666666665, 16.089583333333334, 16.484375, 16.879166666666666, 17.273958333333333, 17.66875, 18.063541666666666, 18.458333333333332, 18.853125000000002, 19.24791666666667, 19.642708333333335, 20.0375, 20.432291666666668, 20.827083333333334, 21.221875, 21.616666666666667, 22.011458333333334, 22.40625, 22.801041666666666, 23.195833333333333, 23.590625, 23.985416666666666, 24.380208333333332, 24.775000000000002, 25.16979166666667, 25.564583333333335, 25.959375, 26.354166666666668, 26.748958333333334, 27.14375, 27.538541666666667, 27.933333333333334]}, '_timestamp': 1717438767.582894}).
Epoch 930/1000, Train Loss: 0.6959640979766846, Val Loss: 0.2744472920894623, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 940/1000, Train Loss: 0.7280371785163879, Val Loss: 0.2746974229812622, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 950/1000, Train Loss: 0.6924698650836945, Val Loss: 0.2751319706439972, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 960/1000, Train Loss: 0.6156047880649567, Val Loss: 0.274759441614151, Train Acc: 0.925, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 970/1000, Train Loss: 0.9000118672847748, Val Loss: 0.2741515040397644, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 980/1000, Train Loss: 0.6415913701057434, Val Loss: 0.27473047375679016, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
Epoch 990/1000, Train Loss: 0.6728399991989136, Val Loss: 0.27478641271591187, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 7.1937956173874025e-06
[32m[I 2024-06-03 14:19:44,860][39m Trial 0 finished with value: 0.17557962238788605 and parameters: {'hidden_size': 264, 'learning_rate': 0.0007193795617387424, 'weight_decay': 5.5314721709267977e-05}. Best is trial 0 with value: 0.17557962238788605.