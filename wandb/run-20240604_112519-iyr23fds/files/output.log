Epoch 0/1000, Train Loss: 2.457655429840088, Val Loss: 0.5735387206077576, Train Acc: 0.575, Val Acc: 1.0, LR: 6.081285276765094e-05
Epoch 10/1000, Train Loss: 2.4152737855911255, Val Loss: 0.5055012702941895, Train Acc: 0.5, Val Acc: 1.0, LR: 5.9606369445874336e-05
Epoch 20/1000, Train Loss: 2.7307153940200806, Val Loss: 0.46562856435775757, Train Acc: 0.625, Val Acc: 1.0, LR: 5.8399886124097724e-05
Epoch 30/1000, Train Loss: 1.8968691229820251, Val Loss: 0.45982256531715393, Train Acc: 0.575, Val Acc: 1.0, LR: 5.719340280232111e-05
Epoch 40/1000, Train Loss: 2.25840961933136, Val Loss: 0.41032782196998596, Train Acc: 0.475, Val Acc: 1.0, LR: 5.5986919480544494e-05
Epoch 50/1000, Train Loss: 2.0278473496437073, Val Loss: 0.38799554109573364, Train Acc: 0.55, Val Acc: 1.0, LR: 5.478043615876789e-05
Epoch 60/1000, Train Loss: 1.6651841402053833, Val Loss: 0.3910892605781555, Train Acc: 0.6, Val Acc: 1.0, LR: 5.357395283699127e-05
Epoch 70/1000, Train Loss: 2.166391134262085, Val Loss: 0.3742331564426422, Train Acc: 0.6, Val Acc: 1.0, LR: 5.236746951521466e-05
Epoch 80/1000, Train Loss: 2.033851981163025, Val Loss: 0.3519166111946106, Train Acc: 0.525, Val Acc: 1.0, LR: 5.116098619343805e-05
Epoch 90/1000, Train Loss: 1.9007059335708618, Val Loss: 0.30346593260765076, Train Acc: 0.5, Val Acc: 1.0, LR: 4.995450287166142e-05
Epoch 100/1000, Train Loss: 2.56624174118042, Val Loss: 0.2843695282936096, Train Acc: 0.375, Val Acc: 1.0, LR: 4.874801954988481e-05
Epoch 110/1000, Train Loss: 1.3487557470798492, Val Loss: 0.25581595301628113, Train Acc: 0.65, Val Acc: 1.0, LR: 4.75415362281082e-05
Epoch 120/1000, Train Loss: 2.390843391418457, Val Loss: 0.23069997131824493, Train Acc: 0.65, Val Acc: 1.0, LR: 4.633505290633159e-05
Epoch 130/1000, Train Loss: 2.1568341851234436, Val Loss: 0.21556925773620605, Train Acc: 0.575, Val Acc: 1.0, LR: 4.512856958455497e-05
Epoch 140/1000, Train Loss: 1.5434620380401611, Val Loss: 0.19179017841815948, Train Acc: 0.7, Val Acc: 1.0, LR: 4.392208626277836e-05
Epoch 150/1000, Train Loss: 1.5996416211128235, Val Loss: 0.1732388287782669, Train Acc: 0.625, Val Acc: 1.0, LR: 4.271560294100173e-05
Epoch 160/1000, Train Loss: 0.8818532824516296, Val Loss: 0.1569155901670456, Train Acc: 0.75, Val Acc: 1.0, LR: 4.1509119619225114e-05
Epoch 170/1000, Train Loss: 1.9584475457668304, Val Loss: 0.13843794167041779, Train Acc: 0.7, Val Acc: 1.0, LR: 4.0302636297448495e-05
Epoch 180/1000, Train Loss: 0.9875737726688385, Val Loss: 0.1263585090637207, Train Acc: 0.725, Val Acc: 1.0, LR: 3.9096152975671884e-05
Epoch 190/1000, Train Loss: 1.4101046919822693, Val Loss: 0.12725555896759033, Train Acc: 0.775, Val Acc: 1.0, LR: 3.788966965389526e-05
Epoch 200/1000, Train Loss: 1.0071443617343903, Val Loss: 0.13049185276031494, Train Acc: 0.725, Val Acc: 1.0, LR: 3.6683186332118633e-05
Epoch 210/1000, Train Loss: 1.2144220173358917, Val Loss: 0.11957614123821259, Train Acc: 0.75, Val Acc: 1.0, LR: 3.547670301034201e-05
Epoch 220/1000, Train Loss: 1.3086615800857544, Val Loss: 0.10843300819396973, Train Acc: 0.725, Val Acc: 1.0, LR: 3.4270219688565376e-05
Epoch 230/1000, Train Loss: 0.6779603064060211, Val Loss: 0.09779335558414459, Train Acc: 0.825, Val Acc: 1.0, LR: 3.3063736366788744e-05
Epoch 240/1000, Train Loss: 1.406215250492096, Val Loss: 0.08423571288585663, Train Acc: 0.75, Val Acc: 1.0, LR: 3.1857253045012126e-05
Epoch 250/1000, Train Loss: 0.898853600025177, Val Loss: 0.07989536225795746, Train Acc: 0.825, Val Acc: 1.0, LR: 3.0650769723235494e-05
Epoch 260/1000, Train Loss: 1.6050982177257538, Val Loss: 0.07762981206178665, Train Acc: 0.85, Val Acc: 1.0, LR: 2.9444286401458903e-05
Epoch 270/1000, Train Loss: 1.2419133186340332, Val Loss: 0.07632853835821152, Train Acc: 0.8, Val Acc: 1.0, LR: 2.8237803079682308e-05
Epoch 280/1000, Train Loss: 0.9117641150951385, Val Loss: 0.0727064311504364, Train Acc: 0.85, Val Acc: 1.0, LR: 2.703131975790572e-05
Epoch 290/1000, Train Loss: 0.6522782146930695, Val Loss: 0.07388703525066376, Train Acc: 0.85, Val Acc: 1.0, LR: 2.5824836436129122e-05
Epoch 300/1000, Train Loss: 0.9446093440055847, Val Loss: 0.07501218467950821, Train Acc: 0.825, Val Acc: 1.0, LR: 2.461835311435252e-05
Epoch 310/1000, Train Loss: 0.7174839228391647, Val Loss: 0.06990166008472443, Train Acc: 0.9, Val Acc: 1.0, LR: 2.3411869792575926e-05
Epoch 320/1000, Train Loss: 0.6628154516220093, Val Loss: 0.06766746938228607, Train Acc: 0.95, Val Acc: 1.0, LR: 2.2205386470799338e-05
Epoch 330/1000, Train Loss: 1.0392833650112152, Val Loss: 0.06881083548069, Train Acc: 0.825, Val Acc: 1.0, LR: 2.099890314902274e-05
Epoch 340/1000, Train Loss: 1.1858644485473633, Val Loss: 0.06773333251476288, Train Acc: 0.8, Val Acc: 1.0, LR: 1.9792419827246155e-05
Epoch 350/1000, Train Loss: 0.7716329097747803, Val Loss: 0.06759678572416306, Train Acc: 0.8, Val Acc: 1.0, LR: 1.8585936505469564e-05
Epoch 360/1000, Train Loss: 0.7546572089195251, Val Loss: 0.06711934506893158, Train Acc: 0.85, Val Acc: 1.0, LR: 1.7379453183692966e-05
Epoch 370/1000, Train Loss: 0.6283934116363525, Val Loss: 0.0675366073846817, Train Acc: 0.8, Val Acc: 1.0, LR: 1.6172969861916378e-05
Epoch 380/1000, Train Loss: 0.7469992339611053, Val Loss: 0.06716285645961761, Train Acc: 0.9, Val Acc: 1.0, LR: 1.4966486540139795e-05
Epoch 390/1000, Train Loss: 0.8858752548694611, Val Loss: 0.06770478188991547, Train Acc: 0.8, Val Acc: 1.0, LR: 1.3760003218363197e-05
Epoch 400/1000, Train Loss: 1.0444374680519104, Val Loss: 0.06804957240819931, Train Acc: 0.775, Val Acc: 1.0, LR: 1.2553519896586599e-05
Epoch 410/1000, Train Loss: 0.9514710307121277, Val Loss: 0.0699252337217331, Train Acc: 0.8, Val Acc: 1.0, LR: 1.134703657481e-05
Epoch 420/1000, Train Loss: 1.498559594154358, Val Loss: 0.06971859186887741, Train Acc: 0.775, Val Acc: 1.0, LR: 1.0140553253033402e-05
Epoch 430/1000, Train Loss: 0.8255296647548676, Val Loss: 0.06829170882701874, Train Acc: 0.85, Val Acc: 1.0, LR: 8.934069931256804e-06
Epoch 440/1000, Train Loss: 0.6337501406669617, Val Loss: 0.06891027092933655, Train Acc: 0.875, Val Acc: 1.0, LR: 7.727586609480206e-06
Epoch 450/1000, Train Loss: 0.558443121612072, Val Loss: 0.06941564381122589, Train Acc: 0.925, Val Acc: 1.0, LR: 6.521103287703609e-06
Epoch 460/1000, Train Loss: 0.5104057341814041, Val Loss: 0.07039614021778107, Train Acc: 0.825, Val Acc: 1.0, LR: 5.314619965927011e-06
Epoch 470/1000, Train Loss: 0.9759826362133026, Val Loss: 0.07069922983646393, Train Acc: 0.775, Val Acc: 1.0, LR: 4.108136644150413e-06
Epoch 480/1000, Train Loss: 0.5485801696777344, Val Loss: 0.07128383219242096, Train Acc: 0.9, Val Acc: 1.0, LR: 2.901653322373815e-06
Epoch 490/1000, Train Loss: 0.9890135824680328, Val Loss: 0.07100903987884521, Train Acc: 0.825, Val Acc: 1.0, LR: 1.695170000597219e-06
Epoch 500/1000, Train Loss: 0.6778724640607834, Val Loss: 0.07066357135772705, Train Acc: 0.875, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 510/1000, Train Loss: 0.6902343705296516, Val Loss: 0.07058411091566086, Train Acc: 0.8, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 520/1000, Train Loss: 1.0656720399856567, Val Loss: 0.07057665288448334, Train Acc: 0.775, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 530/1000, Train Loss: 0.5839861333370209, Val Loss: 0.07043206691741943, Train Acc: 0.85, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 540/1000, Train Loss: 0.8269200026988983, Val Loss: 0.07045521587133408, Train Acc: 0.825, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 550/1000, Train Loss: 0.7772699892520905, Val Loss: 0.07056790590286255, Train Acc: 0.8, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 560/1000, Train Loss: 0.9213889837265015, Val Loss: 0.0705106183886528, Train Acc: 0.825, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 570/1000, Train Loss: 1.5287907868623734, Val Loss: 0.07056870311498642, Train Acc: 0.775, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 580/1000, Train Loss: 0.6250942945480347, Val Loss: 0.07063400745391846, Train Acc: 0.8, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 590/1000, Train Loss: 1.0325307548046112, Val Loss: 0.07087396830320358, Train Acc: 0.75, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 600/1000, Train Loss: 0.5639871656894684, Val Loss: 0.07104147970676422, Train Acc: 0.925, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 610/1000, Train Loss: 0.8595083653926849, Val Loss: 0.07099111378192902, Train Acc: 0.85, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 620/1000, Train Loss: 0.8384020924568176, Val Loss: 0.07083763927221298, Train Acc: 0.775, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 630/1000, Train Loss: 0.816444456577301, Val Loss: 0.0706770047545433, Train Acc: 0.825, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 640/1000, Train Loss: 0.428455650806427, Val Loss: 0.07077927887439728, Train Acc: 0.9, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 650/1000, Train Loss: 0.8473485112190247, Val Loss: 0.0708167776465416, Train Acc: 0.825, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 660/1000, Train Loss: 0.691859781742096, Val Loss: 0.07090093195438385, Train Acc: 0.85, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 670/1000, Train Loss: 1.0237179100513458, Val Loss: 0.07093824446201324, Train Acc: 0.825, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 680/1000, Train Loss: 0.302408367395401, Val Loss: 0.07090778648853302, Train Acc: 0.975, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 690/1000, Train Loss: 1.3555044531822205, Val Loss: 0.07076943665742874, Train Acc: 0.775, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 700/1000, Train Loss: 0.8329211175441742, Val Loss: 0.07080929726362228, Train Acc: 0.9, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 710/1000, Train Loss: 0.9275246560573578, Val Loss: 0.07061968743801117, Train Acc: 0.85, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 720/1000, Train Loss: 0.6473455429077148, Val Loss: 0.07051379978656769, Train Acc: 0.875, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 730/1000, Train Loss: 0.7439321875572205, Val Loss: 0.07061122357845306, Train Acc: 0.825, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 740/1000, Train Loss: 0.5674411207437515, Val Loss: 0.0706324428319931, Train Acc: 0.9, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 750/1000, Train Loss: 0.5323095768690109, Val Loss: 0.07037578523159027, Train Acc: 0.9, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 760/1000, Train Loss: 1.1104645133018494, Val Loss: 0.07024034112691879, Train Acc: 0.75, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 770/1000, Train Loss: 0.9919795542955399, Val Loss: 0.0701654702425003, Train Acc: 0.95, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 780/1000, Train Loss: 0.7096185982227325, Val Loss: 0.07015536725521088, Train Acc: 0.825, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 790/1000, Train Loss: 0.6167304515838623, Val Loss: 0.07000143826007843, Train Acc: 0.85, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 800/1000, Train Loss: 0.6978275179862976, Val Loss: 0.06990991532802582, Train Acc: 0.875, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 810/1000, Train Loss: 0.3537066951394081, Val Loss: 0.07014115154743195, Train Acc: 0.925, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 820/1000, Train Loss: 0.474579319357872, Val Loss: 0.07017950713634491, Train Acc: 0.95, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 830/1000, Train Loss: 0.700206995010376, Val Loss: 0.07011119276285172, Train Acc: 0.9, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 840/1000, Train Loss: 0.5540483891963959, Val Loss: 0.07020813971757889, Train Acc: 0.825, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 850/1000, Train Loss: 0.6799343377351761, Val Loss: 0.07026039063930511, Train Acc: 0.8, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 860/1000, Train Loss: 0.9987328350543976, Val Loss: 0.07039586454629898, Train Acc: 0.85, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 870/1000, Train Loss: 0.7951763868331909, Val Loss: 0.07051386684179306, Train Acc: 0.825, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 880/1000, Train Loss: 0.5451759397983551, Val Loss: 0.07057784497737885, Train Acc: 0.85, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 890/1000, Train Loss: 1.2313884496688843, Val Loss: 0.07058919966220856, Train Acc: 0.825, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 900/1000, Train Loss: 1.1827736794948578, Val Loss: 0.07068207114934921, Train Acc: 0.825, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 910/1000, Train Loss: 1.1603743135929108, Val Loss: 0.07063368707895279, Train Acc: 0.875, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 920/1000, Train Loss: 0.45235928893089294, Val Loss: 0.0706644207239151, Train Acc: 0.95, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 930/1000, Train Loss: 0.8937805891036987, Val Loss: 0.07064128667116165, Train Acc: 0.85, Val Acc: 1.0, LR: 6.093350109982822e-07
[34m[1mwandb[39m[22m: [33mWARNING[39m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'Train Loss': 2.457655429840088, 'Validation Loss': 0.5735387206077576, 'Train Accuracy': 0.575, 'Validation Accuracy': 1.0, '_timestamp': 1717514720.89605}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.weight': {'_type': 'histogram', 'values': [3542, 3621, 3655, 3604, 3615, 3623, 3662, 3578, 3559, 3497, 3728, 3596, 3621, 3725, 3543, 3730, 3622, 3671, 3658, 3757, 3621, 3586, 3610, 3773, 3664, 3693, 3641, 3700, 3598, 3696, 3750, 3642, 3657, 3767, 3681, 3670, 3795, 3654, 3788, 3621, 3669, 3660, 3552, 3660, 3693, 3669, 3712, 3645, 3707, 3569, 3619, 3646, 3565, 3551, 3608, 3561, 3591, 3684, 3689, 3576, 3616, 3587, 3702, 3355], 'bins': [-0.10186440497636795, -0.09868113696575165, -0.09549786895513535, -0.09231460094451904, -0.08913133293390274, -0.08594806492328644, -0.08276479691267014, -0.07958152890205383, -0.07639826089143753, -0.07321499288082123, -0.07003172487020493, -0.06684846431016922, -0.06366519629955292, -0.060481924563646317, -0.057298656553030014, -0.05411538854241371, -0.05093212425708771, -0.047748856246471405, -0.0445655882358551, -0.0413823202252388, -0.0381990522146225, -0.035015784204006195, -0.03183251619338989, -0.02864924818277359, -0.025465980172157288, -0.022282714024186134, -0.019099446013569832, -0.01591617800295353, -0.012732909992337227, -0.009549642913043499, -0.0063663749024271965, -0.0031831073574721813, 1.601874828338623e-07, 0.003183427732437849, 0.006366695277392864, 0.009549963288009167, 0.012733230367302895, 0.015916498377919197, 0.0190997663885355, 0.022283034399151802, 0.025466300547122955, 0.028649568557739258, 0.03183283656835556, 0.03501610457897186, 0.038199372589588165, 0.04138264060020447, 0.04456590861082077, 0.04774917662143707, 0.050932444632053375, 0.05411570891737938, 0.05729897692799568, 0.060482244938611984, 0.06366551667451859, 0.06684878468513489, 0.0700320452451706, 0.0732153132557869, 0.0763985812664032, 0.0795818492770195, 0.0827651172876358, 0.0859483852982521, 0.08913165330886841, 0.09231492131948471, 0.09549818933010101, 0.09868145734071732, 0.10186472535133362]}, '_timestamp': 1717514720.9024808}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.bias': {'_type': 'histogram', 'values': [4, 2, 5, 7, 5, 6, 4, 2, 4, 6, 7, 5, 3, 5, 1, 5, 1, 2, 6, 3, 1, 6, 10, 6, 3, 2, 3, 3, 2, 2, 6, 2, 7, 5, 3, 5, 7, 4, 3, 5, 6, 5, 3, 2, 6, 2, 3, 4, 2, 4, 4, 2, 6, 0, 8, 1, 5, 7, 2, 4, 5, 3, 4, 3], 'bins': [-0.03315461054444313, -0.03212086111307144, -0.031087109819054604, -0.030053358525037766, -0.029019607231020927, -0.02798585779964924, -0.0269521065056324, -0.025918355211615562, -0.024884605780243874, -0.023850854486227036, -0.022817103192210197, -0.02178335376083851, -0.02074960246682167, -0.019715851172804832, -0.018682101741433144, -0.017648350447416306, -0.016614601016044617, -0.015580849722027779, -0.01454709842801094, -0.013513348065316677, -0.012479597702622414, -0.011445846408605576, -0.010412096045911312, -0.009378345683217049, -0.00834459438920021, -0.007310844026505947, -0.006277093663811684, -0.005243342835456133, -0.004209592007100582, -0.0031758416444063187, -0.002142090816050768, -0.0011083402205258608, -7.458962500095367e-05, 0.0009591610287316144, 0.0019929115660488605, 0.0030266623944044113, 0.004060412757098675, 0.0050941635854542255, 0.006127914413809776, 0.00716166477650404, 0.008195415139198303, 0.009229166433215141, 0.010262916795909405, 0.011296667158603668, 0.012330418452620506, 0.01336416881531477, 0.014397919178009033, 0.015431670472025871, 0.01646542176604271, 0.017499171197414398, 0.018532922491431236, 0.019566671922802925, 0.020600423216819763, 0.0216341745108366, 0.02266792394220829, 0.023701675236225128, 0.024735426530241966, 0.025769175961613655, 0.026802927255630493, 0.02783667854964733, 0.02887042798101902, 0.029904179275035858, 0.030937930569052696, 0.031971681863069534, 0.03300543129444122]}, '_timestamp': 1717514720.902733}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.weight': {'_type': 'histogram', 'values': [146, 16, 9, 3, 0, 1, 2, 1, 2, 3, 3, 1, 2, 0, 2, 2, 2, 2, 1, 2, 2, 1, 0, 1, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 6, 2, 0, 2, 2, 0, 1, 2, 1, 2, 1, 3, 3, 2, 0, 0, 1, 1, 1, 0, 1, 2, 4, 4, 5], 'bins': [0.9998781085014343, 0.9998819231987, 0.9998857378959656, 0.9998895525932312, 0.9998933672904968, 0.9998971223831177, 0.9999009370803833, 0.9999047517776489, 0.9999085664749146, 0.9999123811721802, 0.9999161958694458, 0.9999200105667114, 0.999923825263977, 0.9999276399612427, 0.9999313950538635, 0.9999352097511292, 0.9999390244483948, 0.9999428391456604, 0.999946653842926, 0.9999504685401917, 0.9999542832374573, 0.9999580979347229, 0.9999619126319885, 0.9999656677246094, 0.999969482421875, 0.9999732971191406, 0.9999771118164062, 0.9999809265136719, 0.9999847412109375, 0.9999885559082031, 0.9999923706054688, 0.9999961853027344, 1.0, 1.0000038146972656, 1.0000076293945312, 1.0000114440917969, 1.0000152587890625, 1.0000189542770386, 1.0000227689743042, 1.0000265836715698, 1.0000303983688354, 1.000034213066101, 1.0000380277633667, 1.0000418424606323, 1.000045657157898, 1.0000494718551636, 1.0000532865524292, 1.0000571012496948, 1.0000609159469604, 1.000064730644226, 1.0000685453414917, 1.0000723600387573, 1.000076174736023, 1.0000799894332886, 1.0000838041305542, 1.0000874996185303, 1.000091314315796, 1.0000951290130615, 1.0000989437103271, 1.0001027584075928, 1.0001065731048584, 1.000110387802124, 1.0001142024993896, 1.0001180171966553, 1.000121831893921]}, '_timestamp': 1717514720.90292}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.bias': {'_type': 'histogram', 'values': [15, 6, 7, 3, 3, 16, 0, 0, 2, 0, 2, 1, 2, 5, 4, 0, 1, 0, 3, 1, 2, 2, 0, 2, 2, 1, 1, 4, 0, 0, 0, 0, 92, 0, 0, 0, 3, 1, 2, 0, 4, 0, 0, 1, 3, 0, 1, 0, 3, 1, 1, 0, 5, 0, 3, 2, 2, 3, 19, 1, 4, 4, 9, 10], 'bins': [-0.00012188858818262815, -0.00011808600538643077, -0.00011428342259023339, -0.000110480839794036, -0.00010667825699783862, -0.00010287567420164123, -9.907309140544385e-05, -9.527050860924646e-05, -9.146792581304908e-05, -8.76653430168517e-05, -8.386276022065431e-05, -8.006017742445692e-05, -7.625759462825954e-05, -7.245501183206216e-05, -6.865242903586477e-05, -6.484984623966739e-05, -6.104726344347e-05, -5.724467700929381e-05, -5.3442094213096425e-05, -4.963951141689904e-05, -4.5836928620701656e-05, -4.203434582450427e-05, -3.8231763028306887e-05, -3.44291802321095e-05, -3.062659379793331e-05, -2.682401282072533e-05, -2.3021430024527945e-05, -1.921884722833056e-05, -1.5416262613143772e-05, -1.1613680726441089e-05, -7.811097020749003e-06, -4.008514224551618e-06, -2.059314283542335e-07, 3.5966515952168265e-06, 7.3992346187878866e-06, 1.1201817869732622e-05, 1.5004400665930007e-05, 1.8806984371622093e-05, 2.2609567167819478e-05, 2.6412149964016862e-05, 3.0214732760214247e-05, 3.4017317375401035e-05, 3.781990017159842e-05, 4.1622482967795804e-05, 4.542506576399319e-05, 4.922764856019057e-05, 5.303023135638796e-05, 5.683281415258534e-05, 6.063539694878273e-05, 6.443798338295892e-05, 6.82405661791563e-05, 7.204314897535369e-05, 7.584573177155107e-05, 7.964831456774846e-05, 8.345089736394584e-05, 8.725348016014323e-05, 9.105606295634061e-05, 9.4858645752538e-05, 9.866122854873538e-05, 0.00010246381134493276, 0.00010626639414113015, 0.00011006897693732753, 0.00011387155973352492, 0.0001176741425297223, 0.00012147672532591969]}, '_timestamp': 1717514720.9030862}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.weight': {'_type': 'histogram', 'values': [1000, 1027, 1060, 1045, 1093, 1064, 1071, 1075, 1056, 994, 1028, 978, 1015, 1046, 1014, 1108, 1053, 1074, 1050, 1029, 1097, 1079, 1107, 1010, 1145, 1053, 1012, 1042, 1054, 1047, 1016, 1070, 1046, 1064, 1116, 1040, 1021, 1066, 1059, 1016, 1073, 1102, 1051, 1031, 1043, 1034, 1017, 1081, 1015, 1087, 1057, 1042, 1066, 1060, 1006, 1071, 1101, 1003, 1077, 987, 1020, 1015, 1048, 954], 'bins': [-0.1522901952266693, -0.1475311517715454, -0.1427721232175827, -0.1380130797624588, -0.1332540363073349, -0.1284950077533722, -0.12373596429824829, -0.11897692829370499, -0.11421789228916168, -0.10945884883403778, -0.10469981282949448, -0.09994077682495117, -0.09518174082040787, -0.09042269736528397, -0.08566366136074066, -0.08090462535619736, -0.07614558935165405, -0.07138654589653015, -0.06662750989198685, -0.061868470162153244, -0.05710943415760994, -0.05235039442777634, -0.04759135842323303, -0.04283231869339943, -0.038073278963565826, -0.03331424295902252, -0.02855520509183407, -0.023796167224645615, -0.01903712749481201, -0.014278090558946133, -0.009519051760435104, -0.004760013893246651, -9.760260581970215e-07, 0.004758061841130257, 0.00951709970831871, 0.014276138506829739, 0.019035175442695618, 0.02379421517252922, 0.028553253039717674, 0.03331229090690613, 0.03807132691144943, 0.042830366641283035, 0.04758940637111664, 0.05234844237565994, 0.057107482105493546, 0.06186651811003685, 0.06662555783987045, 0.07138459384441376, 0.07614363729953766, 0.08090267330408096, 0.08566170930862427, 0.09042074531316757, 0.09517978876829147, 0.09993882477283478, 0.10469786077737808, 0.10945689678192139, 0.11421594023704529, 0.11897497624158859, 0.1237340122461319, 0.1284930557012558, 0.1332520842552185, 0.1380111277103424, 0.1427701711654663, 0.14752919971942902, 0.15228824317455292]}, '_timestamp': 1717514720.903639}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.bias': {'_type': 'histogram', 'values': [2, 12, 4, 5, 5, 2, 6, 4, 5, 2, 3, 5, 1, 6, 4, 7, 2, 4, 5, 1, 5, 5, 4, 2, 4, 5, 2, 1, 7, 4, 2, 2, 7, 7, 1, 4, 2, 2, 7, 0, 8, 5, 3, 3, 3, 3, 5, 4, 5, 4, 2, 7, 5, 1, 5, 2, 4, 5, 6, 5, 4, 1, 4, 7], 'bins': [-0.0616397000849247, -0.05971084162592888, -0.05778198689222336, -0.05585312843322754, -0.05392426997423172, -0.0519954152405262, -0.05006655678153038, -0.04813770204782486, -0.04620884358882904, -0.04427998512983322, -0.0423511303961277, -0.04042227193713188, -0.03849341720342636, -0.03656455874443054, -0.03463570028543472, -0.0327068455517292, -0.030777987092733383, -0.028849130496382713, -0.026920273900032043, -0.024991415441036224, -0.023062558844685555, -0.021133702248334885, -0.019204843789339066, -0.017275987192988396, -0.015347130596637726, -0.013418274000287056, -0.011489416472613811, -0.009560558944940567, -0.007631702348589897, -0.00570284528657794, -0.003773988224565983, -0.0018451311625540257, 8.372589945793152e-05, 0.0020125829614698887, 0.003941440023481846, 0.005870297085493803, 0.00779915414750576, 0.00972801074385643, 0.011656868271529675, 0.013585725799202919, 0.015514582395553589, 0.01744343899190426, 0.01937229558825493, 0.021301154047250748, 0.023230010643601418, 0.025158867239952087, 0.027087725698947906, 0.029016582295298576, 0.030945438891649246, 0.032874297350645065, 0.034803152084350586, 0.036732010543346405, 0.038660869002342224, 0.040589723736047745, 0.042518582195043564, 0.044447436928749084, 0.046376295387744904, 0.04830515384674072, 0.05023400858044624, 0.05216286703944206, 0.05409172177314758, 0.0560205802321434, 0.05794943869113922, 0.05987829342484474, 0.06180715188384056]}, '_timestamp': 1717514720.9038012}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.weight': {'_type': 'histogram', 'values': [57, 23, 19, 15, 12, 1, 1, 0, 2, 1, 5, 3, 2, 3, 0, 2, 4, 0, 0, 1, 4, 5, 1, 3, 0, 3, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 3, 3, 2, 1, 1, 1, 2, 1, 3, 3, 4, 2, 3, 1, 2, 2, 2, 4, 2, 7, 4, 2, 3, 4, 6, 12], 'bins': [0.9998781085014343, 0.9998819231987, 0.9998857378959656, 0.9998895525932312, 0.9998933672904968, 0.9998971223831177, 0.9999009370803833, 0.9999047517776489, 0.9999085664749146, 0.9999123811721802, 0.9999161958694458, 0.9999200105667114, 0.999923825263977, 0.9999276399612427, 0.9999313950538635, 0.9999352097511292, 0.9999390244483948, 0.9999428391456604, 0.999946653842926, 0.9999504685401917, 0.9999542832374573, 0.9999580979347229, 0.9999619126319885, 0.9999656677246094, 0.999969482421875, 0.9999732971191406, 0.9999771118164062, 0.9999809265136719, 0.9999847412109375, 0.9999885559082031, 0.9999923706054688, 0.9999961853027344, 1.0, 1.0000038146972656, 1.0000076293945312, 1.0000114440917969, 1.0000152587890625, 1.0000189542770386, 1.0000227689743042, 1.0000265836715698, 1.0000303983688354, 1.000034213066101, 1.0000380277633667, 1.0000418424606323, 1.000045657157898, 1.0000494718551636, 1.0000532865524292, 1.0000571012496948, 1.0000609159469604, 1.000064730644226, 1.0000685453414917, 1.0000723600387573, 1.000076174736023, 1.0000799894332886, 1.0000838041305542, 1.0000874996185303, 1.000091314315796, 1.0000951290130615, 1.0000989437103271, 1.0001027584075928, 1.0001065731048584, 1.000110387802124, 1.0001142024993896, 1.0001180171966553, 1.000121831893921]}, '_timestamp': 1717514720.903957}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.bias': {'_type': 'histogram', 'values': [24, 6, 9, 12, 7, 22, 4, 4, 2, 2, 2, 1, 2, 2, 1, 4, 0, 1, 4, 2, 4, 1, 3, 0, 7, 3, 2, 2, 0, 0, 0, 0, 3, 0, 0, 0, 1, 4, 3, 3, 3, 1, 3, 6, 0, 3, 0, 2, 2, 5, 2, 2, 4, 1, 1, 2, 4, 2, 13, 2, 9, 7, 11, 27], 'bins': [-0.00012188887922093272, -0.00011807988630607724, -0.00011427089339122176, -0.00011046190047636628, -0.0001066529075615108, -0.00010284391464665532, -9.903492173179984e-05, -9.522592881694436e-05, -9.141693590208888e-05, -8.76079429872334e-05, -8.379895007237792e-05, -7.998995715752244e-05, -7.618096424266696e-05, -7.237197132781148e-05, -6.8562978412956e-05, -6.475398549810052e-05, -6.0944996221223846e-05, -5.7136003306368366e-05, -5.3327010391512886e-05, -4.9518017476657405e-05, -4.5709024561801925e-05, -4.1900031646946445e-05, -3.8091038732090965e-05, -3.4282045817235485e-05, -3.0473054721369408e-05, -2.6664061806513928e-05, -2.2855068891658448e-05, -1.9046075976802967e-05, -1.5237083971442189e-05, -1.1428091056586709e-05, -7.6190985964785796e-06, -3.810105908996775e-06, -1.1132215149700642e-09, 3.8078794659668347e-06, 7.6168721534486394e-06, 1.1425864613556769e-05, 1.5234857528412249e-05, 1.9043849533773027e-05, 2.2852842448628508e-05, 2.6661835363483988e-05, 3.0470828278339468e-05, 3.4279819374205545e-05, 3.8088812289061025e-05, 4.1897805203916505e-05, 4.5706798118771985e-05, 4.9515791033627465e-05, 5.3324783948482946e-05, 5.7133776863338426e-05, 6.0942769778193906e-05, 6.475175905507058e-05, 6.856075196992606e-05, 7.236974488478154e-05, 7.617873779963702e-05, 7.99877307144925e-05, 8.379672362934798e-05, 8.760571654420346e-05, 9.141470945905894e-05, 9.522370237391442e-05, 9.90326952887699e-05, 0.00010284168820362538, 0.00010665068111848086, 0.00011045967403333634, 0.00011426866694819182, 0.0001180776598630473, 0.00012188665277790278]}, '_timestamp': 1717514720.904107}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.weight': {'_type': 'histogram', 'values': [1015, 1043, 1092, 1050, 1024, 968, 1030, 1013, 1026, 1075, 1029, 1093, 1035, 1046, 1083, 1046, 1097, 1081, 1026, 1088, 1028, 1038, 1016, 1076, 990, 1041, 1066, 1080, 1047, 1001, 999, 1059, 1018, 1033, 1046, 1022, 1069, 1054, 1050, 1078, 1047, 1039, 1050, 998, 1016, 1017, 1031, 1064, 1068, 1109, 1072, 1087, 1105, 1095, 986, 1067, 1045, 1062, 1022, 1084, 989, 1053, 1143, 1061], 'bins': [-0.15228360891342163, -0.1475241780281067, -0.14276474714279175, -0.1380053162574768, -0.13324588537216187, -0.12848645448684692, -0.12372703105211258, -0.11896760016679764, -0.1142081692814827, -0.10944873839616776, -0.10468931496143341, -0.09992988407611847, -0.09517045319080353, -0.09041102230548859, -0.08565159142017365, -0.0808921605348587, -0.07613272964954376, -0.07137330621480942, -0.06661387532949448, -0.061854444444179535, -0.057095013558864594, -0.05233558639883995, -0.04757615551352501, -0.04281672462821007, -0.038057297468185425, -0.03329786658287048, -0.028538435697555542, -0.02377900667488575, -0.019019577652215958, -0.014260146766901016, -0.009500717744231224, -0.004741287790238857, 1.814216375350952e-05, 0.004777572117745876, 0.009537002071738243, 0.014296431094408035, 0.019055861979722977, 0.02381529100239277, 0.02857472002506256, 0.0333341509103775, 0.038093581795692444, 0.04285300895571709, 0.04761243984103203, 0.05237187072634697, 0.05713129788637161, 0.061890728771686554, 0.0666501596570015, 0.07140959054231644, 0.07616901397705078, 0.08092844486236572, 0.08568787574768066, 0.0904473066329956, 0.09520673751831055, 0.09996616840362549, 0.10472559928894043, 0.10948502272367477, 0.11424445360898972, 0.11900388449430466, 0.1237633153796196, 0.12852273881435394, 0.13328216969966888, 0.13804160058498383, 0.14280103147029877, 0.1475604623556137, 0.15231989324092865]}, '_timestamp': 1717514720.904674}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.bias': {'_type': 'histogram', 'values': [5, 6, 6, 5, 2, 5, 4, 6, 5, 3, 7, 4, 4, 2, 3, 8, 3, 2, 9, 5, 4, 3, 5, 6, 5, 6, 4, 9, 4, 3, 5, 2, 4, 2, 2, 5, 1, 5, 2, 5, 3, 2, 2, 7, 4, 2, 6, 5, 4, 2, 4, 8, 3, 2, 2, 5, 0, 5, 3, 2, 2, 1, 5, 4], 'bins': [-0.0616849921643734, -0.05975138396024704, -0.05781777203083038, -0.055884163826704025, -0.05395055189728737, -0.05201694369316101, -0.05008333548903465, -0.048149723559617996, -0.04621611535549164, -0.04428250342607498, -0.042348895221948624, -0.040415287017822266, -0.03848167508840561, -0.03654806688427925, -0.03461445868015289, -0.03268084675073624, -0.03074723854660988, -0.02881362847983837, -0.026880018413066864, -0.024946408346295357, -0.02301279827952385, -0.02107919007539749, -0.019145580008625984, -0.017211969941854477, -0.01527835987508297, -0.013344750739634037, -0.01141114067286253, -0.009477531537413597, -0.00754392147064209, -0.00561031186953187, -0.00367670226842165, -0.00174309266731143, 0.00019051693379878998, 0.00212412653490901, 0.00405773613601923, 0.00599134573712945, 0.00792495533823967, 0.009858565405011177, 0.01179217454046011, 0.013725784607231617, 0.01565939374268055, 0.017593003809452057, 0.019526613876223564, 0.02146022394299507, 0.02339383214712143, 0.025327442213892937, 0.027261052280664444, 0.02919466234743595, 0.03112827241420746, 0.033061880618333817, 0.03499549254775047, 0.03692910075187683, 0.03886270895600319, 0.040796320885419846, 0.042729929089546204, 0.04466353729367256, 0.04659714922308922, 0.048530757427215576, 0.05046436935663223, 0.05239797756075859, 0.05433158576488495, 0.056265197694301605, 0.05819880589842796, 0.06013241782784462, 0.06206602603197098]}, '_timestamp': 1717514720.9048278}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.weight': {'_type': 'histogram', 'values': [68, 25, 14, 22, 7, 8, 10, 4, 3, 1, 4, 2, 1, 1, 1, 3, 3, 0, 1, 1, 3, 4, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 2, 2, 1, 3, 3, 0, 3, 2, 2, 3, 4, 3, 1, 1, 3, 3, 0, 4, 1, 1, 1, 0, 3, 3, 3, 9], 'bins': [0.9998781085014343, 0.9998819231987, 0.9998857378959656, 0.9998895525932312, 0.999893307685852, 0.9998971223831177, 0.9999009370803833, 0.9999047517776489, 0.9999085664749146, 0.9999123811721802, 0.9999161958694458, 0.9999199509620667, 0.9999237656593323, 0.9999275803565979, 0.9999313950538635, 0.9999352097511292, 0.9999390244483948, 0.9999428391456604, 0.9999465942382812, 0.9999504089355469, 0.9999542236328125, 0.9999580383300781, 0.9999618530273438, 0.9999656677246094, 0.999969482421875, 0.9999732375144958, 0.9999770522117615, 0.9999808669090271, 0.9999846816062927, 0.9999884963035583, 0.999992311000824, 0.9999961256980896, 0.9999998807907104, 1.000003695487976, 1.0000075101852417, 1.0000113248825073, 1.000015139579773, 1.0000189542770386, 1.0000227689743042, 1.0000265836715698, 1.0000303983688354, 1.000034213066101, 1.0000380277633667, 1.0000417232513428, 1.0000455379486084, 1.000049352645874, 1.0000531673431396, 1.0000569820404053, 1.000060796737671, 1.0000646114349365, 1.0000684261322021, 1.0000722408294678, 1.0000760555267334, 1.000079870223999, 1.0000836849212646, 1.0000874996185303, 1.000091314315796, 1.000095009803772, 1.0000988245010376, 1.0001026391983032, 1.0001064538955688, 1.0001102685928345, 1.0001140832901, 1.0001178979873657, 1.0001217126846313]}, '_timestamp': 1717514720.90498}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.bias': {'_type': 'histogram', 'values': [44, 13, 7, 9, 6, 12, 0, 1, 0, 4, 4, 3, 5, 2, 2, 5, 2, 3, 4, 1, 3, 5, 2, 0, 2, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 4, 4, 3, 1, 4, 1, 3, 1, 1, 2, 3, 2, 2, 4, 3, 3, 3, 0, 4, 3, 10, 5, 9, 8, 6, 16], 'bins': [-0.00012188714026706293, -0.00011808080307673663, -0.00011427445861045271, -0.00011046812142012641, -0.0001066617842298001, -0.0001028554470394738, -9.904910257318988e-05, -9.524276538286358e-05, -9.143642819253728e-05, -8.763009100221097e-05, -8.382374653592706e-05, -8.001740934560075e-05, -7.621107215527445e-05, -7.240473496494815e-05, -6.859839049866423e-05, -6.479205330833793e-05, -6.0985716118011624e-05, -5.717937528970651e-05, -5.33730344614014e-05, -4.95666972710751e-05, -4.576035644276999e-05, -4.1954019252443686e-05, -3.8147678424138576e-05, -3.434134123381227e-05, -3.053500040550716e-05, -2.6728661396191455e-05, -2.292232238687575e-05, -1.9115983377560042e-05, -1.5309644368244335e-05, -1.1503305358928628e-05, -7.696966349612921e-06, -3.890627340297215e-06, -8.42883309815079e-08, 3.722050678334199e-06, 7.528389687649906e-06, 1.1334728696965612e-05, 1.514106770628132e-05, 1.8947406715597026e-05, 2.2753745724912733e-05, 2.656008473422844e-05, 3.0366423743544146e-05, 3.417276457184926e-05, 3.797910176217556e-05, 4.178544259048067e-05, 4.5591779780806974e-05, 4.9398120609112084e-05, 5.320445779943839e-05, 5.70107986277435e-05, 6.081713945604861e-05, 6.462347664637491e-05, 6.842981383670121e-05, 7.223615830298513e-05, 7.604249549331143e-05, 7.984883268363774e-05, 8.365516987396404e-05, 8.746151434024796e-05, 9.126785153057426e-05, 9.507418872090057e-05, 9.888052591122687e-05, 0.00010268687037751079, 0.00010649320756783709, 0.00011029954475816339, 0.0001141058819484897, 0.00011791222641477361, 0.00012171856360509992]}, '_timestamp': 1717514720.9051242}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.weight': {'_type': 'histogram', 'values': [7, 6, 6, 7, 2, 2, 6, 3, 1, 3, 4, 2, 4, 1, 6, 4, 3, 4, 3, 2, 5, 4, 4, 4, 7, 4, 4, 5, 3, 4, 2, 5, 3, 5, 5, 1, 6, 8, 3, 2, 3, 4, 3, 1, 9, 6, 5, 5, 4, 4, 5, 4, 6, 1, 3, 6, 0, 4, 5, 2, 4, 7, 4, 4], 'bins': [-0.2112186849117279, -0.20458561182022095, -0.197952538728714, -0.19131946563720703, -0.18468640744686127, -0.1780533343553543, -0.17142026126384735, -0.1647871881723404, -0.15815411508083344, -0.15152104198932648, -0.14488796889781952, -0.13825491070747375, -0.1316218376159668, -0.12498876452445984, -0.11835569143295288, -0.11172261834144592, -0.10508954524993896, -0.0984564796090126, -0.09182340651750565, -0.08519033342599869, -0.07855726778507233, -0.07192419469356537, -0.06529112160205841, -0.05865805223584175, -0.05202498286962509, -0.045391909778118134, -0.038758840411901474, -0.032125767320394516, -0.025492697954177856, -0.018859626725316048, -0.012226555496454239, -0.00559348426759243, 0.0010395869612693787, 0.0076726581901311874, 0.014305729418992996, 0.020938800647854805, 0.027571871876716614, 0.03420494124293327, 0.04083801433444023, 0.04747108370065689, 0.05410415679216385, 0.06073722615838051, 0.06737029552459717, 0.07400336861610413, 0.08063644170761108, 0.08726950734853745, 0.0939025804400444, 0.10053565353155136, 0.10716871917247772, 0.11380179226398468, 0.12043486535549164, 0.1270679384469986, 0.13370101153850555, 0.1403340846300125, 0.14696714282035828, 0.15360021591186523, 0.1602332890033722, 0.16686636209487915, 0.1734994351863861, 0.18013250827789307, 0.18676558136940002, 0.1933986395597458, 0.20003171265125275, 0.2066647857427597, 0.21329785883426666]}, '_timestamp': 1717514720.90528}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.bias': {'_type': 'histogram', 'values': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'bins': [-0.4471670091152191, -0.4315420091152191, -0.4159170091152191, -0.4002920091152191, -0.3846670091152191, -0.3690420091152191, -0.3534170091152191, -0.3377920091152191, -0.3221670091152191, -0.3065420091152191, -0.2909170091152191, -0.2752920091152191, -0.2596670091152191, -0.24404200911521912, -0.22841700911521912, -0.21279200911521912, -0.19716700911521912, -0.18154200911521912, -0.16591700911521912, -0.15029200911521912, -0.13466700911521912, -0.11904200911521912, -0.10341700911521912, -0.08779200911521912, -0.07216700911521912, -0.056542012840509415, -0.040917012840509415, -0.025292012840509415, -0.009667012840509415, 0.005957987159490585, 0.021582987159490585, 0.037207987159490585, 0.052832987159490585, 0.06845799088478088, 0.08408299088478088, 0.09970799088478088, 0.11533299088478088, 0.13095799088478088, 0.14658299088478088, 0.16220799088478088, 0.17783299088478088, 0.19345799088478088, 0.20908299088478088, 0.22470799088478088, 0.24033299088478088, 0.2559579908847809, 0.2715829908847809, 0.2872079908847809, 0.3028329908847809, 0.3184579908847809, 0.3340829908847809, 0.3497079908847809, 0.3653329908847809, 0.3809579908847809, 0.3965829908847809, 0.4122079908847809, 0.4278329908847809, 0.4434579908847809, 0.4590829908847809, 0.4747079908847809, 0.4903329908847809, 0.5059579610824585, 0.5215829610824585, 0.5372079610824585, 0.5528329610824585]}, '_timestamp': 1717514720.905453}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TP Reward Distribution': {'_type': 'histogram', 'values': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [2.8666666666666667, 3.153125, 3.439583333333333, 3.7260416666666667, 4.0125, 4.298958333333333, 4.585416666666667, 4.871874999999999, 5.158333333333333, 5.444791666666667, 5.731249999999999, 6.017708333333333, 6.304166666666667, 6.590624999999999, 6.877083333333333, 7.163541666666667, 7.449999999999999, 7.736458333333333, 8.022916666666667, 8.309375, 8.595833333333333, 8.882291666666667, 9.16875, 9.455208333333333, 9.741666666666667, 10.028125, 10.314583333333333, 10.601041666666665, 10.8875, 11.173958333333333, 11.460416666666667, 11.746875, 12.033333333333333, 12.319791666666667, 12.60625, 12.892708333333333, 13.179166666666667, 13.465625, 13.752083333333333, 14.038541666666667, 14.325, 14.611458333333333, 14.897916666666667, 15.184375, 15.470833333333333, 15.757291666666667, 16.04375, 16.33020833333333, 16.616666666666667, 16.903125, 17.18958333333333, 17.476041666666664, 17.7625, 18.04895833333333, 18.335416666666664, 18.621875, 18.90833333333333, 19.194791666666667, 19.48125, 19.76770833333333, 20.054166666666667, 20.340625, 20.62708333333333, 20.913541666666667, 21.2]}, '_timestamp': 1717514720.905617}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TN Reward Distribution': {'_type': 'histogram', 'values': [4, 3, 1, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [2.6666666666666665, 5.452083333333333, 8.2375, 11.022916666666667, 13.808333333333334, 16.59375, 19.37916666666667, 22.164583333333336, 24.950000000000003, 27.73541666666667, 30.520833333333336, 33.30625, 36.09166666666667, 38.87708333333333, 41.6625, 44.447916666666664, 47.233333333333334, 50.018750000000004, 52.80416666666667, 55.58958333333334, 58.375, 61.16041666666667, 63.94583333333333, 66.73125, 69.51666666666668, 72.30208333333334, 75.0875, 77.87291666666668, 80.65833333333335, 83.44375000000001, 86.22916666666667, 89.01458333333335, 91.80000000000001, 94.58541666666667, 97.37083333333335, 100.15625000000001, 102.94166666666668, 105.72708333333334, 108.51250000000002, 111.29791666666668, 114.08333333333334, 116.86875000000002, 119.65416666666668, 122.43958333333335, 125.22500000000001, 128.01041666666669, 130.79583333333332, 133.58125, 136.36666666666667, 139.15208333333334, 141.9375, 144.72291666666666, 147.50833333333333, 150.29375, 153.07916666666668, 155.86458333333334, 158.65, 161.43541666666667, 164.22083333333333, 167.00625, 169.79166666666666, 172.57708333333335, 175.3625, 178.14791666666667, 180.93333333333334]}, '_timestamp': 1717514720.905764}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FP Reward Distribution': {'_type': 'histogram', 'values': [3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [2.6666666666666665, 6.832291666666666, 10.997916666666665, 15.163541666666665, 19.329166666666666, 23.494791666666664, 27.660416666666666, 31.826041666666665, 35.99166666666666, 40.15729166666666, 44.32291666666666, 48.488541666666656, 52.65416666666666, 56.81979166666666, 60.98541666666666, 65.15104166666666, 69.31666666666666, 73.48229166666667, 77.64791666666666, 81.81354166666667, 85.97916666666666, 90.14479166666666, 94.31041666666665, 98.47604166666666, 102.64166666666667, 106.80729166666666, 110.97291666666666, 115.13854166666665, 119.30416666666666, 123.46979166666665, 127.63541666666666, 131.80104166666663, 135.96666666666664, 140.13229166666665, 144.29791666666665, 148.46354166666663, 152.62916666666663, 156.79479166666664, 160.96041666666665, 165.12604166666662, 169.29166666666663, 173.45729166666663, 177.62291666666664, 181.78854166666665, 185.95416666666662, 190.11979166666663, 194.28541666666663, 198.45104166666664, 202.61666666666665, 206.78229166666662, 210.94791666666663, 215.11354166666663, 219.27916666666664, 223.44479166666662, 227.61041666666662, 231.77604166666663, 235.94166666666663, 240.10729166666664, 244.27291666666662, 248.43854166666662, 252.60416666666663, 256.76979166666666, 260.93541666666664, 265.1010416666667, 269.26666666666665]}, '_timestamp': 1717514720.905905}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FN Reward Distribution': {'_type': 'histogram', 'values': [3, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [4.0, 5.019791666666666, 6.039583333333333, 7.059375, 8.079166666666666, 9.098958333333332, 10.11875, 11.138541666666667, 12.158333333333333, 13.178125, 14.197916666666666, 15.217708333333333, 16.2375, 17.257291666666667, 18.277083333333334, 19.296875, 20.316666666666666, 21.336458333333333, 22.35625, 23.376041666666666, 24.395833333333332, 25.415625, 26.435416666666665, 27.45520833333333, 28.475, 29.494791666666668, 30.514583333333334, 31.534375, 32.55416666666667, 33.57395833333334, 34.59375, 35.61354166666666, 36.63333333333333, 37.653125, 38.672916666666666, 39.692708333333336, 40.7125, 41.73229166666667, 42.75208333333333, 43.771875, 44.791666666666664, 45.811458333333334, 46.83125, 47.85104166666667, 48.87083333333333, 49.890625, 50.91041666666666, 51.93020833333333, 52.95, 53.969791666666666, 54.989583333333336, 56.009375, 57.02916666666667, 58.04895833333333, 59.06875, 60.088541666666664, 61.108333333333334, 62.128125, 63.14791666666667, 64.16770833333334, 65.1875, 66.20729166666666, 67.22708333333333, 68.246875, 69.26666666666667]}, '_timestamp': 1717514720.906039}).
Epoch 940/1000, Train Loss: 1.4142634272575378, Val Loss: 0.07061456143856049, Train Acc: 0.85, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 950/1000, Train Loss: 0.9710682928562164, Val Loss: 0.07038748264312744, Train Acc: 0.825, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 960/1000, Train Loss: 0.8743141293525696, Val Loss: 0.0703282281756401, Train Acc: 0.85, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 970/1000, Train Loss: 0.5650189816951752, Val Loss: 0.07047000527381897, Train Acc: 0.85, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 980/1000, Train Loss: 0.8704051375389099, Val Loss: 0.07036751508712769, Train Acc: 0.775, Val Acc: 1.0, LR: 6.093350109982822e-07
Epoch 990/1000, Train Loss: 0.4563419073820114, Val Loss: 0.07031241059303284, Train Acc: 0.875, Val Acc: 1.0, LR: 6.093350109982822e-07
Finished training model...
Simulating on true reward function...
 ****** Running generation 0 ******
[33m[W 2024-06-04 11:25:38,012][39m Trial 0 failed with parameters: {'hidden_size': 259, 'learning_rate': 6.09335010998286e-05, 'weight_decay': 0.00040856221202477394} because of the following error: The value None could not be cast to float..
[33m[W 2024-06-04 11:25:38,012][39m Trial 0 failed with value None.
Population's average fitness: 61.57000 stdev: 96.83527
Best fitness: 354.66667 - size: (4, 20) - species 1 - id 15
Average adjusted fitness: 0.158
Mean genetic distance 1.171, standard deviation 0.287
Population of 20 members in 1 species:
   ID   age  size  fitness  adj fit  stag
  ====  ===  ====  =======  =======  ====
     1    0    20    354.7    0.158     0
Total extinctions: 0
Generation time: 0.908 sec
 ****** Running generation 1 ******
Population's average fitness: 332.31000 stdev: 898.34729
Best fitness: 4213.06667 - size: (4, 19) - species 1 - id 21
Average adjusted fitness: 0.077
Mean genetic distance 1.090, standard deviation 0.181
Population of 20 members in 1 species:
   ID   age  size  fitness  adj fit  stag
  ====  ===  ====  =======  =======  ====
     1    1    20   4213.1    0.077     0
Total extinctions: 0
Generation time: 2.565 sec (1.736 average)
 ****** Running generation 2 ******
Population's average fitness: 5302.50333 stdev: 12474.46777
Best fitness: 40889.33333 - size: (4, 19) - species 1 - id 45
Average adjusted fitness: 0.130
Mean genetic distance 1.273, standard deviation 0.227
Population of 20 members in 1 species:
   ID   age  size  fitness  adj fit  stag
  ====  ===  ====  =======  =======  ====
     1    2    20  40889.3    0.130     0
Total extinctions: 0
Generation time: 7.740 sec (3.737 average)
 ****** Running generation 3 ******