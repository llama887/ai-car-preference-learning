Epoch 0/1000, Train Loss: 1.7919546961784363, Val Loss: 0.5676552057266235, Train Acc: 0.65, Val Acc: 1.0, LR: 0.0004460884365472215
Epoch 10/1000, Train Loss: 2.1438083052635193, Val Loss: 0.46439141035079956, Train Acc: 0.5, Val Acc: 0.8999999761581421, LR: 0.00043723836235668925
Epoch 20/1000, Train Loss: 1.7733996510505676, Val Loss: 0.41228118538856506, Train Acc: 0.65, Val Acc: 0.8999999761581421, LR: 0.00042838828816615715
Epoch 30/1000, Train Loss: 1.4388065338134766, Val Loss: 0.4008238911628723, Train Acc: 0.65, Val Acc: 0.8999999761581421, LR: 0.00041953821397562477
Epoch 40/1000, Train Loss: 1.5938209295272827, Val Loss: 0.39184072613716125, Train Acc: 0.575, Val Acc: 0.8999999761581421, LR: 0.0004106881397850927
Epoch 50/1000, Train Loss: 1.7009316682815552, Val Loss: 0.3481186032295227, Train Acc: 0.6, Val Acc: 0.8999999761581421, LR: 0.00040183806559456045
Epoch 60/1000, Train Loss: 1.2911296486854553, Val Loss: 0.322796493768692, Train Acc: 0.65, Val Acc: 0.8999999761581421, LR: 0.0003929879914040282
Epoch 70/1000, Train Loss: 1.5948017239570618, Val Loss: 0.3145110011100769, Train Acc: 0.625, Val Acc: 0.8999999761581421, LR: 0.0003841379172134959
Epoch 80/1000, Train Loss: 0.9554427266120911, Val Loss: 0.3148449659347534, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 0.00037528784302296353
Epoch 90/1000, Train Loss: 1.5069159865379333, Val Loss: 0.3098464608192444, Train Acc: 0.55, Val Acc: 0.8999999761581421, LR: 0.00036643776883243126
Epoch 100/1000, Train Loss: 1.3436147570610046, Val Loss: 0.29154783487319946, Train Acc: 0.625, Val Acc: 0.8999999761581421, LR: 0.00035758769464189894
Epoch 110/1000, Train Loss: 1.106406331062317, Val Loss: 0.2731645703315735, Train Acc: 0.7, Val Acc: 0.8999999761581421, LR: 0.0003487376204513667
Epoch 120/1000, Train Loss: 0.836858332157135, Val Loss: 0.2771657705307007, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 0.00033988754626083456
Epoch 130/1000, Train Loss: 0.9188958704471588, Val Loss: 0.2772619128227234, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 0.00033103747207030235
Epoch 140/1000, Train Loss: 1.4862228631973267, Val Loss: 0.27705812454223633, Train Acc: 0.625, Val Acc: 0.8999999761581421, LR: 0.0003221873978797701
Epoch 150/1000, Train Loss: 0.9375553131103516, Val Loss: 0.2683759331703186, Train Acc: 0.675, Val Acc: 0.8999999761581421, LR: 0.0003133373236892379
Epoch 160/1000, Train Loss: 0.9098775386810303, Val Loss: 0.2672758996486664, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 0.0003044872494987056
Epoch 170/1000, Train Loss: 1.1469220519065857, Val Loss: 0.2708212733268738, Train Acc: 0.7, Val Acc: 0.8999999761581421, LR: 0.0002956371753081734
Epoch 180/1000, Train Loss: 1.0866633653640747, Val Loss: 0.2976894974708557, Train Acc: 0.675, Val Acc: 0.8999999761581421, LR: 0.00028678710111764105
Epoch 190/1000, Train Loss: 1.1450498700141907, Val Loss: 0.26995301246643066, Train Acc: 0.7, Val Acc: 0.8999999761581421, LR: 0.0002779370269271088
Epoch 200/1000, Train Loss: 1.0081300139427185, Val Loss: 0.2747139632701874, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 0.0002690869527365764
Epoch 210/1000, Train Loss: 1.0149422883987427, Val Loss: 0.2717569172382355, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 0.0002602368785460441
Epoch 220/1000, Train Loss: 0.9302813410758972, Val Loss: 0.26752397418022156, Train Acc: 0.675, Val Acc: 0.8999999761581421, LR: 0.00025138680435551187
Epoch 230/1000, Train Loss: 1.0347410142421722, Val Loss: 0.2587742507457733, Train Acc: 0.7, Val Acc: 0.8999999761581421, LR: 0.00024253673016497965
Epoch 240/1000, Train Loss: 0.8691478371620178, Val Loss: 0.2652735710144043, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.0002336866559744474
Epoch 250/1000, Train Loss: 1.212595283985138, Val Loss: 0.2611503005027771, Train Acc: 0.675, Val Acc: 0.8999999761581421, LR: 0.0002248365817839152
Epoch 260/1000, Train Loss: 0.6646548211574554, Val Loss: 0.25522422790527344, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 0.00021598650759338298
Epoch 270/1000, Train Loss: 0.8026880323886871, Val Loss: 0.26270583271980286, Train Acc: 0.7, Val Acc: 0.8999999761581421, LR: 0.00020713643340285076
Epoch 280/1000, Train Loss: 0.8107655942440033, Val Loss: 0.254640132188797, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.00019828635921231855
Epoch 290/1000, Train Loss: 0.809511810541153, Val Loss: 0.26003170013427734, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 0.00018943628502178633
Epoch 300/1000, Train Loss: 0.9300324022769928, Val Loss: 0.2613268196582794, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.00018058621083125412
Epoch 310/1000, Train Loss: 0.7759689092636108, Val Loss: 0.2634775638580322, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.0001717361366407219
Epoch 320/1000, Train Loss: 0.8081590235233307, Val Loss: 0.2581325173377991, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 0.00016288606245018969
Epoch 330/1000, Train Loss: 1.0874244570732117, Val Loss: 0.25272971391677856, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 0.00015403598825965747
Epoch 340/1000, Train Loss: 0.9457448124885559, Val Loss: 0.24349550902843475, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.00014518591406912525
Epoch 350/1000, Train Loss: 0.9313219487667084, Val Loss: 0.2603086829185486, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.00013633583987859304
Epoch 360/1000, Train Loss: 0.7127208113670349, Val Loss: 0.25255686044692993, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 0.00012748576568806082
Epoch 370/1000, Train Loss: 0.8075311779975891, Val Loss: 0.24648067355155945, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 0.00011863569149752861
Epoch 380/1000, Train Loss: 0.9464333653450012, Val Loss: 0.2401132881641388, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 0.00010978561730699638
Epoch 390/1000, Train Loss: 0.928386002779007, Val Loss: 0.24621614813804626, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 0.00010093554311646416
Epoch 400/1000, Train Loss: 0.7530174851417542, Val Loss: 0.2577730417251587, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 9.208546892593195e-05
Epoch 410/1000, Train Loss: 0.8814886808395386, Val Loss: 0.2450050562620163, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 8.323539473539973e-05
Epoch 420/1000, Train Loss: 0.6700448095798492, Val Loss: 0.2367170751094818, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 7.438532054486752e-05
Epoch 430/1000, Train Loss: 0.9662914872169495, Val Loss: 0.2406802624464035, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 6.55352463543353e-05
Epoch 440/1000, Train Loss: 0.8878398835659027, Val Loss: 0.2424926459789276, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 5.668517216380307e-05
Epoch 450/1000, Train Loss: 0.9030905663967133, Val Loss: 0.2478560507297516, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.7835097973270825e-05
Epoch 460/1000, Train Loss: 0.8261817395687103, Val Loss: 0.2429271936416626, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 3.898502378273858e-05
Epoch 470/1000, Train Loss: 1.2525649070739746, Val Loss: 0.24030379951000214, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 3.0134949592206337e-05
Epoch 480/1000, Train Loss: 0.8182920217514038, Val Loss: 0.24381586909294128, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 2.1284875401674067e-05
Epoch 490/1000, Train Loss: 0.7941462099552155, Val Loss: 0.24484948813915253, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 1.2434801211141805e-05
Epoch 500/1000, Train Loss: 0.7278975546360016, Val Loss: 0.24439653754234314, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 510/1000, Train Loss: 0.8797605037689209, Val Loss: 0.24395236372947693, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 520/1000, Train Loss: 0.9729157388210297, Val Loss: 0.2442505806684494, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 530/1000, Train Loss: 0.6430404484272003, Val Loss: 0.24411003291606903, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 540/1000, Train Loss: 0.6032470613718033, Val Loss: 0.244130939245224, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 550/1000, Train Loss: 0.985151618719101, Val Loss: 0.24414443969726562, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 560/1000, Train Loss: 0.7017277032136917, Val Loss: 0.24398572742938995, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 570/1000, Train Loss: 0.7549686133861542, Val Loss: 0.24445216357707977, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 580/1000, Train Loss: 1.0622062683105469, Val Loss: 0.24464111030101776, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 590/1000, Train Loss: 0.7176199555397034, Val Loss: 0.24453601241111755, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 600/1000, Train Loss: 1.1641404032707214, Val Loss: 0.24445649981498718, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 610/1000, Train Loss: 0.5856814086437225, Val Loss: 0.24496512115001678, Train Acc: 0.925, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 620/1000, Train Loss: 0.678406223654747, Val Loss: 0.24380064010620117, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
[34m[1mwandb[39m[22m: [33mWARNING[39m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'Train Loss': 1.7919546961784363, 'Validation Loss': 0.5676552057266235, 'Train Accuracy': 0.65, 'Validation Accuracy': 1.0, '_timestamp': 1717437633.1762078}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.weight': {'_type': 'histogram', 'values': [3655, 6677, 6630, 6750, 6546, 6683, 6718, 6671, 6589, 6664, 6589, 6720, 6607, 6654, 6559, 6568, 6701, 6565, 6772, 6657, 6593, 6914, 7001, 7082, 6634, 6844, 6602, 6687, 6836, 6625, 6724, 6648, 6742, 6649, 6696, 6553, 6542, 6554, 6625, 6538, 6614, 6589, 6539, 6579, 6611, 6700, 6647, 6607, 6652, 6598, 6675, 6548, 6640, 6723, 6655, 6612, 6788, 6595, 6567, 6671, 6580, 6689, 6686, 3671], 'bins': [-0.09456995129585266, -0.09161446988582611, -0.08865898847579956, -0.08570350706577301, -0.08274801820516586, -0.07979253679513931, -0.07683705538511276, -0.07388157397508621, -0.07092609256505966, -0.06797061115503311, -0.06501512974500656, -0.06205964460968971, -0.05910416319966316, -0.056148678064346313, -0.05319319665431976, -0.05023771524429321, -0.04728223383426666, -0.044326748698949814, -0.041371267288923264, -0.03841578587889671, -0.035460300743579865, -0.032504819333553314, -0.029549337923526764, -0.026593854650855064, -0.023638371378183365, -0.020682889968156815, -0.017727406695485115, -0.014771925285458565, -0.011816442012786865, -0.00886095967143774, -0.005905477330088615, -0.0029499949887394905, 5.487352609634399e-06, 0.0029609696939587593, 0.005916452035307884, 0.008871934376657009, 0.011827416718006134, 0.014782899990677834, 0.017738381400704384, 0.020693864673376083, 0.023649346083402634, 0.026604829356074333, 0.029560312628746033, 0.03251579403877258, 0.03547127544879913, 0.03842676058411598, 0.04138224199414253, 0.04433772340416908, 0.04729320853948593, 0.05024868994951248, 0.05320417135953903, 0.05615965276956558, 0.05911513790488243, 0.06207061931490898, 0.06502610445022583, 0.06798158586025238, 0.07093706727027893, 0.07389254868030548, 0.07684803009033203, 0.07980351150035858, 0.08275899291038513, 0.08571448177099228, 0.08866996318101883, 0.09162544459104538, 0.09458092600107193]}, '_timestamp': 1717437633.182698}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.bias': {'_type': 'histogram', 'values': [6, 6, 11, 3, 6, 3, 10, 5, 3, 7, 5, 7, 8, 6, 12, 9, 10, 9, 5, 6, 9, 6, 8, 6, 8, 5, 5, 8, 7, 7, 9, 7, 8, 7, 6, 11, 5, 7, 10, 9, 6, 4, 9, 6, 17, 3, 13, 10, 6, 8, 7, 8, 8, 8, 6, 3, 5, 5, 7, 12, 7, 7, 10, 7], 'bins': [-0.032326631247997284, -0.03131498396396637, -0.030303338542580605, -0.02929169312119484, -0.028280045837163925, -0.02726840041577816, -0.026256754994392395, -0.02524510771036148, -0.024233462288975716, -0.0232218150049448, -0.022210169583559036, -0.02119852416217327, -0.020186876878142357, -0.019175231456756592, -0.018163584172725677, -0.017151938751339912, -0.016140293329954147, -0.015128646045923233, -0.014116999693214893, -0.013105354271829128, -0.012093707919120789, -0.011082061566412449, -0.01007041521370411, -0.00905876886099577, -0.00804712250828743, -0.007035476621240377, -0.006023830734193325, -0.005012184381484985, -0.004000538028776646, -0.0029888921417295933, -0.0019772457890212536, -0.0009655997273512185, 4.604645073413849e-05, 0.0010576925706118345, 0.0020693386904895306, 0.0030809850431978703, 0.004092630930244923, 0.005104277282953262, 0.006115923635661602, 0.007127569522708654, 0.008139215409755707, 0.009150861762464046, 0.010162508115172386, 0.011174154467880726, 0.012185800820589066, 0.013197447173297405, 0.01420909259468317, 0.01522073894739151, 0.016232386231422424, 0.01724403165280819, 0.018255677074193954, 0.01926732435822487, 0.020278969779610634, 0.021290617063641548, 0.022302262485027313, 0.02331390790641308, 0.024325555190443993, 0.025337200611829758, 0.026348847895860672, 0.027360493317246437, 0.028372138738632202, 0.029383786022663116, 0.03039543144404888, 0.03140707686543465, 0.03241872414946556]}, '_timestamp': 1717437633.18296}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.weight': {'_type': 'histogram', 'values': [302, 30, 24, 15, 7, 5, 2, 0, 3, 2, 3, 1, 2, 1, 0, 1, 0, 1, 1, 2, 1, 1, 3, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 7, 5, 3, 1, 1, 0, 3, 2, 0, 0, 0, 1, 0, 1, 0, 1, 0, 2, 2, 4, 1, 2, 2, 1, 3, 2, 9], 'bins': [0.9991058707237244, 0.999133825302124, 0.9991617798805237, 0.9991896748542786, 0.9992176294326782, 0.9992455840110779, 0.9992735385894775, 0.9993014335632324, 0.9993293881416321, 0.9993573427200317, 0.9993852972984314, 0.9994131922721863, 0.9994411468505859, 0.9994691014289856, 0.9994970560073853, 0.9995249509811401, 0.9995529055595398, 0.9995808601379395, 0.9996088147163391, 0.9996367692947388, 0.9996646642684937, 0.9996926188468933, 0.999720573425293, 0.9997485280036926, 0.9997764229774475, 0.9998043775558472, 0.9998323321342468, 0.9998602867126465, 0.9998881816864014, 0.999916136264801, 0.9999440908432007, 0.9999720454216003, 1.0, 1.0000278949737549, 1.0000559091567993, 1.0000838041305542, 1.000111699104309, 1.0001397132873535, 1.0001676082611084, 1.0001955032348633, 1.0002235174179077, 1.0002514123916626, 1.000279426574707, 1.000307321548462, 1.0003352165222168, 1.0003632307052612, 1.0003911256790161, 1.000419020652771, 1.0004470348358154, 1.0004749298095703, 1.0005029439926147, 1.0005308389663696, 1.0005587339401245, 1.000586748123169, 1.0006146430969238, 1.0006426572799683, 1.0006705522537231, 1.000698447227478, 1.0007264614105225, 1.0007543563842773, 1.0007822513580322, 1.0008102655410767, 1.0008381605148315, 1.000866174697876, 1.0008940696716309]}, '_timestamp': 1717437633.183158}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.bias': {'_type': 'histogram', 'values': [21, 15, 10, 7, 6, 19, 2, 4, 4, 4, 5, 2, 3, 4, 0, 0, 1, 2, 3, 0, 5, 2, 2, 3, 2, 5, 3, 1, 0, 0, 0, 0, 180, 0, 0, 0, 4, 5, 5, 3, 2, 4, 3, 12, 2, 1, 2, 0, 3, 6, 2, 2, 4, 4, 2, 4, 1, 2, 8, 0, 11, 14, 9, 37], 'bins': [-0.0008940836996771395, -0.0008661437314003706, -0.0008382038213312626, -0.0008102638530544937, -0.0007823238847777247, -0.0007543839165009558, -0.0007264439482241869, -0.0006985040381550789, -0.00067056406987831, -0.000642624101601541, -0.000614684191532433, -0.0005867442232556641, -0.0005588042549788952, -0.0005308642867021263, -0.0005029243184253573, -0.00047498440835624933, -0.0004470444400794804, -0.0004191044718027115, -0.000391164532629773, -0.00036322459345683455, -0.00033528462518006563, -0.0003073446569032967, -0.00027940471773035824, -0.0002514647785574198, -0.00022352481028065085, -0.00019558485655579716, -0.00016764490283094347, -0.00013970494910608977, -0.00011176499538123608, -8.382504165638238e-05, -5.588508793152869e-05, -2.7945134206674993e-05, -5.180481821298599e-09, 2.7934773243032396e-05, 5.587472696788609e-05, 8.381468069273978e-05, 0.00011175463441759348, 0.00013969458814244717, 0.00016763454186730087, 0.00019557449559215456, 0.00022351444931700826, 0.0002514544175937772, 0.00027939435676671565, 0.0003073342959396541, 0.00033527426421642303, 0.00036321423249319196, 0.0003911541716661304, 0.0004190941108390689, 0.0004470340791158378, 0.00047497404739260674, 0.0005029139574617147, 0.0005308539257384837, 0.0005587938940152526, 0.0005867338622920215, 0.0006146738305687904, 0.0006426137406378984, 0.0006705537089146674, 0.0006984936771914363, 0.0007264335872605443, 0.0007543735555373132, 0.0007823135238140821, 0.0008102534920908511, 0.00083819346036762, 0.000866133370436728, 0.0008940733387134969]}, '_timestamp': 1717437633.183328}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.weight': {'_type': 'histogram', 'values': [2100, 3448, 3407, 3462, 3397, 3393, 3505, 3473, 3391, 3398, 3405, 3433, 3467, 3508, 3431, 3464, 3543, 3522, 3483, 3430, 3400, 3432, 3340, 3478, 3433, 3406, 3497, 3515, 3416, 3399, 3451, 3751, 3880, 3423, 3616, 3394, 3327, 3400, 3426, 3486, 3376, 3447, 3431, 3380, 3381, 3452, 3472, 3452, 3394, 3411, 3546, 3428, 3473, 3489, 3393, 3498, 3436, 3424, 3399, 3462, 3458, 3415, 3370, 1972], 'bins': [-0.1141478642821312, -0.11057978123426437, -0.10701169818639755, -0.10344361513853073, -0.09987553209066391, -0.09630745649337769, -0.09273937344551086, -0.08917129039764404, -0.08560320734977722, -0.0820351243019104, -0.07846704125404358, -0.07489895820617676, -0.07133087515830994, -0.06776279211044312, -0.06419471651315689, -0.06062662973999977, -0.05705855041742325, -0.05349046736955643, -0.049922384321689606, -0.046354301273822784, -0.04278621822595596, -0.03921813890337944, -0.03565005585551262, -0.0320819728076458, -0.028513889759778976, -0.024945808574557304, -0.021377725526690483, -0.01780964434146881, -0.01424156129360199, -0.010673479177057743, -0.007105397060513496, -0.0035373149439692497, 3.076717257499695e-05, 0.0035988492891192436, 0.00716693140566349, 0.010735013522207737, 0.014303095638751984, 0.017871178686618805, 0.021439259871840477, 0.0250073429197073, 0.02857542410492897, 0.03214350715279579, 0.03571159020066261, 0.039279673248529434, 0.04284775257110596, 0.04641583561897278, 0.0499839186668396, 0.05355200171470642, 0.05712008476257324, 0.060688164085149765, 0.06425625085830688, 0.06782432645559311, 0.07139240950345993, 0.07496049255132675, 0.07852857559919357, 0.0820966586470604, 0.08566474169492722, 0.08923282474279404, 0.09280090779066086, 0.09636899083852768, 0.0999370664358139, 0.10350514948368073, 0.10707323253154755, 0.11064131557941437, 0.11420939862728119]}, '_timestamp': 1717437633.184719}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.bias': {'_type': 'histogram', 'values': [5, 6, 5, 2, 10, 8, 18, 9, 5, 7, 5, 11, 10, 0, 8, 8, 9, 7, 9, 7, 8, 8, 7, 10, 9, 13, 8, 6, 4, 9, 5, 10, 7, 9, 3, 2, 6, 9, 3, 4, 13, 6, 9, 5, 7, 11, 6, 6, 7, 3, 6, 10, 3, 6, 2, 8, 6, 5, 13, 10, 5, 10, 11, 10], 'bins': [-0.04680001735687256, -0.045341283082962036, -0.043882548809051514, -0.04242381453514099, -0.04096508026123047, -0.039506345987319946, -0.038047611713409424, -0.0365888737142086, -0.03513013944029808, -0.03367140516638756, -0.032212670892477036, -0.030753936618566513, -0.02929520234465599, -0.027836468070745468, -0.026377733796834946, -0.024918999522924423, -0.0234602652490139, -0.02200152911245823, -0.020542794838547707, -0.019084060564637184, -0.01762532629072666, -0.01616659201681614, -0.014707856811583042, -0.01324912253767252, -0.011790387332439423, -0.0103316530585289, -0.008872918784618378, -0.007414184045046568, -0.005955449305474758, -0.004496715031564236, -0.003037980291992426, -0.0015792457852512598, -0.00012051127851009369, 0.0013382232282310724, 0.0027969577349722385, 0.004255692474544048, 0.005714426748454571, 0.0071731614880263805, 0.00863189622759819, 0.010090630501508713, 0.011549364775419235, 0.013008099980652332, 0.014466834254562855, 0.015925569459795952, 0.017384303733706474, 0.018843038007616997, 0.02030177228152752, 0.02176050655543804, 0.023219242691993713, 0.024677976965904236, 0.02613671123981476, 0.02759544551372528, 0.029054179787635803, 0.030512914061546326, 0.03197164833545685, 0.03343038260936737, 0.03488911688327789, 0.036347851157188416, 0.037806589156389236, 0.03926532343029976, 0.04072405770421028, 0.042182791978120804, 0.043641526252031326, 0.04510026052594185, 0.04655899479985237]}, '_timestamp': 1717437633.184897}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.weight': {'_type': 'histogram', 'values': [215, 59, 41, 28, 3, 4, 9, 2, 0, 4, 2, 3, 2, 1, 2, 0, 2, 3, 3, 2, 0, 3, 4, 2, 3, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 4, 3, 2, 1, 1, 4, 0, 3, 2, 2, 2, 3, 0, 1, 2, 4, 1, 2, 0, 3, 0, 0, 1, 1, 5, 5, 7], 'bins': [0.9991058707237244, 0.9991337656974792, 0.9991617202758789, 0.9991896152496338, 0.9992175102233887, 0.9992454648017883, 0.9992733597755432, 0.9993012547492981, 0.9993292093276978, 0.9993571043014526, 0.9993849992752075, 0.9994129538536072, 0.9994408488273621, 0.9994687438011169, 0.9994966983795166, 0.9995245933532715, 0.9995524883270264, 0.999580442905426, 0.9996083378791809, 0.9996362328529358, 0.9996641874313354, 0.9996920824050903, 0.9997199773788452, 0.9997479319572449, 0.9997758269309998, 0.9998037219047546, 0.9998316764831543, 0.9998595714569092, 0.9998874664306641, 0.9999154210090637, 0.9999433159828186, 0.9999712109565735, 0.9999991655349731, 1.000027060508728, 1.000054955482483, 1.0000828504562378, 1.0001107454299927, 1.000138759613037, 1.000166654586792, 1.0001945495605469, 1.0002224445343018, 1.0002503395080566, 1.0002782344818115, 1.000306248664856, 1.0003341436386108, 1.0003620386123657, 1.0003899335861206, 1.0004178285598755, 1.0004457235336304, 1.0004737377166748, 1.0005016326904297, 1.0005295276641846, 1.0005574226379395, 1.0005853176116943, 1.0006132125854492, 1.0006412267684937, 1.0006691217422485, 1.0006970167160034, 1.0007249116897583, 1.0007528066635132, 1.000780701637268, 1.0008087158203125, 1.0008366107940674, 1.0008645057678223, 1.0008924007415771]}, '_timestamp': 1717437633.185057}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.bias': {'_type': 'histogram', 'values': [34, 15, 18, 21, 6, 26, 6, 7, 6, 2, 5, 1, 4, 9, 2, 2, 3, 5, 4, 9, 3, 1, 3, 1, 7, 4, 6, 6, 0, 0, 0, 6, 0, 0, 0, 1, 12, 13, 7, 8, 3, 4, 0, 4, 3, 5, 3, 2, 7, 3, 2, 4, 4, 2, 4, 5, 3, 5, 27, 8, 19, 19, 24, 44], 'bins': [-0.0008940185653045774, -0.0008660804014652967, -0.0008381422376260161, -0.0008102040737867355, -0.0007822659099474549, -0.0007543277461081743, -0.0007263895822688937, -0.0006984514184296131, -0.0006705132545903325, -0.0006425751489587128, -0.0006146369851194322, -0.0005866988212801516, -0.000558760657440871, -0.0005308224936015904, -0.0005028843297623098, -0.0004749461659230292, -0.0004470080020837486, -0.000419069838244468, -0.00039113167440518737, -0.00036319351056590676, -0.00033525534672662616, -0.000307317211991176, -0.0002793790481518954, -0.0002514408843126148, -0.0002235027204733342, -0.0001955645566340536, -0.00016762639279477298, -0.0001396882435074076, -0.000111750079668127, -8.38119158288464e-05, -5.5873759265523404e-05, -2.7935599064221606e-05, 2.561137080192566e-09, 2.794072133838199e-05, 5.587888153968379e-05, 8.381703810300678e-05, 0.00011175520194228739, 0.000139693365781568, 0.00016763151506893337, 0.00019556967890821397, 0.00022350784274749458, 0.0002514460065867752, 0.0002793841704260558, 0.0003073223342653364, 0.00033526046900078654, 0.00036319863284006715, 0.00039113679667934775, 0.00041907496051862836, 0.00044701312435790896, 0.00047495128819718957, 0.0005028894520364702, 0.0005308276158757508, 0.0005587657797150314, 0.000586703943554312, 0.0006146421073935926, 0.0006425802712328732, 0.0006705183768644929, 0.0006984565407037735, 0.0007263947045430541, 0.0007543328683823347, 0.0007822710322216153, 0.0008102091960608959, 0.0008381473599001765, 0.0008660855237394571, 0.0008940236875787377]}, '_timestamp': 1717437633.1852102}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.weight': {'_type': 'histogram', 'values': [2241, 3411, 3346, 3486, 3531, 3426, 3437, 3453, 3377, 3399, 3476, 3422, 3372, 3451, 3457, 3325, 3445, 3452, 3483, 3486, 3459, 3500, 3358, 3497, 3393, 3502, 3393, 3370, 3392, 3413, 3532, 3473, 3545, 3426, 3528, 3496, 3534, 3465, 3494, 3406, 3454, 3415, 3404, 3519, 3412, 3458, 3421, 3501, 3505, 3501, 3282, 3324, 3331, 3406, 3495, 3501, 3591, 3388, 3555, 3466, 3396, 3424, 3404, 2284], 'bins': [-0.11419699341058731, -0.11062809079885483, -0.10705918818712234, -0.10349027812480927, -0.09992137551307678, -0.0963524729013443, -0.09278357028961182, -0.08921466022729874, -0.08564575761556625, -0.08207685500383377, -0.07850795239210129, -0.07493904232978821, -0.07137013971805573, -0.06780123710632324, -0.06423233449459076, -0.06066342815756798, -0.0570945218205452, -0.053525619208812714, -0.04995671659708023, -0.04638781026005745, -0.042818907648324966, -0.039250001311302185, -0.0356810986995697, -0.03211219236254692, -0.028543289750814438, -0.024974385276436806, -0.021405480802059174, -0.01783657632768154, -0.01426767185330391, -0.010698767378926277, -0.007129862904548645, -0.003560958430171013, 7.946044206619263e-06, 0.0035768505185842514, 0.0071457549929618835, 0.010714659467339516, 0.014283563941717148, 0.01785246841609478, 0.021421372890472412, 0.024990277364850044, 0.028559181839227676, 0.03212808445096016, 0.03569699078798294, 0.039265893399715424, 0.042834799736738205, 0.04640370234847069, 0.04997260868549347, 0.05354151129722595, 0.057110413908958435, 0.060679320245981216, 0.064248226583004, 0.06781712919473648, 0.07138603180646896, 0.07495493441820145, 0.07852384448051453, 0.08209274709224701, 0.08566164970397949, 0.08923055231571198, 0.09279946237802505, 0.09636836498975754, 0.09993726760149002, 0.1035061702132225, 0.10707508027553558, 0.11064398288726807, 0.11421288549900055]}, '_timestamp': 1717437633.186599}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.bias': {'_type': 'histogram', 'values': [6, 6, 8, 13, 3, 6, 2, 8, 6, 7, 8, 14, 5, 3, 8, 7, 9, 9, 4, 9, 15, 4, 8, 9, 6, 6, 6, 12, 8, 6, 5, 10, 4, 8, 10, 11, 7, 11, 12, 9, 10, 7, 5, 11, 7, 7, 6, 2, 4, 7, 4, 8, 7, 7, 7, 7, 8, 6, 9, 2, 3, 10, 5, 10], 'bins': [-0.046597760170698166, -0.045152075588703156, -0.043706391006708145, -0.042260706424713135, -0.040815018117427826, -0.039369333535432816, -0.037923648953437805, -0.036477964371442795, -0.035032279789447784, -0.033586595207452774, -0.032140906900167465, -0.030695224180817604, -0.029249537736177444, -0.027803853154182434, -0.026358168572187424, -0.024912482127547264, -0.023466797545552254, -0.022021112963557243, -0.020575426518917084, -0.019129741936922073, -0.017684057354927063, -0.016238370910286903, -0.014792686328291893, -0.013347001746296883, -0.011901316232979298, -0.010455630719661713, -0.009009946137666702, -0.007564260624349117, -0.00611857557669282, -0.004672890529036522, -0.0032272052485495806, -0.001781520084477961, -0.00033583492040634155, 0.001109850243665278, 0.0025555354077368975, 0.004001220688223839, 0.0054469057358801365, 0.006892590783536434, 0.00833827629685402, 0.00978396087884903, 0.011229646392166615, 0.0126753319054842, 0.01412101648747921, 0.015566702000796795, 0.01701238751411438, 0.01845807209610939, 0.0199037566781044, 0.02134944312274456, 0.02279512770473957, 0.02424081228673458, 0.02568649873137474, 0.02713218331336975, 0.02857786789536476, 0.03002355434000492, 0.03146923705935478, 0.03291492536664009, 0.0343606099486351, 0.03580629453063011, 0.03725197911262512, 0.03869766369462013, 0.04014334827661514, 0.04158903658390045, 0.04303472116589546, 0.04448040574789047, 0.04592609032988548]}, '_timestamp': 1717437633.1867578}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.weight': {'_type': 'histogram', 'values': [245, 57, 35, 16, 12, 10, 5, 3, 3, 6, 4, 2, 1, 0, 0, 1, 1, 1, 2, 0, 2, 0, 0, 1, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 7, 7, 3, 3, 3, 3, 2, 0, 1, 1, 0, 1, 1, 2, 0, 1, 1, 1, 0, 0, 1, 0, 2, 2, 1, 4, 5], 'bins': [0.9991058707237244, 0.9991337656974792, 0.9991617202758789, 0.9991896152496338, 0.9992175698280334, 0.9992454648017883, 0.999273419380188, 0.9993013143539429, 0.9993292689323425, 0.9993571639060974, 0.9993851184844971, 0.999413013458252, 0.9994409680366516, 0.9994688630104065, 0.9994968175888062, 0.999524712562561, 0.9995526671409607, 0.9995805621147156, 0.9996085166931152, 0.9996364116668701, 0.9996643662452698, 0.9996922612190247, 0.9997202157974243, 0.9997481107711792, 0.9997760653495789, 0.9998039603233337, 0.9998319149017334, 0.9998598098754883, 0.9998877644538879, 0.9999156594276428, 0.9999436140060425, 0.9999715089797974, 0.9999994039535522, 1.0000272989273071, 1.0000553131103516, 1.0000832080841064, 1.0001111030578613, 1.0001389980316162, 1.0001670122146606, 1.0001949071884155, 1.0002228021621704, 1.0002506971359253, 1.0002787113189697, 1.0003066062927246, 1.0003345012664795, 1.0003623962402344, 1.0003904104232788, 1.0004183053970337, 1.0004462003707886, 1.0004740953445435, 1.000502109527588, 1.0005300045013428, 1.0005578994750977, 1.0005857944488525, 1.000613808631897, 1.0006417036056519, 1.0006695985794067, 1.0006974935531616, 1.000725507736206, 1.000753402709961, 1.0007812976837158, 1.0008091926574707, 1.0008372068405151, 1.00086510181427, 1.000892996788025]}, '_timestamp': 1717437633.186913}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.bias': {'_type': 'histogram', 'values': [51, 21, 14, 14, 10, 33, 5, 5, 5, 2, 1, 1, 6, 5, 6, 4, 1, 8, 1, 3, 7, 6, 5, 2, 7, 4, 5, 10, 0, 0, 0, 0, 2, 0, 0, 1, 3, 5, 6, 7, 2, 8, 6, 3, 6, 1, 9, 7, 5, 3, 6, 5, 3, 3, 3, 3, 6, 3, 33, 2, 9, 19, 12, 44], 'bins': [-0.0008940956322476268, -0.0008661560132168233, -0.0008382163941860199, -0.0008102767169475555, -0.0007823370979167521, -0.0007543974788859487, -0.0007264578016474843, -0.0006985181826166809, -0.0006705785635858774, -0.000642638944555074, -0.0006146993255242705, -0.0005867596482858062, -0.0005588200292550027, -0.0005308804102241993, -0.0005029407329857349, -0.0004750011139549315, -0.00044706149492412806, -0.0004191218758933246, -0.0003911822277586907, -0.0003632425796240568, -0.0003353029605932534, -0.00030736334156244993, -0.00027942369342781603, -0.00025148404529318213, -0.0002235444262623787, -0.00019560479267966002, -0.00016766515909694135, -0.00013972552551422268, -0.00011178589193150401, -8.384625834878534e-05, -5.590662476606667e-05, -2.7966991183348e-05, -2.735760062932968e-08, 2.791227598208934e-05, 5.585190956480801e-05, 8.379154314752668e-05, 0.00011173117673024535, 0.00013967081031296402, 0.0001676104438956827, 0.00019555007747840136, 0.00022348971106112003, 0.0002514293300919235, 0.0002793689782265574, 0.00030730862636119127, 0.0003352482453919947, 0.00036318786442279816, 0.00039112751255743206, 0.00041906716069206595, 0.0004470067797228694, 0.00047494639875367284, 0.0005028860177844763, 0.0005308256950229406, 0.0005587653140537441, 0.0005867049330845475, 0.0006146446103230119, 0.0006425842293538153, 0.0006705238483846188, 0.0006984634674154222, 0.0007264030864462256, 0.00075434276368469, 0.0007822823827154934, 0.0008102220017462969, 0.0008381616789847612, 0.0008661012980155647, 0.0008940409170463681]}, '_timestamp': 1717437633.1870599}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.weight': {'_type': 'histogram', 'values': [10, 6, 7, 7, 5, 4, 9, 8, 5, 8, 3, 9, 10, 4, 5, 8, 6, 14, 7, 6, 10, 6, 4, 2, 6, 12, 15, 8, 5, 13, 8, 9, 9, 6, 7, 8, 8, 14, 7, 6, 8, 7, 10, 1, 9, 3, 7, 11, 12, 1, 6, 6, 10, 6, 7, 5, 8, 8, 5, 7, 6, 6, 9, 5], 'bins': [-0.16006508469581604, -0.15505991876125336, -0.15005475282669067, -0.145049586892128, -0.1400444209575653, -0.13503925502300262, -0.13003408908843994, -0.12502890825271606, -0.12002374976873398, -0.1150185838341713, -0.11001341044902802, -0.10500824451446533, -0.10000307857990265, -0.09499791264533997, -0.08999274671077728, -0.0849875807762146, -0.07998241484165192, -0.07497724145650864, -0.06997207552194595, -0.06496690958738327, -0.05996174365282059, -0.054956573992967606, -0.04995140805840492, -0.04494624212384224, -0.03994107246398926, -0.034935906529426575, -0.02993074059486389, -0.02492557279765606, -0.019920405000448227, -0.014915239065885544, -0.009910071268677711, -0.004904904402792454, 0.00010026246309280396, 0.005105429328978062, 0.01011059619486332, 0.015115763992071152, 0.020120929926633835, 0.025126097723841667, 0.0301312655210495, 0.03513643145561218, 0.040141597390174866, 0.04514676705002785, 0.05015193298459053, 0.055157098919153214, 0.060162268579006195, 0.06516743451356888, 0.07017260044813156, 0.07517776638269424, 0.08018293976783752, 0.08518810570240021, 0.09019327163696289, 0.09519843757152557, 0.10020360350608826, 0.10520876944065094, 0.11021393537521362, 0.1152191087603569, 0.12022427469491959, 0.12522943317890167, 0.13023461401462555, 0.13523977994918823, 0.14024494588375092, 0.1452501118183136, 0.15025527775287628, 0.15526044368743896, 0.16026560962200165]}, '_timestamp': 1717437633.187213}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.bias': {'_type': 'histogram', 'values': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'bins': [-0.4952250123023987, -0.4796000123023987, -0.4639750123023987, -0.4483500123023987, -0.4327250123023987, -0.4171000123023987, -0.4014750123023987, -0.3858500123023987, -0.3702250123023987, -0.3546000123023987, -0.3389750123023987, -0.3233500123023987, -0.3077250123023987, -0.2921000123023987, -0.2764750123023987, -0.2608500123023987, -0.24522502720355988, -0.22960002720355988, -0.21397502720355988, -0.19835002720355988, -0.18272502720355988, -0.16710002720355988, -0.15147502720355988, -0.13585002720355988, -0.12022502720355988, -0.10460002720355988, -0.08897502720355988, -0.07335002720355988, -0.057725027203559875, -0.042100027203559875, -0.026475025340914726, -0.010850025340914726, 0.0047749741934239864, 0.020399974659085274, 0.036024972796440125, 0.051649972796440125, 0.06727497279644012, 0.08289997279644012, 0.09852497279644012, 0.11414997279644012, 0.12977497279644012, 0.14539997279644012, 0.16102497279644012, 0.17664997279644012, 0.19227497279644012, 0.20789997279644012, 0.22352497279644012, 0.23914997279644012, 0.2547749876976013, 0.2703999876976013, 0.2860249876976013, 0.3016499876976013, 0.3172749876976013, 0.3328999876976013, 0.3485249876976013, 0.3641499876976013, 0.3797749876976013, 0.3953999876976013, 0.4110249876976013, 0.4266499876976013, 0.4422749876976013, 0.4578999876976013, 0.4735249876976013, 0.4891499876976013, 0.5047749876976013]}, '_timestamp': 1717437633.187383}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TP Reward Distribution': {'_type': 'histogram', 'values': [2, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [2.6666666666666665, 3.0614583333333334, 3.45625, 3.8510416666666663, 4.245833333333334, 4.640625, 5.035416666666666, 5.430208333333333, 5.824999999999999, 6.219791666666666, 6.614583333333333, 7.009375, 7.404166666666667, 7.798958333333333, 8.19375, 8.588541666666666, 8.983333333333333, 9.378124999999999, 9.772916666666665, 10.167708333333334, 10.5625, 10.957291666666666, 11.352083333333333, 11.746875, 12.141666666666666, 12.536458333333332, 12.931249999999999, 13.326041666666665, 13.720833333333333, 14.115625, 14.510416666666666, 14.905208333333333, 15.299999999999999, 15.694791666666665, 16.089583333333334, 16.484375, 16.879166666666666, 17.273958333333333, 17.66875, 18.063541666666666, 18.458333333333332, 18.853125000000002, 19.24791666666667, 19.642708333333335, 20.0375, 20.432291666666668, 20.827083333333334, 21.221875, 21.616666666666667, 22.011458333333334, 22.40625, 22.801041666666666, 23.195833333333333, 23.590625, 23.985416666666666, 24.380208333333332, 24.775000000000002, 25.16979166666667, 25.564583333333335, 25.959375, 26.354166666666668, 26.748958333333334, 27.14375, 27.538541666666667, 27.933333333333334]}, '_timestamp': 1717437633.1878169}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TN Reward Distribution': {'_type': 'histogram', 'values': [6, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [3.6, 6.366666666666667, 9.133333333333333, 11.9, 14.666666666666666, 17.433333333333334, 20.200000000000003, 22.96666666666667, 25.733333333333334, 28.5, 31.266666666666666, 34.03333333333333, 36.800000000000004, 39.56666666666667, 42.333333333333336, 45.1, 47.86666666666667, 50.63333333333333, 53.4, 56.166666666666664, 58.93333333333333, 61.7, 64.46666666666667, 67.23333333333333, 70.0, 72.76666666666667, 75.53333333333333, 78.3, 81.06666666666666, 83.83333333333333, 86.6, 89.36666666666666, 92.13333333333333, 94.89999999999999, 97.66666666666666, 100.43333333333332, 103.19999999999999, 105.96666666666665, 108.73333333333332, 111.49999999999999, 114.26666666666665, 117.03333333333333, 119.8, 122.56666666666666, 125.33333333333333, 128.1, 130.86666666666667, 133.63333333333333, 136.4, 139.16666666666666, 141.93333333333334, 144.7, 147.46666666666667, 150.23333333333332, 153.0, 155.76666666666665, 158.53333333333333, 161.29999999999998, 164.06666666666666, 166.83333333333331, 169.6, 172.36666666666665, 175.13333333333333, 177.89999999999998, 180.66666666666666]}, '_timestamp': 1717437633.1880262}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FP Reward Distribution': {'_type': 'histogram', 'values': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1], 'bins': [3.533333333333333, 3.8249999999999997, 4.116666666666666, 4.408333333333333, 4.699999999999999, 4.991666666666666, 5.283333333333333, 5.574999999999999, 5.866666666666666, 6.158333333333333, 6.449999999999999, 6.741666666666666, 7.033333333333333, 7.324999999999999, 7.616666666666666, 7.908333333333332, 8.2, 8.491666666666667, 8.783333333333331, 9.075, 9.366666666666665, 9.658333333333331, 9.95, 10.241666666666665, 10.533333333333331, 10.825, 11.116666666666665, 11.408333333333331, 11.7, 11.991666666666665, 12.283333333333331, 12.575, 12.866666666666665, 13.158333333333331, 13.45, 13.741666666666665, 14.033333333333331, 14.325, 14.616666666666665, 14.908333333333331, 15.199999999999998, 15.491666666666665, 15.783333333333331, 16.074999999999996, 16.366666666666667, 16.65833333333333, 16.949999999999996, 17.241666666666667, 17.53333333333333, 17.824999999999996, 18.116666666666667, 18.40833333333333, 18.699999999999996, 18.991666666666667, 19.28333333333333, 19.574999999999996, 19.866666666666667, 20.15833333333333, 20.449999999999996, 20.741666666666667, 21.03333333333333, 21.324999999999996, 21.616666666666667, 21.90833333333333, 22.2]}, '_timestamp': 1717437633.188191}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FN Reward Distribution': {'_type': 'histogram', 'values': [8, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [2.6666666666666665, 5.503125, 8.339583333333334, 11.176041666666666, 14.0125, 16.848958333333332, 19.68541666666667, 22.521875, 25.358333333333334, 28.194791666666667, 31.03125, 33.86770833333333, 36.704166666666666, 39.540625, 42.37708333333333, 45.213541666666664, 48.05, 50.88645833333333, 53.72291666666666, 56.559374999999996, 59.39583333333333, 62.23229166666666, 65.06875, 67.90520833333333, 70.74166666666667, 73.578125, 76.41458333333334, 79.25104166666667, 82.0875, 84.92395833333333, 87.76041666666667, 90.596875, 93.43333333333334, 96.26979166666668, 99.10625, 101.94270833333334, 104.77916666666667, 107.61562500000001, 110.45208333333333, 113.28854166666667, 116.125, 118.96145833333334, 121.79791666666667, 124.634375, 127.47083333333333, 130.30729166666666, 133.14374999999998, 135.9802083333333, 138.81666666666666, 141.653125, 144.48958333333331, 147.32604166666667, 150.1625, 152.99895833333332, 155.83541666666665, 158.671875, 161.50833333333333, 164.34479166666665, 167.18124999999998, 170.01770833333333, 172.85416666666666, 175.69062499999998, 178.5270833333333, 181.36354166666666, 184.2]}, '_timestamp': 1717437633.188342}).
Epoch 630/1000, Train Loss: 0.7194719016551971, Val Loss: 0.2432202845811844, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 640/1000, Train Loss: 0.8387756049633026, Val Loss: 0.24207668006420135, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 650/1000, Train Loss: 0.9808832406997681, Val Loss: 0.24149160087108612, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 660/1000, Train Loss: 0.7607153058052063, Val Loss: 0.24218392372131348, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 670/1000, Train Loss: 0.7660033702850342, Val Loss: 0.2419985979795456, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 680/1000, Train Loss: 0.7809149026870728, Val Loss: 0.24114194512367249, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 690/1000, Train Loss: 0.723956972360611, Val Loss: 0.24119313061237335, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 700/1000, Train Loss: 0.7609938681125641, Val Loss: 0.2413497269153595, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 710/1000, Train Loss: 0.8366177678108215, Val Loss: 0.2408505380153656, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 720/1000, Train Loss: 0.7645374238491058, Val Loss: 0.24035879969596863, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 730/1000, Train Loss: 0.7047257721424103, Val Loss: 0.2414625883102417, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 740/1000, Train Loss: 0.7033318877220154, Val Loss: 0.2415076047182083, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 750/1000, Train Loss: 0.621275007724762, Val Loss: 0.24196751415729523, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 760/1000, Train Loss: 0.964997410774231, Val Loss: 0.24142947793006897, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 770/1000, Train Loss: 0.9813240170478821, Val Loss: 0.24197249114513397, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 780/1000, Train Loss: 0.7554323673248291, Val Loss: 0.24187517166137695, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 790/1000, Train Loss: 0.9744952321052551, Val Loss: 0.24125900864601135, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 800/1000, Train Loss: 0.6470106244087219, Val Loss: 0.24136629700660706, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 810/1000, Train Loss: 0.5784063041210175, Val Loss: 0.24182815849781036, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 820/1000, Train Loss: 0.7582289278507233, Val Loss: 0.2415911853313446, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 830/1000, Train Loss: 0.8635872304439545, Val Loss: 0.24189570546150208, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 840/1000, Train Loss: 0.6995356678962708, Val Loss: 0.24171121418476105, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 850/1000, Train Loss: 1.1807078421115875, Val Loss: 0.2421329766511917, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 860/1000, Train Loss: 0.660165011882782, Val Loss: 0.2414424866437912, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 870/1000, Train Loss: 0.6770838499069214, Val Loss: 0.24177832901477814, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 880/1000, Train Loss: 0.9399802684783936, Val Loss: 0.24128618836402893, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 890/1000, Train Loss: 1.2260901033878326, Val Loss: 0.24096927046775818, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 900/1000, Train Loss: 0.6721183806657791, Val Loss: 0.24113552272319794, Train Acc: 0.9, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 910/1000, Train Loss: 0.7739015519618988, Val Loss: 0.24097266793251038, Train Acc: 0.9, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 920/1000, Train Loss: 1.046708732843399, Val Loss: 0.2405143529176712, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 930/1000, Train Loss: 0.7153299301862717, Val Loss: 0.24028344452381134, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 940/1000, Train Loss: 0.6573530435562134, Val Loss: 0.23977228999137878, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 950/1000, Train Loss: 0.6453040242195129, Val Loss: 0.23999008536338806, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 960/1000, Train Loss: 0.9467885196208954, Val Loss: 0.23980660736560822, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 970/1000, Train Loss: 0.8483608663082123, Val Loss: 0.24058178067207336, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 980/1000, Train Loss: 0.5792331248521805, Val Loss: 0.24088601768016815, Train Acc: 0.9, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
Epoch 990/1000, Train Loss: 0.7509804666042328, Val Loss: 0.24085161089897156, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 4.469734439662765e-06
[32m[I 2024-06-03 14:00:58,526][39m Trial 0 finished with value: 0.14591246843338013 and parameters: {'hidden_size': 467, 'learning_rate': 0.00044697344396627475, 'weight_decay': 0.000993100861449791}. Best is trial 0 with value: 0.14591246843338013.