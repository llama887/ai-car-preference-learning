Epoch 0/1000, Train Loss: 2.0487685203552246, Val Loss: 0.650377094745636, Train Acc: 0.45, Val Acc: 0.800000011920929, LR: 0.00027757031700212224
Epoch 10/1000, Train Loss: 1.6865479946136475, Val Loss: 0.546042799949646, Train Acc: 0.475, Val Acc: 0.8999999761581421, LR: 0.0002693019214523765
Epoch 20/1000, Train Loss: 1.4966941177845001, Val Loss: 0.5360832810401917, Train Acc: 0.525, Val Acc: 0.8999999761581421, LR: 0.0002610335259026305
Epoch 30/1000, Train Loss: 1.4703739285469055, Val Loss: 0.5178953409194946, Train Acc: 0.475, Val Acc: 0.8500000238418579, LR: 0.00025276513035288453
Epoch 40/1000, Train Loss: 1.2740862369537354, Val Loss: 0.5081332921981812, Train Acc: 0.6125, Val Acc: 0.800000011920929, LR: 0.00024449673480313845
Epoch 50/1000, Train Loss: 1.3971697986125946, Val Loss: 0.46989569067955017, Train Acc: 0.4625, Val Acc: 0.8999999761581421, LR: 0.0002362283392533926
Epoch 60/1000, Train Loss: 1.3759879171848297, Val Loss: 0.44641178846359253, Train Acc: 0.5875, Val Acc: 0.8500000238418579, LR: 0.00022795994370364685
Epoch 70/1000, Train Loss: 1.1841272115707397, Val Loss: 0.408174604177475, Train Acc: 0.5875, Val Acc: 0.8500000238418579, LR: 0.00021969154815390114
Epoch 80/1000, Train Loss: 0.9298791885375977, Val Loss: 0.3863604664802551, Train Acc: 0.65, Val Acc: 0.8999999761581421, LR: 0.0002114231526041554
Epoch 90/1000, Train Loss: 0.9368714690208435, Val Loss: 0.35237619280815125, Train Acc: 0.7125, Val Acc: 0.8999999761581421, LR: 0.00020315475705440973
Epoch 100/1000, Train Loss: 1.084791898727417, Val Loss: 0.32892686128616333, Train Acc: 0.625, Val Acc: 0.8999999761581421, LR: 0.00019488636150466403
Epoch 110/1000, Train Loss: 0.9509551674127579, Val Loss: 0.3077479600906372, Train Acc: 0.7, Val Acc: 0.8999999761581421, LR: 0.0001866179659549183
Epoch 120/1000, Train Loss: 1.1618869304656982, Val Loss: 0.3292502164840698, Train Acc: 0.65, Val Acc: 0.8999999761581421, LR: 0.0001783495704051726
Epoch 130/1000, Train Loss: 0.8246046006679535, Val Loss: 0.2961867153644562, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 0.0001700811748554269
Epoch 140/1000, Train Loss: 0.860807478427887, Val Loss: 0.2829110026359558, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 0.00016181277930568116
Epoch 150/1000, Train Loss: 0.7909061014652252, Val Loss: 0.27569055557250977, Train Acc: 0.7375, Val Acc: 0.8999999761581421, LR: 0.0001535443837559356
Epoch 160/1000, Train Loss: 1.0060962736606598, Val Loss: 0.2599884271621704, Train Acc: 0.6875, Val Acc: 0.8999999761581421, LR: 0.00014527598820618986
Epoch 170/1000, Train Loss: 0.671151414513588, Val Loss: 0.25757020711898804, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.00013700759265644415
Epoch 180/1000, Train Loss: 0.7658155262470245, Val Loss: 0.24734941124916077, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 0.00012873919710669856
Epoch 190/1000, Train Loss: 0.836425393819809, Val Loss: 0.24140210449695587, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 0.00012047080155695287
Epoch 200/1000, Train Loss: 0.8739510476589203, Val Loss: 0.24877884984016418, Train Acc: 0.7125, Val Acc: 0.8999999761581421, LR: 0.00011220240600720707
Epoch 210/1000, Train Loss: 0.8050983250141144, Val Loss: 0.2381458729505539, Train Acc: 0.7375, Val Acc: 0.8999999761581421, LR: 0.00010393401045746126
Epoch 220/1000, Train Loss: 0.8505026698112488, Val Loss: 0.24266104400157928, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 9.566561490771543e-05
Epoch 230/1000, Train Loss: 0.6707149147987366, Val Loss: 0.24289819598197937, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 8.739721935796963e-05
Epoch 240/1000, Train Loss: 0.7698786705732346, Val Loss: 0.24232736229896545, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 7.912882380822376e-05
Epoch 250/1000, Train Loss: 0.7770284116268158, Val Loss: 0.24287685751914978, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 7.0860428258478e-05
Epoch 260/1000, Train Loss: 0.7930866628885269, Val Loss: 0.23992875218391418, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 6.259203270873215e-05
Epoch 270/1000, Train Loss: 0.8063155859708786, Val Loss: 0.24375399947166443, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 5.432363715898635e-05
Epoch 280/1000, Train Loss: 0.6877778917551041, Val Loss: 0.24093130230903625, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 4.6055241609240595e-05
Epoch 290/1000, Train Loss: 0.5730856508016586, Val Loss: 0.24274244904518127, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 3.778684605949484e-05
Epoch 300/1000, Train Loss: 0.7320612668991089, Val Loss: 0.24096429347991943, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 2.951845050974907e-05
Epoch 310/1000, Train Loss: 0.5152458250522614, Val Loss: 0.24651546776294708, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 2.1250054960003276e-05
Epoch 320/1000, Train Loss: 0.8711895644664764, Val Loss: 0.25127241015434265, Train Acc: 0.7125, Val Acc: 0.8999999761581421, LR: 1.2981659410257465e-05
Epoch 330/1000, Train Loss: 0.6234357208013535, Val Loss: 0.25385838747024536, Train Acc: 0.7875, Val Acc: 0.8999999761581421, LR: 4.7132638605116655e-06
Epoch 340/1000, Train Loss: 0.8422215580940247, Val Loss: 0.25481271743774414, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 350/1000, Train Loss: 0.711641252040863, Val Loss: 0.2540361285209656, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 360/1000, Train Loss: 0.8200083076953888, Val Loss: 0.2545531988143921, Train Acc: 0.7125, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 370/1000, Train Loss: 0.8107078671455383, Val Loss: 0.25425422191619873, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 380/1000, Train Loss: 0.5517754554748535, Val Loss: 0.25381335616111755, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 390/1000, Train Loss: 0.580694779753685, Val Loss: 0.2546335458755493, Train Acc: 0.7875, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 400/1000, Train Loss: 0.5942691415548325, Val Loss: 0.2537088096141815, Train Acc: 0.8375, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 410/1000, Train Loss: 0.6285880506038666, Val Loss: 0.2528015971183777, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 420/1000, Train Loss: 0.6997779756784439, Val Loss: 0.2536790668964386, Train Acc: 0.7625, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 430/1000, Train Loss: 0.7595230042934418, Val Loss: 0.25455230474472046, Train Acc: 0.8125, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 440/1000, Train Loss: 0.6235561817884445, Val Loss: 0.25433677434921265, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 450/1000, Train Loss: 0.6349013894796371, Val Loss: 0.25371354818344116, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 460/1000, Train Loss: 0.6190461367368698, Val Loss: 0.25268271565437317, Train Acc: 0.7625, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 470/1000, Train Loss: 0.7504838705062866, Val Loss: 0.25177696347236633, Train Acc: 0.7625, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 480/1000, Train Loss: 0.6804502159357071, Val Loss: 0.2510395348072052, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 490/1000, Train Loss: 0.7210215479135513, Val Loss: 0.24965982139110565, Train Acc: 0.7375, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 500/1000, Train Loss: 0.588924914598465, Val Loss: 0.2514851689338684, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 510/1000, Train Loss: 0.7460731118917465, Val Loss: 0.25295260548591614, Train Acc: 0.7625, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 520/1000, Train Loss: 0.5973267406225204, Val Loss: 0.2539696991443634, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 530/1000, Train Loss: 0.7764534056186676, Val Loss: 0.2542327046394348, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 540/1000, Train Loss: 0.6173193007707596, Val Loss: 0.25289350748062134, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 550/1000, Train Loss: 0.7202762216329575, Val Loss: 0.2521754801273346, Train Acc: 0.725, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 560/1000, Train Loss: 0.547766774892807, Val Loss: 0.2521182894706726, Train Acc: 0.875, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 570/1000, Train Loss: 0.6094082891941071, Val Loss: 0.25194859504699707, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 580/1000, Train Loss: 0.7374827265739441, Val Loss: 0.2520877718925476, Train Acc: 0.7375, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 590/1000, Train Loss: 0.632538765668869, Val Loss: 0.25210142135620117, Train Acc: 0.8125, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 600/1000, Train Loss: 0.7378705888986588, Val Loss: 0.25216466188430786, Train Acc: 0.7, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 610/1000, Train Loss: 0.75944022834301, Val Loss: 0.2520800530910492, Train Acc: 0.7375, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 620/1000, Train Loss: 0.6439332813024521, Val Loss: 0.2525899112224579, Train Acc: 0.7625, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 630/1000, Train Loss: 0.4785931780934334, Val Loss: 0.25273799896240234, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 640/1000, Train Loss: 0.5311552286148071, Val Loss: 0.2520633637905121, Train Acc: 0.8125, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 650/1000, Train Loss: 0.6790393143892288, Val Loss: 0.25230836868286133, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 660/1000, Train Loss: 0.6864172667264938, Val Loss: 0.2531948983669281, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 670/1000, Train Loss: 0.572504311800003, Val Loss: 0.2534900903701782, Train Acc: 0.8375, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 680/1000, Train Loss: 0.6654451489448547, Val Loss: 0.2534501552581787, Train Acc: 0.8375, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 690/1000, Train Loss: 0.5779559761285782, Val Loss: 0.25292325019836426, Train Acc: 0.8125, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 700/1000, Train Loss: 0.660293698310852, Val Loss: 0.2527998089790344, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 710/1000, Train Loss: 0.6651953011751175, Val Loss: 0.25258737802505493, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 720/1000, Train Loss: 0.5865458548069, Val Loss: 0.25206810235977173, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 730/1000, Train Loss: 0.6832243651151657, Val Loss: 0.25197088718414307, Train Acc: 0.7625, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 740/1000, Train Loss: 0.6695751845836639, Val Loss: 0.25156405568122864, Train Acc: 0.7875, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 750/1000, Train Loss: 0.7466335743665695, Val Loss: 0.2518613934516907, Train Acc: 0.7625, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 760/1000, Train Loss: 0.8269741088151932, Val Loss: 0.25158002972602844, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 770/1000, Train Loss: 0.6539573967456818, Val Loss: 0.25302720069885254, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 780/1000, Train Loss: 0.6258103400468826, Val Loss: 0.25310587882995605, Train Acc: 0.8375, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 790/1000, Train Loss: 0.7187884449958801, Val Loss: 0.253028005361557, Train Acc: 0.75, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 800/1000, Train Loss: 0.7388230413198471, Val Loss: 0.2522449493408203, Train Acc: 0.7375, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 810/1000, Train Loss: 0.6927420198917389, Val Loss: 0.25192636251449585, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 820/1000, Train Loss: 0.5399248600006104, Val Loss: 0.25232672691345215, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 830/1000, Train Loss: 0.6343814879655838, Val Loss: 0.2520495057106018, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 840/1000, Train Loss: 0.5786509066820145, Val Loss: 0.2536722719669342, Train Acc: 0.8125, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 850/1000, Train Loss: 0.5920558273792267, Val Loss: 0.2523173689842224, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 860/1000, Train Loss: 0.6210299283266068, Val Loss: 0.2517901360988617, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
[34m[1mwandb[39m[22m: [33mWARNING[39m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'Train Loss': 2.0487685203552246, 'Validation Loss': 0.650377094745636, 'Train Accuracy': 0.45, 'Validation Accuracy': 0.800000011920929, '_timestamp': 1717516527.430977}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.weight': {'_type': 'histogram', 'values': [1615, 2030, 1908, 2077, 1921, 1908, 1915, 1832, 2010, 1871, 1965, 1882, 1947, 1935, 1920, 1929, 1941, 1933, 1965, 1967, 1886, 1861, 1865, 1930, 1991, 2002, 1907, 1907, 1938, 1906, 1928, 1947, 1942, 1931, 1985, 1935, 1862, 1933, 1926, 1920, 1930, 1935, 1889, 1902, 1946, 1907, 1865, 1947, 1861, 1912, 1886, 1948, 1851, 1887, 1930, 1951, 1916, 1967, 1941, 2029, 1933, 1921, 1899, 1274], 'bins': [-0.10845530033111572, -0.1050667092204094, -0.10167812556028366, -0.09828953444957733, -0.0949009507894516, -0.09151235967874527, -0.08812377601861954, -0.08473518490791321, -0.08134660124778748, -0.07795801013708115, -0.07456942647695541, -0.07118083536624908, -0.06779225170612335, -0.06440366059541702, -0.06101507321000099, -0.05762648582458496, -0.05423789843916893, -0.0508493110537529, -0.04746072366833687, -0.04407213628292084, -0.040683548897504807, -0.037294961512088776, -0.033906374126672745, -0.030517784878611565, -0.027129197493195534, -0.023740610107779503, -0.020352022722363472, -0.01696343533694744, -0.013574847020208836, -0.010186259634792805, -0.0067976717837154865, -0.003409084165468812, -2.049654722213745e-05, 0.003368091071024537, 0.006756678689271212, 0.01014526654034853, 0.01353385392576456, 0.016922442242503166, 0.020311029627919197, 0.023699617013335228, 0.02708820439875126, 0.03047679178416729, 0.03386538103222847, 0.0372539684176445, 0.04064255580306053, 0.04403114318847656, 0.04741973057389259, 0.050808317959308624, 0.054196905344724655, 0.057585492730140686, 0.06097408011555672, 0.06436266750097275, 0.06775125861167908, 0.07113984227180481, 0.07452843338251114, 0.07791701704263687, 0.0813056081533432, 0.08469419181346893, 0.08808278292417526, 0.091471366584301, 0.09485995769500732, 0.09824854135513306, 0.10163713246583939, 0.10502571612596512, 0.10841430723667145]}, '_timestamp': 1717516527.4347131}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.bias': {'_type': 'histogram', 'values': [5, 0, 5, 3, 2, 1, 2, 3, 5, 1, 1, 1, 3, 2, 2, 3, 0, 4, 3, 3, 3, 0, 3, 0, 5, 2, 2, 1, 4, 0, 0, 3, 4, 1, 1, 1, 3, 5, 2, 1, 3, 4, 3, 2, 1, 2, 0, 1, 3, 2, 4, 1, 2, 2, 0, 3, 0, 1, 2, 0, 4, 5, 0, 1], 'bins': [-0.03250087425112724, -0.031491104513406754, -0.030481332913041115, -0.029471563175320625, -0.028461791574954987, -0.027452021837234497, -0.02644225023686886, -0.02543248049914837, -0.02442270889878273, -0.02341293916106224, -0.02240316942334175, -0.021393397822976112, -0.020383628085255623, -0.019373856484889984, -0.018364086747169495, -0.017354315146803856, -0.016344545409083366, -0.015334774740040302, -0.014325004070997238, -0.013315233401954174, -0.01230546273291111, -0.01129569299519062, -0.010285922326147556, -0.009276151657104492, -0.008266380988061428, -0.007256610319018364, -0.0062468396499753, -0.005237069446593523, -0.004227298777550459, -0.003217528108507395, -0.0022077576722949743, -0.001197987119667232, -0.00018821656703948975, 0.0008215539855882525, 0.0018313245382159948, 0.0028410949744284153, 0.0038508656434714794, 0.0048606363125145435, 0.00587040651589632, 0.0068801771849393845, 0.007889947853982449, 0.008899718523025513, 0.009909489192068577, 0.010919259861111641, 0.01192902959883213, 0.012938800267875195, 0.013948570936918259, 0.014958341605961323, 0.015968112275004387, 0.016977882012724876, 0.017987653613090515, 0.018997423350811005, 0.020007194951176643, 0.021016964688897133, 0.02202673628926277, 0.02303650602698326, 0.02404627576470375, 0.02505604736506939, 0.02606581710278988, 0.027075588703155518, 0.028085358440876007, 0.029095130041241646, 0.030104899778962135, 0.031114671379327774, 0.032124441117048264]}, '_timestamp': 1717516527.434983}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.weight': {'_type': 'histogram', 'values': [52, 6, 4, 1, 4, 3, 2, 0, 1, 0, 0, 1, 2, 0, 4, 1, 0, 1, 0, 0, 2, 0, 2, 0, 0, 2, 0, 3, 1, 4, 1, 0, 4, 1, 2, 1, 1, 0, 0, 2, 1, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 3, 1, 0, 1, 1, 1, 2, 0, 3, 3, 1, 1, 4], 'bins': [0.9991657137870789, 0.9991917014122009, 0.999217689037323, 0.9992436766624451, 0.9992696642875671, 0.9992956519126892, 0.9993216395378113, 0.9993475675582886, 0.9993735551834106, 0.9993995428085327, 0.9994255304336548, 0.9994515180587769, 0.9994775056838989, 0.999503493309021, 0.9995294809341431, 0.9995554685592651, 0.9995814561843872, 0.9996074438095093, 0.9996334314346313, 0.9996594190597534, 0.9996853470802307, 0.9997113347053528, 0.9997373223304749, 0.9997633099555969, 0.999789297580719, 0.9998152852058411, 0.9998412728309631, 0.9998672604560852, 0.9998932480812073, 0.9999192357063293, 0.9999452233314514, 0.9999712109565735, 0.9999971389770508, 1.0000231266021729, 1.000049114227295, 1.000075101852417, 1.000101089477539, 1.0001270771026611, 1.0001530647277832, 1.0001790523529053, 1.0002050399780273, 1.0002310276031494, 1.0002570152282715, 1.0002830028533936, 1.0003089904785156, 1.0003349781036377, 1.0003609657287598, 1.0003869533538818, 1.000412940979004, 1.000438928604126, 1.000464916229248, 1.0004909038543701, 1.0005167722702026, 1.0005427598953247, 1.0005687475204468, 1.0005947351455688, 1.000620722770691, 1.000646710395813, 1.000672698020935, 1.0006986856460571, 1.0007246732711792, 1.0007506608963013, 1.0007766485214233, 1.0008026361465454, 1.0008286237716675]}, '_timestamp': 1717516527.4351711}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.bias': {'_type': 'histogram', 'values': [5, 0, 2, 3, 1, 1, 1, 2, 4, 1, 0, 0, 2, 2, 0, 1, 1, 6, 2, 0, 0, 0, 1, 1, 1, 0, 1, 2, 2, 1, 1, 2, 43, 1, 1, 2, 1, 1, 1, 2, 0, 1, 1, 1, 2, 0, 4, 1, 0, 2, 1, 0, 2, 2, 1, 4, 1, 0, 2, 2, 3, 1, 4, 3], 'bins': [-0.0008344374364241958, -0.0008084385190159082, -0.0007824396016076207, -0.0007564406841993332, -0.0007304417667910457, -0.0007044428493827581, -0.0006784439319744706, -0.0006524450145661831, -0.0006264460971578956, -0.000600447179749608, -0.0005744482623413205, -0.000548449344933033, -0.0005224504275247455, -0.0004964515101164579, -0.00047045256360433996, -0.00044445364619605243, -0.0004184547287877649, -0.0003924558113794774, -0.00036645689397118986, -0.00034045797656290233, -0.0003144590591546148, -0.0002884601417463273, -0.00026246122433803976, -0.000236462292377837, -0.00021046337496954948, -0.00018446445756126195, -0.00015846554015297443, -0.0001324666227446869, -0.00010646769806044176, -8.046878065215424e-05, -5.4469859605887905e-05, -2.8470940378610976e-05, -2.4720211513340473e-06, 2.352689807594288e-05, 4.952581730321981e-05, 7.552473834948614e-05, 0.00010152365575777367, 0.0001275225804420188, 0.00015352149785030633, 0.00017952041525859386, 0.00020551933266688138, 0.0002315182500751689, 0.00025751718203537166, 0.0002835160994436592, 0.0003095150168519467, 0.00033551393426023424, 0.00036151285166852176, 0.0003875117690768093, 0.0004135106864850968, 0.00043950960389338434, 0.00046550852130167186, 0.0004915074678137898, 0.0005175063852220774, 0.0005435053026303649, 0.0005695042200386524, 0.0005955031374469399, 0.0006215020548552275, 0.000647500972263515, 0.0006734998896718025, 0.00069949880708009, 0.0007254977244883776, 0.0007514966418966651, 0.0007774955593049526, 0.0008034944767132401, 0.0008294933941215277]}, '_timestamp': 1717516527.435337}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.weight': {'_type': 'histogram', 'values': [271, 298, 327, 276, 279, 267, 296, 304, 280, 323, 291, 299, 326, 292, 300, 256, 299, 283, 287, 286, 280, 286, 276, 309, 292, 259, 291, 312, 288, 277, 281, 297, 320, 273, 279, 280, 270, 316, 269, 309, 273, 296, 319, 265, 317, 275, 271, 289, 284, 288, 303, 289, 291, 306, 293, 290, 271, 269, 295, 298, 299, 276, 287, 248], 'bins': [-0.21065117418766022, -0.20406781136989594, -0.19748443365097046, -0.19090107083320618, -0.1843176931142807, -0.17773433029651642, -0.17115095257759094, -0.16456758975982666, -0.15798422694206238, -0.1514008492231369, -0.14481748640537262, -0.13823410868644714, -0.13165074586868286, -0.12506736814975739, -0.1184840053319931, -0.11190063506364822, -0.10531726479530334, -0.09873389452695847, -0.09215052425861359, -0.08556715399026871, -0.07898378372192383, -0.07240042090415955, -0.06581705063581467, -0.05923368036746979, -0.05265031009912491, -0.04606693983078003, -0.03948356956243515, -0.03290020301938057, -0.02631683275103569, -0.01973346248269081, -0.013150094076991081, -0.006566724739968777, 1.6644597053527832e-05, 0.006600013934075832, 0.013183383271098137, 0.019766751676797867, 0.026350121945142746, 0.032933492213487625, 0.039516858756542206, 0.046100229024887085, 0.052683599293231964, 0.05926696956157684, 0.06585033982992172, 0.0724337100982666, 0.07901707291603088, 0.08560044318437576, 0.09218381345272064, 0.09876718372106552, 0.1053505539894104, 0.11193392425775528, 0.11851729452610016, 0.12510065734386444, 0.13168403506278992, 0.1382673978805542, 0.14485077559947968, 0.15143413841724396, 0.15801751613616943, 0.16460087895393372, 0.171184241771698, 0.17776761949062347, 0.18435098230838776, 0.19093436002731323, 0.19751772284507751, 0.204101100564003, 0.21068446338176727]}, '_timestamp': 1717516527.435612}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.bias': {'_type': 'histogram', 'values': [4, 2, 3, 2, 7, 0, 3, 1, 3, 2, 0, 0, 3, 1, 1, 3, 2, 1, 2, 2, 3, 4, 1, 1, 1, 1, 5, 3, 1, 2, 4, 1, 1, 1, 3, 5, 0, 3, 2, 3, 2, 2, 3, 1, 2, 3, 0, 0, 2, 2, 5, 4, 4, 3, 2, 2, 2, 2, 0, 1, 3, 0, 2, 2], 'bins': [-0.08513350039720535, -0.08246156573295593, -0.07978963851928711, -0.07711771130561829, -0.07444577664136887, -0.07177384197711945, -0.06910191476345062, -0.0664299875497818, -0.06375805288553238, -0.06108612194657326, -0.058414191007614136, -0.055742260068655014, -0.05307032912969589, -0.05039839819073677, -0.04772646725177765, -0.04505453631281853, -0.042382605373859406, -0.039710674434900284, -0.03703874349594116, -0.03436681255698204, -0.03169488161802292, -0.029022950679063797, -0.026351019740104675, -0.023679088801145554, -0.021007157862186432, -0.01833522692322731, -0.01566329598426819, -0.012991365045309067, -0.010319434106349945, -0.007647503167390823, -0.004975572228431702, -0.00230364128947258, 0.00036828964948654175, 0.0030402205884456635, 0.005712151527404785, 0.008384082466363907, 0.011056013405323029, 0.01372794434428215, 0.016399875283241272, 0.019071806222200394, 0.021743737161159515, 0.024415668100118637, 0.02708759903907776, 0.02975952997803688, 0.032431460916996, 0.035103391855955124, 0.037775322794914246, 0.04044725373387337, 0.04311918467283249, 0.04579111561179161, 0.04846304655075073, 0.051134977489709854, 0.053806908428668976, 0.0564788393676281, 0.05915077030658722, 0.06182270124554634, 0.06449463218450546, 0.06716656684875488, 0.0698384940624237, 0.07251042127609253, 0.07518235594034195, 0.07785429060459137, 0.08052621781826019, 0.08319814503192902, 0.08587007969617844]}, '_timestamp': 1717516527.435772}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.weight': {'_type': 'histogram', 'values': [4, 7, 8, 5, 6, 2, 3, 5, 1, 2, 3, 2, 2, 4, 1, 0, 1, 0, 2, 0, 1, 1, 0, 1, 1, 2, 0, 1, 4, 1, 2, 3, 1, 2, 2, 1, 3, 0, 0, 1, 0, 0, 1, 2, 2, 2, 0, 0, 0, 1, 4, 4, 4, 2, 2, 3, 1, 2, 2, 4, 1, 4, 4, 6], 'bins': [0.9991668462753296, 0.9991928339004517, 0.9992188215255737, 0.999244749546051, 0.9992707371711731, 0.9992967247962952, 0.9993226528167725, 0.9993486404418945, 0.9993746280670166, 0.9994006156921387, 0.9994266033172607, 0.999452531337738, 0.9994785189628601, 0.9995045065879822, 0.9995304346084595, 0.9995564222335815, 0.9995824098587036, 0.9996083974838257, 0.9996343851089478, 0.999660313129425, 0.9996863007545471, 0.9997122883796692, 0.9997382164001465, 0.9997642040252686, 0.9997901916503906, 0.9998161792755127, 0.9998421669006348, 0.9998680949211121, 0.9998940825462341, 0.9999200701713562, 0.9999459981918335, 0.9999719858169556, 0.9999979734420776, 1.0000239610671997, 1.0000499486923218, 1.0000759363174438, 1.0001018047332764, 1.0001277923583984, 1.0001537799835205, 1.0001797676086426, 1.0002057552337646, 1.0002317428588867, 1.0002577304840088, 1.0002837181091309, 1.000309705734253, 1.0003355741500854, 1.0003615617752075, 1.0003875494003296, 1.0004135370254517, 1.0004395246505737, 1.0004655122756958, 1.0004914999008179, 1.0005173683166504, 1.0005433559417725, 1.0005693435668945, 1.0005953311920166, 1.0006213188171387, 1.0006473064422607, 1.0006732940673828, 1.0006992816925049, 1.000725269317627, 1.0007511377334595, 1.0007771253585815, 1.0008031129837036, 1.0008291006088257]}, '_timestamp': 1717516527.435931}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.bias': {'_type': 'histogram', 'values': [6, 4, 0, 6, 5, 2, 1, 6, 3, 0, 1, 2, 2, 1, 1, 2, 1, 4, 3, 1, 0, 2, 2, 0, 2, 2, 2, 1, 4, 1, 3, 1, 0, 2, 1, 1, 3, 2, 0, 3, 0, 2, 0, 1, 2, 2, 1, 1, 1, 1, 1, 1, 3, 3, 3, 2, 2, 1, 3, 3, 8, 4, 3, 5], 'bins': [-0.0008344975649379194, -0.0008084167493507266, -0.0007823359337635338, -0.0007562551181763411, -0.0007301743025891483, -0.0007040934870019555, -0.0006780126714147627, -0.00065193185582757, -0.0006258510402403772, -0.0005997702246531844, -0.0005736894090659916, -0.0005476085934787989, -0.0005215277778916061, -0.0004954469623044133, -0.000469366175821051, -0.00044328536023385823, -0.00041720454464666545, -0.0003911237290594727, -0.0003650429134722799, -0.00033896209788508713, -0.00031288128229789436, -0.0002868004667107016, -0.0002607196511235088, -0.00023463885008823127, -0.0002085580345010385, -0.00018247721891384572, -0.00015639640332665294, -0.00013031558773946017, -0.00010423477942822501, -7.815396384103224e-05, -5.207315189181827e-05, -2.59923381236149e-05, 8.847564458847046e-08, 2.616928941279184e-05, 5.225010318099521e-05, 7.833091513020918e-05, 0.00010441173071740195, 0.0001304925390286371, 0.00015657335461582989, 0.00018265417020302266, 0.00020873498579021543, 0.0002348158013774082, 0.00026089660241268575, 0.0002869774179998785, 0.0003130582335870713, 0.0003391390491742641, 0.00036521986476145685, 0.0003913006803486496, 0.0004173814959358424, 0.00044346231152303517, 0.00046954312711022794, 0.0004956239135935903, 0.000521704729180783, 0.0005477855447679758, 0.0005738663603551686, 0.0005999471759423614, 0.0006260279915295541, 0.0006521088071167469, 0.0006781896227039397, 0.0007042704382911325, 0.0007303512538783252, 0.000756432069465518, 0.0007825128850527108, 0.0008085937006399035, 0.0008346745162270963]}, '_timestamp': 1717516527.436079}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.weight': {'_type': 'histogram', 'values': [257, 308, 289, 284, 284, 270, 284, 288, 256, 308, 260, 259, 264, 293, 279, 277, 283, 288, 307, 311, 300, 292, 277, 311, 290, 300, 293, 281, 289, 285, 276, 308, 288, 305, 278, 299, 310, 308, 289, 279, 319, 279, 293, 315, 285, 266, 293, 311, 288, 270, 300, 274, 266, 310, 304, 304, 268, 304, 278, 276, 282, 307, 335, 262], 'bins': [-0.21070276200771332, -0.20412269234657288, -0.19754262268543243, -0.190962553024292, -0.18438248336315155, -0.1778024137020111, -0.17122234404087067, -0.16464228928089142, -0.15806221961975098, -0.15148214995861053, -0.1449020802974701, -0.13832201063632965, -0.1317419409751892, -0.12516187131404877, -0.11858180165290833, -0.11200173199176788, -0.10542166233062744, -0.0988416001200676, -0.09226153045892715, -0.08568146079778671, -0.07910139113664627, -0.07252132147550583, -0.06594125181436539, -0.059361185878515244, -0.0527811199426651, -0.04620105028152466, -0.039620980620384216, -0.033040910959243774, -0.02646084502339363, -0.01988077536225319, -0.013300707563757896, -0.006720638833940029, -0.00014057010412216187, 0.006439498625695705, 0.013019567355513573, 0.019599635154008865, 0.026179704815149307, 0.03275977075099945, 0.03933984041213989, 0.045919910073280334, 0.052499979734420776, 0.05908004567027092, 0.06566011160612106, 0.0722401812672615, 0.07882025092840195, 0.08540032058954239, 0.09198039025068283, 0.09856045991182327, 0.10514052212238312, 0.11172059178352356, 0.118300661444664, 0.12488073110580444, 0.13146080076694489, 0.13804087042808533, 0.14462094008922577, 0.1512010097503662, 0.15778107941150665, 0.1643611490726471, 0.17094120383262634, 0.17752127349376678, 0.18410134315490723, 0.19068141281604767, 0.1972614824771881, 0.20384155213832855, 0.210421621799469]}, '_timestamp': 1717516527.436338}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.bias': {'_type': 'histogram', 'values': [3, 1, 4, 4, 0, 2, 4, 4, 3, 1, 1, 0, 4, 5, 1, 4, 6, 3, 3, 1, 2, 2, 0, 3, 1, 2, 3, 2, 0, 2, 2, 3, 4, 3, 0, 1, 1, 4, 1, 1, 3, 2, 2, 2, 2, 0, 4, 2, 2, 1, 3, 1, 4, 2, 2, 2, 0, 1, 1, 0, 1, 3, 3, 2], 'bins': [-0.08300963044166565, -0.08039087057113647, -0.0777721181511879, -0.07515335828065872, -0.07253460586071014, -0.06991584599018097, -0.06729709357023239, -0.06467833369970322, -0.06205957755446434, -0.059440821409225464, -0.05682206153869629, -0.05420330539345741, -0.051584549248218536, -0.04896579310297966, -0.046347036957740784, -0.04372828081250191, -0.04110952466726303, -0.038490764796733856, -0.03587200865149498, -0.033253252506256104, -0.030634496361017227, -0.0280157383531332, -0.025396982207894325, -0.02277822606265545, -0.020159468054771423, -0.017540711909532547, -0.01492195576429367, -0.01230319868773222, -0.009684441611170769, -0.007065685465931892, -0.0044469283893704414, -0.0018281718948855996, 0.0007905848324298859, 0.0034093414433300495, 0.006028098054230213, 0.008646855130791664, 0.01126561127603054, 0.013884368352591991, 0.016503125429153442, 0.01912188157439232, 0.021740637719631195, 0.02435939572751522, 0.026978151872754097, 0.029596908017992973, 0.032215666025877, 0.034834422171115875, 0.03745317831635475, 0.04007193446159363, 0.0426906943321228, 0.04530945047736168, 0.047928206622600555, 0.05054696276783943, 0.05316571891307831, 0.055784475058317184, 0.05840323120355606, 0.061021991074085236, 0.06364074349403381, 0.06625950336456299, 0.06887826323509216, 0.07149701565504074, 0.07411577552556992, 0.0767345279455185, 0.07935328781604767, 0.08197204023599625, 0.08459080010652542]}, '_timestamp': 1717516527.436487}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.weight': {'_type': 'histogram', 'values': [11, 10, 7, 4, 12, 8, 4, 3, 4, 1, 2, 0, 3, 2, 0, 2, 0, 1, 1, 1, 2, 0, 2, 1, 2, 2, 0, 1, 1, 5, 1, 1, 3, 2, 2, 1, 1, 1, 4, 1, 1, 2, 1, 1, 0, 2, 0, 0, 2, 1, 2, 1, 3, 0, 0, 0, 3, 0, 0, 3, 3, 0, 1, 1], 'bins': [0.9991645216941833, 0.9991905689239502, 0.999216616153717, 0.9992426633834839, 0.9992687106132507, 0.9992947578430176, 0.9993208050727844, 0.9993468523025513, 0.9993728995323181, 0.999398946762085, 0.9994249939918518, 0.9994511008262634, 0.9994771480560303, 0.9995031952857971, 0.999529242515564, 0.9995552897453308, 0.9995813369750977, 0.9996073842048645, 0.9996334314346313, 0.9996594786643982, 0.999685525894165, 0.9997115731239319, 0.9997376203536987, 0.9997636675834656, 0.9997897148132324, 0.9998157620429993, 0.9998418092727661, 0.999867856502533, 0.9998939037322998, 0.9999199509620667, 0.9999459981918335, 0.9999720454216003, 0.9999980926513672, 1.0000241994857788, 1.0000501871109009, 1.0000762939453125, 1.0001022815704346, 1.0001283884048462, 1.0001543760299683, 1.0001804828643799, 1.000206470489502, 1.0002325773239136, 1.0002585649490356, 1.0002846717834473, 1.0003107786178589, 1.000336766242981, 1.0003628730773926, 1.0003888607025146, 1.0004149675369263, 1.0004409551620483, 1.00046706199646, 1.000493049621582, 1.0005191564559937, 1.0005451440811157, 1.0005712509155273, 1.0005972385406494, 1.000623345375061, 1.000649333000183, 1.0006754398345947, 1.0007014274597168, 1.0007275342941284, 1.0007535219192505, 1.000779628753662, 1.0008056163787842, 1.0008317232131958]}, '_timestamp': 1717516527.43664}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.bias': {'_type': 'histogram', 'values': [2, 8, 6, 1, 7, 4, 2, 3, 5, 0, 2, 1, 2, 1, 0, 2, 3, 1, 0, 1, 1, 3, 0, 2, 3, 2, 0, 1, 4, 2, 1, 2, 2, 0, 5, 4, 1, 3, 0, 4, 2, 0, 2, 1, 0, 0, 2, 1, 3, 1, 6, 2, 1, 3, 3, 2, 1, 2, 0, 3, 1, 3, 2, 4], 'bins': [-0.0008206702186726034, -0.0007948566926643252, -0.0007690431084483862, -0.0007432295824401081, -0.000717415998224169, -0.0006916024722158909, -0.0006657888879999518, -0.0006399753619916737, -0.0006141618359833956, -0.0005883482517674565, -0.0005625347257591784, -0.0005367211415432394, -0.0005109076155349612, -0.00048509406042285264, -0.00045928050531074405, -0.00043346695019863546, -0.00040765339508652687, -0.00038183986907824874, -0.00035602631396614015, -0.00033021275885403156, -0.000304399203741923, -0.0002785856486298144, -0.0002527720935177058, -0.00022695855295751244, -0.00020114501239731908, -0.0001753314572852105, -0.0001495179021731019, -0.00012370434706099331, -9.789080650079995e-05, -7.207725138869137e-05, -4.626370355254039e-05, -2.045015207841061e-05, 5.3633993957191706e-06, 3.117695086984895e-05, 5.699050234397873e-05, 8.280405018012971e-05, 0.0001086176052922383, 0.00013443114585243165, 0.00016024470096454024, 0.00018605825607664883, 0.00021187181118875742, 0.00023768535174895078, 0.00026349889230914414, 0.00028931244742125273, 0.0003151260025333613, 0.0003409395576454699, 0.0003667531127575785, 0.0003925666678696871, 0.0004183801938779652, 0.0004441937489900738, 0.0004700073041021824, 0.0004958208883181214, 0.0005216344143263996, 0.0005474479403346777, 0.0005732615245506167, 0.0005990750505588949, 0.0006248886347748339, 0.000650702160783112, 0.0006765156867913902, 0.0007023292710073292, 0.0007281427970156074, 0.0007539563812315464, 0.0007797699072398245, 0.0008055834914557636, 0.0008313970174640417]}, '_timestamp': 1717516527.436784}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.weight': {'_type': 'histogram', 'values': [4, 1, 2, 3, 2, 2, 3, 3, 1, 3, 1, 3, 1, 2, 0, 1, 1, 1, 2, 1, 5, 1, 4, 3, 2, 2, 1, 2, 0, 3, 1, 6, 2, 1, 5, 3, 0, 2, 1, 1, 0, 3, 4, 3, 6, 3, 3, 2, 2, 2, 1, 4, 5, 1, 1, 2, 1, 1, 3, 3, 1, 1, 1, 1], 'bins': [-0.2859582006931305, -0.2768765091896057, -0.26779481768608093, -0.25871312618255615, -0.24963143467903137, -0.2405497431755066, -0.2314680516719818, -0.22238636016845703, -0.21330466866493225, -0.20422297716140747, -0.1951412856578827, -0.1860595941543579, -0.17697790265083313, -0.16789621114730835, -0.15881451964378357, -0.1497328281402588, -0.140651136636734, -0.13156946003437042, -0.12248776108026505, -0.11340606957674026, -0.10432437807321548, -0.0952426865696907, -0.08616099506616592, -0.07707930356264114, -0.06799761950969696, -0.05891592428088188, -0.0498342327773571, -0.04075254127383232, -0.03167085349559784, -0.02258916012942791, -0.013507469557225704, -0.004425778519362211, 0.004655912518501282, 0.013737604022026062, 0.022819295525550842, 0.03190098702907562, 0.0409826785326004, 0.050064366310834885, 0.059146057814359665, 0.06822775304317474, 0.07730944454669952, 0.08639112859964371, 0.09547282010316849, 0.10455451160669327, 0.11363620311021805, 0.12271789461374283, 0.1317995935678482, 0.14088128507137299, 0.14996296167373657, 0.15904465317726135, 0.16812634468078613, 0.1772080361843109, 0.1862897276878357, 0.19537141919136047, 0.20445311069488525, 0.21353480219841003, 0.22261649370193481, 0.2316981852054596, 0.24077987670898438, 0.24986156821250916, 0.25894325971603394, 0.2680249512195587, 0.2771066427230835, 0.2861883342266083, 0.29527002573013306]}, '_timestamp': 1717516527.4369369}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.bias': {'_type': 'histogram', 'values': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'bins': [-0.4172009825706482, -0.4015759825706482, -0.3859509825706482, -0.3703259825706482, -0.3547009825706482, -0.3390759825706482, -0.3234509825706482, -0.3078259825706482, -0.2922009825706482, -0.2765759825706482, -0.2609509825706482, -0.2453259825706482, -0.2297009825706482, -0.2140759825706482, -0.1984509825706482, -0.1828259825706482, -0.1672009825706482, -0.1515759825706482, -0.1359509825706482, -0.1203259825706482, -0.1047009825706482, -0.0890759825706482, -0.0734509825706482, -0.05782598257064819, -0.04220098257064819, -0.026575982570648193, -0.010950982570648193, 0.004674017429351807, 0.020299017429351807, 0.03592401742935181, 0.05154901742935181, 0.0671740174293518, 0.0827990174293518, 0.0984240174293518, 0.1140490174293518, 0.1296740174293518, 0.1452990174293518, 0.1609240174293518, 0.1765490174293518, 0.1921740174293518, 0.2077990174293518, 0.2234240174293518, 0.2390490174293518, 0.2546740174293518, 0.2702990174293518, 0.2859240174293518, 0.3015490174293518, 0.3171740174293518, 0.3327990174293518, 0.3484240174293518, 0.3640490174293518, 0.3796740174293518, 0.3952990174293518, 0.4109240174293518, 0.4265490174293518, 0.4421740174293518, 0.4577990174293518, 0.4734240174293518, 0.4890490174293518, 0.5046740174293518, 0.5202990174293518, 0.5359240174293518, 0.5515490174293518, 0.5671740174293518, 0.5827990174293518]}, '_timestamp': 1717516527.437108}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TP Reward Distribution': {'_type': 'histogram', 'values': [1, 1, 2, 2, 2, 2, 1, 0, 2, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1], 'bins': [2.933333333333333, 3.233333333333333, 3.533333333333333, 3.833333333333333, 4.133333333333333, 4.433333333333334, 4.7333333333333325, 5.033333333333333, 5.333333333333333, 5.633333333333333, 5.933333333333334, 6.2333333333333325, 6.533333333333333, 6.833333333333333, 7.133333333333333, 7.433333333333334, 7.7333333333333325, 8.033333333333333, 8.333333333333332, 8.633333333333333, 8.933333333333334, 9.233333333333333, 9.533333333333333, 9.833333333333332, 10.133333333333333, 10.433333333333334, 10.733333333333333, 11.033333333333333, 11.333333333333334, 11.633333333333333, 11.933333333333334, 12.233333333333333, 12.533333333333333, 12.833333333333334, 13.133333333333333, 13.433333333333334, 13.733333333333333, 14.033333333333333, 14.333333333333334, 14.633333333333333, 14.933333333333334, 15.233333333333333, 15.533333333333333, 15.833333333333334, 16.133333333333333, 16.433333333333334, 16.73333333333333, 17.03333333333333, 17.333333333333332, 17.633333333333333, 17.933333333333334, 18.23333333333333, 18.53333333333333, 18.833333333333332, 19.133333333333333, 19.433333333333334, 19.733333333333334, 20.03333333333333, 20.333333333333332, 20.633333333333333, 20.933333333333334, 21.233333333333334, 21.53333333333333, 21.833333333333332, 22.133333333333333]}, '_timestamp': 1717516527.437527}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TN Reward Distribution': {'_type': 'histogram', 'values': [2, 1, 1, 0, 4, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], 'bins': [3.533333333333333, 3.8010416666666664, 4.06875, 4.336458333333333, 4.604166666666666, 4.871874999999999, 5.139583333333333, 5.407291666666667, 5.675, 5.942708333333333, 6.210416666666666, 6.478125, 6.745833333333334, 7.013541666666667, 7.28125, 7.548958333333333, 7.816666666666666, 8.084375, 8.352083333333333, 8.619791666666666, 8.8875, 9.155208333333334, 9.422916666666666, 9.690625, 9.958333333333332, 10.226041666666667, 10.493749999999999, 10.761458333333334, 11.029166666666667, 11.296875, 11.564583333333333, 11.832291666666666, 12.1, 12.367708333333333, 12.635416666666666, 12.903125, 13.170833333333333, 13.438541666666666, 13.706249999999999, 13.973958333333332, 14.241666666666665, 14.509375, 14.777083333333334, 15.044791666666667, 15.3125, 15.580208333333333, 15.847916666666666, 16.115625, 16.383333333333333, 16.651041666666664, 16.91875, 17.186458333333334, 17.454166666666666, 17.721874999999997, 17.989583333333332, 18.257291666666667, 18.525, 18.792708333333334, 19.06041666666667, 19.328125, 19.59583333333333, 19.863541666666663, 20.13125, 20.398958333333333, 20.666666666666668]}, '_timestamp': 1717516527.4377239}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FP Reward Distribution': {'_type': 'histogram', 'values': [4, 4, 3, 0, 0, 0, 0, 0, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [3.3333333333333335, 5.069791666666667, 6.80625, 8.542708333333334, 10.279166666666667, 12.015625000000002, 13.752083333333335, 15.488541666666668, 17.225, 18.961458333333333, 20.697916666666668, 22.434375, 24.170833333333334, 25.907291666666666, 27.64375, 29.380208333333332, 31.116666666666667, 32.853125000000006, 34.58958333333334, 36.32604166666667, 38.06250000000001, 39.79895833333334, 41.53541666666667, 43.271875, 45.00833333333334, 46.74479166666667, 48.48125, 50.21770833333334, 51.95416666666667, 53.690625000000004, 55.427083333333336, 57.163541666666674, 58.900000000000006, 60.63645833333334, 62.372916666666676, 64.109375, 65.84583333333333, 67.58229166666666, 69.31875, 71.05520833333333, 72.79166666666667, 74.528125, 76.26458333333333, 78.00104166666667, 79.7375, 81.47395833333333, 83.21041666666666, 84.946875, 86.68333333333334, 88.41979166666667, 90.15625, 91.89270833333333, 93.62916666666666, 95.365625, 97.10208333333334, 98.83854166666667, 100.575, 102.31145833333333, 104.04791666666667, 105.784375, 107.52083333333333, 109.25729166666667, 110.99375, 112.73020833333334, 114.46666666666667]}, '_timestamp': 1717516527.437883}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FN Reward Distribution': {'_type': 'histogram', 'values': [8, 3, 5, 2, 1, 0, 1, 0, 0, 0, 1, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [2.6666666666666665, 4.14375, 5.620833333333334, 7.097916666666666, 8.575, 10.052083333333332, 11.529166666666667, 13.00625, 14.483333333333333, 15.960416666666665, 17.4375, 18.914583333333333, 20.39166666666667, 21.868750000000002, 23.345833333333335, 24.822916666666668, 26.3, 27.777083333333334, 29.254166666666666, 30.73125, 32.20833333333333, 33.68541666666667, 35.162499999999994, 36.63958333333333, 38.11666666666667, 39.59375, 41.07083333333333, 42.547916666666666, 44.025, 45.50208333333333, 46.979166666666664, 48.45625, 49.93333333333333, 51.41041666666666, 52.887499999999996, 54.36458333333333, 55.84166666666666, 57.318749999999994, 58.79583333333333, 60.27291666666666, 61.74999999999999, 63.22708333333333, 64.70416666666667, 66.18125, 67.65833333333333, 69.13541666666667, 70.6125, 72.08958333333334, 73.56666666666668, 75.04375, 76.52083333333334, 77.99791666666667, 79.47500000000001, 80.95208333333333, 82.42916666666667, 83.90625, 85.38333333333334, 86.86041666666667, 88.3375, 89.81458333333333, 91.29166666666667, 92.76875, 94.24583333333334, 95.72291666666666, 97.2]}, '_timestamp': 1717516527.4380338}).
Epoch 870/1000, Train Loss: 0.6164860352873802, Val Loss: 0.25130802392959595, Train Acc: 0.7875, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 880/1000, Train Loss: 0.6863865256309509, Val Loss: 0.25297123193740845, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 890/1000, Train Loss: 0.5680683851242065, Val Loss: 0.2519245445728302, Train Acc: 0.85, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 900/1000, Train Loss: 0.6303017735481262, Val Loss: 0.25045353174209595, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 910/1000, Train Loss: 0.6768696457147598, Val Loss: 0.2509264349937439, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 920/1000, Train Loss: 0.5586176365613937, Val Loss: 0.25110477209091187, Train Acc: 0.8375, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 930/1000, Train Loss: 0.5700129717588425, Val Loss: 0.2518027722835541, Train Acc: 0.825, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 940/1000, Train Loss: 0.5919036865234375, Val Loss: 0.2518596053123474, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 950/1000, Train Loss: 0.6035079061985016, Val Loss: 0.2517954111099243, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 960/1000, Train Loss: 0.6475595235824585, Val Loss: 0.25102561712265015, Train Acc: 0.775, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 970/1000, Train Loss: 0.5773656666278839, Val Loss: 0.25150784850120544, Train Acc: 0.8125, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 980/1000, Train Loss: 0.625744566321373, Val Loss: 0.2522483766078949, Train Acc: 0.8, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Epoch 990/1000, Train Loss: 0.7119907885789871, Val Loss: 0.2524634301662445, Train Acc: 0.7875, Val Acc: 0.8999999761581421, LR: 2.7839715655709815e-06
Finished training model...
Simulating on true reward function...
Running for a maximum of 11 generations...
 ****** Running generation 0 ******
Population's average fitness: 83.73333 stdev: 169.65408
Best fitness: 698.53333 - size: (4, 20) - species 1 - id 8
Average adjusted fitness: 0.111
Mean genetic distance 0.817, standard deviation 0.175
Population of 20 members in 1 species:
   ID   age  size  fitness  adj fit  stag
  ====  ===  ====  =======  =======  ====
     1    0    20    698.5    0.111     0
Total extinctions: 0
Generation time: 1.168 sec
 ****** Running generation 1 ******
[33m[W 2024-06-04 11:55:45,717][39m Trial 0 failed with parameters: {'hidden_size': 136, 'learning_rate': 0.0002783971565570968, 'weight_decay': 8.804655049081892e-05} because of the following error: The value None could not be cast to float..
[33m[W 2024-06-04 11:55:45,717][39m Trial 0 failed with value None.
Population's average fitness: 352.66000 stdev: 579.86005
Best fitness: 2470.00000 - size: (4, 20) - species 1 - id 24
Average adjusted fitness: 0.140
Mean genetic distance 0.859, standard deviation 0.189
Population of 20 members in 1 species:
   ID   age  size  fitness  adj fit  stag
  ====  ===  ====  =======  =======  ====
     1    1    20   2470.0    0.140     0
Total extinctions: 0
Generation time: 1.986 sec (1.577 average)
 ****** Running generation 2 ******
Population's average fitness: 3947.07000 stdev: 9962.74292
Best fitness: 40889.33333 - size: (4, 20) - species 1 - id 42
Average adjusted fitness: 0.096
Mean genetic distance 0.826, standard deviation 0.182
Population of 20 members in 1 species:
   ID   age  size  fitness  adj fit  stag
  ====  ===  ====  =======  =======  ====
     1    2    20  40889.3    0.096     0
Total extinctions: 0
Generation time: 7.720 sec (3.625 average)
 ****** Running generation 3 ******
Traceback (most recent call last):
  File "/Users/alextang/Documents/EmergeLab/ai-car-preference-learning/main.py", line 77, in <module>
    start_simulation("./config/agent_config.txt", args.generations[0])
  File "/Users/alextang/Documents/EmergeLab/ai-car-preference-learning/main.py", line 17, in start_simulation
    run_population(
  File "/Users/alextang/Documents/EmergeLab/ai-car-preference-learning/agent.py", line 404, in run_population
    best_genome = population.run(
  File "/Users/alextang/.pyenv/versions/nocturne_lab/lib/python3.10/site-packages/neat/population.py", line 89, in run
    fitness_function(list(iteritems(self.population)), self.config)
  File "/Users/alextang/Documents/EmergeLab/ai-car-preference-learning/agent.py", line 325, in run_simulation
    car.update(game_map)
  File "/Users/alextang/Documents/EmergeLab/ai-car-preference-learning/agent.py", line 175, in update
    self.check_radar(d, game_map)
  File "/Users/alextang/Documents/EmergeLab/ai-car-preference-learning/agent.py", line 108, in check_radar
    y = int(
KeyboardInterrupt