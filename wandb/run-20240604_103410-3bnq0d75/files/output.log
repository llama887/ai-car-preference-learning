Epoch 0/1000, Train Loss: 2.684103012084961, Val Loss: 0.6057740449905396, Train Acc: 0.6, Val Acc: 0.800000011920929, LR: 0.0005771293724596562
Epoch 10/1000, Train Loss: 2.071822226047516, Val Loss: 0.5748218297958374, Train Acc: 0.6, Val Acc: 0.800000011920929, LR: 0.0005656795402171146
Epoch 20/1000, Train Loss: 1.0922508239746094, Val Loss: 0.5493053793907166, Train Acc: 0.65, Val Acc: 0.800000011920929, LR: 0.0005542297079745728
Epoch 30/1000, Train Loss: 1.6083356142044067, Val Loss: 0.5253051519393921, Train Acc: 0.5, Val Acc: 0.800000011920929, LR: 0.0005427798757320311
Epoch 40/1000, Train Loss: 1.3773913979530334, Val Loss: 0.5046716332435608, Train Acc: 0.625, Val Acc: 0.800000011920929, LR: 0.0005313300434894891
Epoch 50/1000, Train Loss: 1.1304905712604523, Val Loss: 0.488302618265152, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 0.0005198802112469474
Epoch 60/1000, Train Loss: 1.2522112131118774, Val Loss: 0.4664284586906433, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 0.0005084303790044056
Epoch 70/1000, Train Loss: 1.3765460848808289, Val Loss: 0.43532681465148926, Train Acc: 0.625, Val Acc: 0.800000011920929, LR: 0.0004969805467618638
Epoch 80/1000, Train Loss: 0.9312692284584045, Val Loss: 0.39864641427993774, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 0.0004855307145193221
Epoch 90/1000, Train Loss: 1.00973379611969, Val Loss: 0.4023513197898865, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 0.00047408088227678076
Epoch 100/1000, Train Loss: 1.0037235617637634, Val Loss: 0.39910373091697693, Train Acc: 0.625, Val Acc: 0.800000011920929, LR: 0.00046263105003423936
Epoch 110/1000, Train Loss: 0.8891777694225311, Val Loss: 0.378665030002594, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 0.000451181217791698
Epoch 120/1000, Train Loss: 0.7504835724830627, Val Loss: 0.39003485441207886, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 0.0004397313855491567
Epoch 130/1000, Train Loss: 0.7670947313308716, Val Loss: 0.3824829161167145, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 0.0004282815533066154
Epoch 140/1000, Train Loss: 0.8562994003295898, Val Loss: 0.3965834677219391, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 0.000416831721064074
Epoch 150/1000, Train Loss: 0.8690858483314514, Val Loss: 0.3840891122817993, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 0.0004053818888215328
Epoch 160/1000, Train Loss: 0.9036710560321808, Val Loss: 0.3954400420188904, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 0.0003939320565789915
Epoch 170/1000, Train Loss: 0.7068688273429871, Val Loss: 0.38398265838623047, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 0.00038248222433645015
Epoch 180/1000, Train Loss: 1.0153357982635498, Val Loss: 0.40109795331954956, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 0.00037103239209390865
Epoch 190/1000, Train Loss: 0.8524075150489807, Val Loss: 0.3851330876350403, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 0.0003595825598513673
Epoch 200/1000, Train Loss: 0.6872304379940033, Val Loss: 0.3813360333442688, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 0.0003481327276088258
Epoch 210/1000, Train Loss: 1.0734423995018005, Val Loss: 0.4025961756706238, Train Acc: 0.625, Val Acc: 0.800000011920929, LR: 0.0003366828953662844
Epoch 220/1000, Train Loss: 0.7651540637016296, Val Loss: 0.39653149247169495, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 0.0003252330631237431
Epoch 230/1000, Train Loss: 0.8409940302371979, Val Loss: 0.3940190374851227, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 0.00031378323088120184
Epoch 240/1000, Train Loss: 0.9480248987674713, Val Loss: 0.38640373945236206, Train Acc: 0.65, Val Acc: 0.800000011920929, LR: 0.00030233339863866055
Epoch 250/1000, Train Loss: 1.1382310390472412, Val Loss: 0.3828165531158447, Train Acc: 0.625, Val Acc: 0.800000011920929, LR: 0.0002908835663961192
Epoch 260/1000, Train Loss: 1.0138834118843079, Val Loss: 0.39189594984054565, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 0.00027943373415357804
Epoch 270/1000, Train Loss: 0.711968719959259, Val Loss: 0.39251893758773804, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 0.00026798390191103675
Epoch 280/1000, Train Loss: 0.7465153932571411, Val Loss: 0.3835725784301758, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 0.0002565340696684955
Epoch 290/1000, Train Loss: 0.7315089702606201, Val Loss: 0.3859894871711731, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 0.0002450842374259542
Epoch 300/1000, Train Loss: 0.538042813539505, Val Loss: 0.38737183809280396, Train Acc: 0.875, Val Acc: 0.800000011920929, LR: 0.0002336344051834128
Epoch 310/1000, Train Loss: 0.7387222647666931, Val Loss: 0.39073383808135986, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 0.00022218457294087137
Epoch 320/1000, Train Loss: 0.6581574082374573, Val Loss: 0.37340253591537476, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 0.00021073474069832995
Epoch 330/1000, Train Loss: 0.5806642770767212, Val Loss: 0.39919304847717285, Train Acc: 0.875, Val Acc: 0.800000011920929, LR: 0.0001992849084557885
Epoch 340/1000, Train Loss: 0.641929030418396, Val Loss: 0.39301472902297974, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 0.00018783507621324708
Epoch 350/1000, Train Loss: 0.7262950241565704, Val Loss: 0.39111101627349854, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 0.00017638524397070563
Epoch 360/1000, Train Loss: 0.6097663938999176, Val Loss: 0.3883635699748993, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 0.00016493541172816418
Epoch 370/1000, Train Loss: 0.7630808353424072, Val Loss: 0.3946087956428528, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 0.00015348557948562273
Epoch 380/1000, Train Loss: 0.6014085710048676, Val Loss: 0.3916725516319275, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 0.00014203574724308129
Epoch 390/1000, Train Loss: 0.6923094987869263, Val Loss: 0.39761561155319214, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 0.00013058591500053984
Epoch 400/1000, Train Loss: 0.6605203151702881, Val Loss: 0.38807210326194763, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 0.00011913608275799844
Epoch 410/1000, Train Loss: 0.8072267174720764, Val Loss: 0.39175939559936523, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 0.0001076862505154571
Epoch 420/1000, Train Loss: 0.6987970471382141, Val Loss: 0.3999311029911041, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 9.623641827291572e-05
Epoch 430/1000, Train Loss: 0.7133129239082336, Val Loss: 0.38095220923423767, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 8.478658603037436e-05
Epoch 440/1000, Train Loss: 0.9755026996135712, Val Loss: 0.38065391778945923, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 7.3336753787833e-05
Epoch 450/1000, Train Loss: 0.752506822347641, Val Loss: 0.38888493180274963, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 6.188692154529168e-05
Epoch 460/1000, Train Loss: 0.6561128199100494, Val Loss: 0.3958868086338043, Train Acc: 0.85, Val Acc: 0.800000011920929, LR: 5.043708930275033e-05
Epoch 470/1000, Train Loss: 0.6560552418231964, Val Loss: 0.3954578638076782, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 3.8987257060208993e-05
Epoch 480/1000, Train Loss: 0.66473388671875, Val Loss: 0.3908763825893402, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 2.7537424817667654e-05
Epoch 490/1000, Train Loss: 0.6067287921905518, Val Loss: 0.38660138845443726, Train Acc: 0.85, Val Acc: 0.800000011920929, LR: 1.6087592575126294e-05
Epoch 500/1000, Train Loss: 0.7579583823680878, Val Loss: 0.3854582905769348, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 510/1000, Train Loss: 0.6584064662456512, Val Loss: 0.3852653503417969, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 520/1000, Train Loss: 0.6115005165338516, Val Loss: 0.38623493909835815, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 530/1000, Train Loss: 0.533435270190239, Val Loss: 0.386145681142807, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 540/1000, Train Loss: 0.6071354746818542, Val Loss: 0.38529786467552185, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 550/1000, Train Loss: 0.6939124464988708, Val Loss: 0.3855099678039551, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 560/1000, Train Loss: 0.6999043822288513, Val Loss: 0.385599285364151, Train Acc: 0.85, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 570/1000, Train Loss: 0.8719072341918945, Val Loss: 0.38626569509506226, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 580/1000, Train Loss: 0.9410649836063385, Val Loss: 0.3866649270057678, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 590/1000, Train Loss: 0.757007896900177, Val Loss: 0.3862845003604889, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 600/1000, Train Loss: 0.6225365698337555, Val Loss: 0.38631752133369446, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 610/1000, Train Loss: 0.8763770163059235, Val Loss: 0.3870215117931366, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 620/1000, Train Loss: 0.6423410177230835, Val Loss: 0.3881199359893799, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 630/1000, Train Loss: 0.5815830528736115, Val Loss: 0.38810935616493225, Train Acc: 0.85, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 640/1000, Train Loss: 0.5930264294147491, Val Loss: 0.38859236240386963, Train Acc: 0.875, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 650/1000, Train Loss: 0.8184358775615692, Val Loss: 0.3882387578487396, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 660/1000, Train Loss: 0.7743433117866516, Val Loss: 0.38872843980789185, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 670/1000, Train Loss: 0.7216644883155823, Val Loss: 0.38824039697647095, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 680/1000, Train Loss: 0.7988399267196655, Val Loss: 0.387887179851532, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 690/1000, Train Loss: 0.6533896327018738, Val Loss: 0.3881438374519348, Train Acc: 0.85, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
[34m[1mwandb[39m[22m: [33mWARNING[39m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'Train Loss': 2.684103012084961, 'Validation Loss': 0.6057740449905396, 'Train Accuracy': 0.6, 'Validation Accuracy': 0.800000011920929, '_timestamp': 1717511651.653122}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.weight': {'_type': 'histogram', 'values': [2839, 6208, 6161, 6214, 6144, 6234, 6171, 6262, 6193, 6107, 6232, 6215, 6075, 6313, 6197, 6030, 6549, 6390, 6226, 6198, 6257, 6229, 6148, 6183, 6360, 6286, 6255, 6185, 6392, 6230, 6224, 6206, 6124, 6166, 6295, 6120, 5966, 6012, 6161, 6093, 5973, 6103, 6173, 6004, 6109, 6109, 6095, 6142, 6121, 6179, 6082, 6136, 6081, 6209, 6327, 6066, 6156, 6042, 6229, 6270, 6147, 6089, 6208, 2900], 'bins': [-0.09602613002061844, -0.09302476048469543, -0.09002339094877243, -0.08702202141284943, -0.08402065187692642, -0.08101928234100342, -0.07801791280508041, -0.07501654326915741, -0.0720151737332344, -0.0690138041973114, -0.0660124346613884, -0.0630110651254654, -0.06000969931483269, -0.05700832977890968, -0.05400696024298668, -0.051005590707063675, -0.04800422117114067, -0.04500285163521767, -0.04200148209929466, -0.03900011256337166, -0.035998743027448654, -0.03299737349152565, -0.029996005818247795, -0.02699463628232479, -0.023993266746401787, -0.020991897210478783, -0.01799052767455578, -0.014989159069955349, -0.011987789534032345, -0.00898641999810934, -0.005985050927847624, -0.0029836816247552633, 1.7687678337097168e-05, 0.0030190569814294577, 0.006020426284521818, 0.009021795354783535, 0.01202316489070654, 0.015024534426629543, 0.018025903031229973, 0.021027272567152977, 0.02402864210307598, 0.027030011638998985, 0.03003138117492199, 0.033032748848199844, 0.03603411838412285, 0.03903548792004585, 0.04203685745596886, 0.04503822699189186, 0.048039596527814865, 0.05104096606373787, 0.05404233559966087, 0.05704370513558388, 0.06004507467150688, 0.06304644048213959, 0.06604781001806259, 0.0690491795539856, 0.0720505490899086, 0.0750519186258316, 0.07805328816175461, 0.08105465769767761, 0.08405602723360062, 0.08705739676952362, 0.09005876630544662, 0.09306013584136963, 0.09606150537729263]}, '_timestamp': 1717511651.656248}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc1.bias': {'_type': 'histogram', 'values': [11, 8, 7, 3, 9, 7, 6, 9, 9, 4, 2, 6, 5, 8, 4, 8, 11, 4, 6, 6, 5, 7, 7, 4, 2, 5, 9, 8, 8, 8, 6, 17, 8, 9, 7, 6, 11, 8, 8, 5, 4, 7, 6, 9, 9, 6, 5, 9, 3, 3, 7, 4, 4, 12, 5, 6, 10, 6, 8, 5, 8, 1, 4, 10], 'bins': [-0.032177191227674484, -0.03117206133902073, -0.030166931450366974, -0.029161803424358368, -0.028156673535704613, -0.027151543647050858, -0.02614641562104225, -0.025141285732388496, -0.02413615584373474, -0.023131025955080986, -0.02212589606642723, -0.021120768040418625, -0.02011563815176487, -0.019110508263111115, -0.01810538023710251, -0.017100250348448753, -0.016095120459794998, -0.015089990571141243, -0.014084861613810062, -0.013079732656478882, -0.012074602767825127, -0.011069472879171371, -0.010064343921840191, -0.00905921496450901, -0.008054085075855255, -0.007048955652862787, -0.006043826229870319, -0.0050386968068778515, -0.004033567383885384, -0.0030284379608929157, -0.002023308537900448, -0.00101817911490798, -1.3049691915512085e-05, 0.0009920797310769558, 0.0019972091540694237, 0.0030023385770618916, 0.0040074680000543594, 0.005012597423046827, 0.006017726846039295, 0.007022856269031763, 0.008027985692024231, 0.009033115580677986, 0.010038244538009167, 0.011043373495340347, 0.012048503383994102, 0.013053633272647858, 0.014058762229979038, 0.015063891187310219, 0.016069021075963974, 0.01707415096461773, 0.018079280853271484, 0.01908440887928009, 0.020089538767933846, 0.0210946686565876, 0.022099796682596207, 0.023104926571249962, 0.024110056459903717, 0.025115186348557472, 0.026120316237211227, 0.027125444263219833, 0.02813057415187359, 0.029135704040527344, 0.03014083206653595, 0.031145961955189705, 0.03215109184384346]}, '_timestamp': 1717511651.656468}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.weight': {'_type': 'histogram', 'values': [277, 23, 14, 10, 7, 4, 1, 0, 3, 2, 2, 1, 1, 3, 1, 2, 1, 3, 1, 2, 0, 3, 2, 2, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 2, 6, 3, 1, 3, 4, 1, 0, 1, 2, 0, 2, 2, 1, 1, 1, 1, 1, 0, 1, 1, 0, 4, 5, 2, 3, 11], 'bins': [0.998843252658844, 0.9988793730735779, 0.9989155530929565, 0.9989516735076904, 0.9989878535270691, 0.999023973941803, 0.9990601539611816, 0.9990962743759155, 0.9991323947906494, 0.9991685748100281, 0.999204695224762, 0.9992408752441406, 0.9992769956588745, 0.9993131160736084, 0.9993492960929871, 0.999385416507721, 0.9994215965270996, 0.9994577169418335, 0.9994938969612122, 0.999530017375946, 0.9995661377906799, 0.9996023178100586, 0.9996384382247925, 0.9996746182441711, 0.999710738658905, 0.9997469186782837, 0.9997830390930176, 0.9998191595077515, 0.9998553395271301, 0.999891459941864, 0.9999276399612427, 0.9999637603759766, 0.9999998807907104, 1.0000360012054443, 1.0000722408294678, 1.0001083612442017, 1.0001444816589355, 1.0001806020736694, 1.0002168416976929, 1.0002529621124268, 1.0002890825271606, 1.0003252029418945, 1.0003613233566284, 1.0003975629806519, 1.0004336833953857, 1.0004698038101196, 1.0005059242248535, 1.0005420446395874, 1.0005782842636108, 1.0006144046783447, 1.0006505250930786, 1.0006866455078125, 1.0007227659225464, 1.0007590055465698, 1.0007951259613037, 1.0008312463760376, 1.0008673667907715, 1.000903606414795, 1.0009397268295288, 1.0009758472442627, 1.0010119676589966, 1.0010480880737305, 1.001084327697754, 1.0011204481124878, 1.0011565685272217]}, '_timestamp': 1717511651.6566489}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn1.bias': {'_type': 'histogram', 'values': [31, 8, 6, 5, 3, 49, 2, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 0, 3, 2, 1, 1, 2, 2, 2, 3, 2, 2, 1, 0, 0, 0, 123, 0, 0, 0, 2, 4, 7, 1, 2, 4, 1, 4, 5, 3, 1, 2, 3, 1, 2, 1, 3, 5, 3, 2, 4, 5, 33, 3, 9, 11, 16, 33], 'bins': [-0.001156743266619742, -0.0011205957271158695, -0.001084448304027319, -0.0010483007645234466, -0.001012153341434896, -0.0009760058019310236, -0.0009398583206348121, -0.0009037107811309397, -0.0008675632998347282, -0.0008314158185385168, -0.0007952683372423053, -0.0007591208559460938, -0.0007229733164422214, -0.0006868258351460099, -0.0006506783538497984, -0.000614530872553587, -0.0005783833330497146, -0.0005422358517535031, -0.0005060883704572916, -0.0004699408891610801, -0.0004337933787610382, -0.0003976458974648267, -0.00036149838706478477, -0.0003253509057685733, -0.0002892034244723618, -0.00025305591407231987, -0.00021690841822419316, -0.00018076092237606645, -0.00014461344107985497, -0.00010846593795577064, -7.231844938360155e-05, -3.617095353547484e-05, -2.3457687348127365e-08, 3.612403816077858e-05, 7.227153400890529e-05, 0.00010841902258107439, 0.0001445665257051587, 0.0001807140070013702, 0.0002168615028494969, 0.0002530089986976236, 0.00028915650909766555, 0.00032530399039387703, 0.0003614514716900885, 0.00039759898209013045, 0.00043374646338634193, 0.00046989397378638387, 0.0005060414550825953, 0.0005421889363788068, 0.0005783364176750183, 0.0006144839571788907, 0.0006506314384751022, 0.0006867789197713137, 0.0007229264010675251, 0.0007590739405713975, 0.000795221421867609, 0.0008313689031638205, 0.000867516384460032, 0.0009036638657562435, 0.0009398114052601159, 0.0009759588865563273, 0.0010121064260601997, 0.0010482538491487503, 0.0010844013886526227, 0.0011205488117411733, 0.0011566963512450457]}, '_timestamp': 1717511651.656812}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.weight': {'_type': 'histogram', 'values': [1612, 2961, 3043, 2872, 2946, 2906, 2984, 3089, 2980, 3064, 2992, 2853, 2926, 2874, 2923, 2885, 2956, 3058, 2983, 2989, 2906, 2891, 2968, 2875, 2958, 2922, 2939, 3136, 2956, 3032, 2852, 3266, 3376, 2985, 2907, 2980, 2926, 2955, 2987, 2874, 2920, 2844, 2923, 2825, 3042, 2989, 2922, 3007, 2981, 2949, 2913, 2920, 3053, 2986, 2931, 2901, 2939, 2902, 2892, 2956, 2890, 2897, 2925, 1530], 'bins': [-0.1189785897731781, -0.11526070535182953, -0.11154281347990036, -0.10782492905855179, -0.10410703718662262, -0.10038915276527405, -0.09667126834392548, -0.09295337647199631, -0.08923549205064774, -0.08551760017871857, -0.08179971575737, -0.07808183133602142, -0.07436393946409225, -0.07064605504274368, -0.06692816317081451, -0.06321027874946594, -0.05949239060282707, -0.0557745024561882, -0.05205661430954933, -0.04833872988820076, -0.04462084174156189, -0.04090295359492302, -0.03718506544828415, -0.03346717730164528, -0.029749291017651558, -0.026031402871012688, -0.022313516587018967, -0.018595628440380096, -0.0148777412250638, -0.011159854009747505, -0.007441966328769922, -0.003724078880622983, -6.191432476043701e-06, 0.0037116960156708956, 0.007429583463817835, 0.011147471144795418, 0.014865358360111713, 0.01858324557542801, 0.02230113372206688, 0.0260190200060606, 0.02973690815269947, 0.03345479443669319, 0.03717268258333206, 0.04089057072997093, 0.0446084588766098, 0.04832634702324867, 0.052044231444597244, 0.055762119591236115, 0.059480007737874985, 0.06319789588451385, 0.06691578030586243, 0.0706336721777916, 0.07435155659914017, 0.07806944847106934, 0.08178733289241791, 0.08550521731376648, 0.08922310918569565, 0.09294099360704422, 0.09665888547897339, 0.10037676990032196, 0.10409465432167053, 0.1078125461935997, 0.11153043061494827, 0.11524832248687744, 0.11896620690822601]}, '_timestamp': 1717511651.657987}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc2.bias': {'_type': 'histogram', 'values': [5, 7, 4, 10, 6, 8, 3, 6, 3, 4, 9, 7, 9, 5, 5, 8, 9, 5, 1, 7, 5, 6, 8, 7, 5, 6, 11, 11, 5, 8, 7, 5, 2, 4, 9, 4, 11, 12, 4, 5, 6, 9, 12, 6, 12, 4, 8, 10, 6, 7, 8, 5, 10, 1, 2, 6, 4, 13, 6, 11, 6, 6, 9, 9], 'bins': [-0.04793361574411392, -0.04642760381102562, -0.04492159187793732, -0.043415576219558716, -0.04190956428647041, -0.04040355235338211, -0.03889753669500351, -0.03739152476191521, -0.035885512828826904, -0.0343795008957386, -0.0328734889626503, -0.0313674733042717, -0.029861461371183395, -0.028355449438095093, -0.02684943564236164, -0.02534342184662819, -0.023837409913539886, -0.022331397980451584, -0.020825384184718132, -0.01931937038898468, -0.017813358455896378, -0.016307346522808075, -0.014801332727074623, -0.013295319862663746, -0.011789306998252869, -0.010283294133841991, -0.008777281269431114, -0.007271268405020237, -0.00576525554060936, -0.0042592426761984825, -0.0027532298117876053, -0.001247216947376728, 0.00025879591703414917, 0.0017648087814450264, 0.0032708216458559036, 0.004776834510266781, 0.006282847374677658, 0.007788860239088535, 0.009294873103499413, 0.01080088596791029, 0.012306898832321167, 0.013812911696732044, 0.015318924561142921, 0.016824938356876373, 0.018330950289964676, 0.01983696222305298, 0.02134297601878643, 0.022848989814519882, 0.024355001747608185, 0.025861013680696487, 0.02736702747642994, 0.02887304127216339, 0.030379053205251694, 0.031885065138339996, 0.0333910807967186, 0.0348970927298069, 0.0364031046628952, 0.037909116595983505, 0.03941512852907181, 0.04092114418745041, 0.04242715612053871, 0.043933168053627014, 0.045439183712005615, 0.04694519564509392, 0.04845120757818222]}, '_timestamp': 1717511651.658151}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.weight': {'_type': 'histogram', 'values': [172, 51, 32, 18, 7, 4, 7, 9, 3, 2, 3, 1, 5, 1, 1, 5, 2, 1, 1, 2, 3, 0, 2, 3, 0, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 5, 6, 8, 5, 1, 7, 7, 4, 3, 4, 1, 1, 4, 3, 0, 1, 1, 0, 2, 2, 1, 1, 1, 4, 4, 4, 2, 9], 'bins': [0.998843252658844, 0.9988793730735779, 0.9989154934883118, 0.9989516735076904, 0.9989877939224243, 0.9990239143371582, 0.9990600347518921, 0.999096155166626, 0.9991322755813599, 0.9991684556007385, 0.9992045760154724, 0.9992406964302063, 0.9992768168449402, 0.9993129372596741, 0.999349057674408, 0.9993852376937866, 0.9994213581085205, 0.9994574785232544, 0.9994935989379883, 0.9995297193527222, 0.999565839767456, 0.9996020197868347, 0.9996381402015686, 0.9996742606163025, 0.9997103810310364, 0.9997465014457703, 0.9997826218605042, 0.9998188018798828, 0.9998549222946167, 0.9998910427093506, 0.9999271631240845, 0.9999632835388184, 0.9999994039535522, 1.0000355243682861, 1.00007164478302, 1.0001078844070435, 1.0001440048217773, 1.0001801252365112, 1.0002162456512451, 1.000252366065979, 1.000288486480713, 1.0003246068954468, 1.0003607273101807, 1.0003968477249146, 1.0004329681396484, 1.0004690885543823, 1.0005052089691162, 1.0005414485931396, 1.0005775690078735, 1.0006136894226074, 1.0006498098373413, 1.0006859302520752, 1.000722050666809, 1.000758171081543, 1.0007942914962769, 1.0008304119110107, 1.0008665323257446, 1.0009026527404785, 1.0009387731552124, 1.0009750127792358, 1.0010111331939697, 1.0010472536087036, 1.0010833740234375, 1.0011194944381714, 1.0011556148529053]}, '_timestamp': 1717511651.65831}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn2.bias': {'_type': 'histogram', 'values': [45, 20, 17, 14, 4, 29, 4, 7, 4, 0, 6, 6, 3, 1, 2, 7, 1, 3, 2, 1, 5, 8, 4, 4, 3, 5, 6, 8, 1, 0, 0, 0, 4, 0, 0, 0, 5, 6, 6, 7, 3, 5, 5, 5, 6, 3, 1, 4, 3, 1, 7, 2, 2, 6, 2, 5, 7, 2, 20, 4, 14, 20, 13, 44], 'bins': [-0.0011567514156922698, -0.001120603526942432, -0.0010844555217772722, -0.0010483076330274343, -0.0010121597442775965, -0.0009760117973200977, -0.0009398638503625989, -0.0009037159034051001, -0.0008675680146552622, -0.0008314200676977634, -0.0007952721207402647, -0.0007591242319904268, -0.000722976285032928, -0.0006868283380754292, -0.0006506804493255913, -0.0006145325023680925, -0.0005783846136182547, -0.0005422366666607559, -0.0005060887197032571, -0.00046994080184958875, -0.0004337928839959204, -0.00039764493703842163, -0.0003614970191847533, -0.00032534910133108497, -0.0002892011543735862, -0.00025305323651991785, -0.00021690531866624951, -0.00018075738626066595, -0.0001446094538550824, -0.00010846153600141406, -7.23136035958305e-05, -3.6165678466204554e-05, -1.775333657860756e-08, 3.613017179304734e-05, 7.227809692267329e-05, 0.00010842602932825685, 0.00014457394718192518, 0.00018072187958750874, 0.0002168698119930923, 0.00025301772984676063, 0.00028916564770042896, 0.00032531359465792775, 0.0003614615125115961, 0.0003976094303652644, 0.0004337573773227632, 0.00046990529517643154, 0.0005060532130300999, 0.0005422011599875987, 0.0005783491069450974, 0.0006144969956949353, 0.0006506449426524341, 0.000686792831402272, 0.0007229407783597708, 0.0007590887253172696, 0.0007952366140671074, 0.0008313845610246062, 0.000867532507982105, 0.0009036803967319429, 0.0009398283436894417, 0.0009759762906469405, 0.0010121242376044393, 0.0010482721263542771, 0.001084420015104115, 0.0011205680202692747, 0.0011567159090191126]}, '_timestamp': 1717511651.658464}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.weight': {'_type': 'histogram', 'values': [1781, 3078, 2935, 3060, 2928, 2920, 2902, 2873, 2934, 2941, 2946, 2963, 3002, 2959, 2875, 3013, 2948, 2978, 3029, 2930, 2966, 2935, 2948, 2909, 3028, 2923, 2901, 2923, 2883, 2924, 2931, 2982, 3078, 2984, 2907, 2997, 2958, 2951, 2947, 2893, 3026, 3098, 3038, 3046, 2967, 2913, 2925, 2893, 3009, 2941, 2882, 2905, 2838, 2944, 2905, 2907, 2935, 2987, 3133, 2816, 2964, 2900, 2940, 1749], 'bins': [-0.11884970217943192, -0.1151333823800087, -0.11141706258058548, -0.10770074278116226, -0.10398442298173904, -0.10026810318231583, -0.09655178338289261, -0.09283546358346939, -0.08911914378404617, -0.08540282398462296, -0.08168650418519974, -0.07797018438577652, -0.0742538645863533, -0.07053754478693008, -0.06682122498750687, -0.06310490518808365, -0.05938858538866043, -0.05567226931452751, -0.051955949515104294, -0.048239629715681076, -0.04452330991625786, -0.04080699011683464, -0.03709067031741142, -0.033374350517988205, -0.029658030718564987, -0.02594171091914177, -0.02222539111971855, -0.018509071320295334, -0.014792751520872116, -0.011076431721448898, -0.0073601119220256805, -0.0036437921226024628, 7.2527676820755e-05, 0.0037888474762439728, 0.0075051672756671906, 0.011221487075090408, 0.014937806874513626, 0.018654126673936844, 0.02237044647336006, 0.02608676627278328, 0.029803086072206497, 0.033519405871629715, 0.03723572567105293, 0.04095204547047615, 0.04466836526989937, 0.048384685069322586, 0.052101004868745804, 0.05581732466816902, 0.05953364074230194, 0.06324996054172516, 0.06696628034114838, 0.0706826001405716, 0.07439891993999481, 0.07811523973941803, 0.08183155953884125, 0.08554787933826447, 0.08926419913768768, 0.0929805189371109, 0.09669683873653412, 0.10041315853595734, 0.10412947833538055, 0.10784579813480377, 0.11156211793422699, 0.11527843773365021, 0.11899475753307343]}, '_timestamp': 1717511651.659752}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc3.bias': {'_type': 'histogram', 'values': [6, 1, 9, 9, 9, 8, 4, 2, 4, 5, 11, 4, 8, 4, 4, 4, 5, 4, 8, 7, 3, 5, 4, 8, 7, 9, 12, 4, 5, 7, 12, 4, 4, 7, 6, 16, 8, 4, 7, 9, 8, 11, 8, 9, 5, 5, 7, 3, 7, 8, 12, 15, 3, 10, 7, 6, 13, 4, 5, 8, 3, 7, 8, 3], 'bins': [-0.04708856716752052, -0.045597661286592484, -0.044106755405664444, -0.042615849524736404, -0.041124943643808365, -0.039634037762880325, -0.038143131881952286, -0.036652226001024246, -0.03516132012009621, -0.03367041423916817, -0.03217950835824013, -0.030688602477312088, -0.02919769659638405, -0.02770679071545601, -0.02621588483452797, -0.02472497895359993, -0.02323407307267189, -0.0217431653290987, -0.020252259448170662, -0.018761353567242622, -0.017270447686314583, -0.015779541805386543, -0.014288636855781078, -0.012797730974853039, -0.011306824162602425, -0.009815918281674385, -0.008325012400746346, -0.006834106985479593, -0.005343200638890266, -0.0038522949907928705, -0.0023613888770341873, -0.0008704830543138087, 0.0006204228848218918, 0.0021113287657499313, 0.003602234646677971, 0.0050931405276060104, 0.00658404640853405, 0.00807495228946209, 0.009565858170390129, 0.011056764051318169, 0.012547669932246208, 0.014038576744496822, 0.015529482625424862, 0.017020387575030327, 0.018511293455958366, 0.020002199336886406, 0.021493105217814445, 0.022984011098742485, 0.024474918842315674, 0.025965824723243713, 0.027456730604171753, 0.028947636485099792, 0.030438542366027832, 0.03192944824695587, 0.03342035412788391, 0.03491126000881195, 0.03640216588973999, 0.03789307177066803, 0.03938397765159607, 0.04087488353252411, 0.04236578941345215, 0.04385669529438019, 0.04534760117530823, 0.04683850705623627, 0.04832941293716431]}, '_timestamp': 1717511651.659977}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.weight': {'_type': 'histogram', 'values': [203, 46, 30, 25, 8, 9, 8, 2, 4, 5, 2, 0, 2, 4, 1, 4, 3, 0, 3, 0, 6, 1, 0, 2, 3, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 6, 3, 6, 4, 2, 0, 3, 2, 2, 1, 1, 4, 0, 2, 0, 2, 0, 1, 2, 2, 0, 1, 0, 1, 5, 2, 3], 'bins': [0.998843252658844, 0.9988793730735779, 0.9989154934883118, 0.9989516139030457, 0.9989877343177795, 0.9990238547325134, 0.9990599751472473, 0.9990960955619812, 0.9991322159767151, 0.999168336391449, 0.9992044568061829, 0.9992406368255615, 0.9992767572402954, 0.9993128776550293, 0.9993489980697632, 0.9993851184844971, 0.999421238899231, 0.9994573593139648, 0.9994934797286987, 0.9995296001434326, 0.9995657205581665, 0.9996018409729004, 0.9996379613876343, 0.9996740818023682, 0.999710202217102, 0.9997463226318359, 0.9997824430465698, 0.9998185634613037, 0.9998546838760376, 0.9998908042907715, 0.9999269247055054, 0.9999630451202393, 0.9999991655349731, 1.000035285949707, 1.000071406364441, 1.0001075267791748, 1.0001436471939087, 1.0001797676086426, 1.0002158880233765, 1.0002520084381104, 1.0002881288528442, 1.0003242492675781, 1.000360369682312, 1.0003966093063354, 1.0004327297210693, 1.0004688501358032, 1.000504970550537, 1.000541090965271, 1.0005772113800049, 1.0006133317947388, 1.0006494522094727, 1.0006855726242065, 1.0007216930389404, 1.0007578134536743, 1.0007939338684082, 1.000830054283142, 1.000866174697876, 1.0009022951126099, 1.0009384155273438, 1.0009745359420776, 1.0010106563568115, 1.0010467767715454, 1.0010828971862793, 1.0011190176010132, 1.001155138015747]}, '_timestamp': 1717511651.6601658}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/bn3.bias': {'_type': 'histogram', 'values': [51, 14, 24, 15, 9, 22, 4, 4, 4, 2, 2, 4, 4, 7, 5, 3, 6, 6, 2, 3, 4, 3, 4, 3, 9, 2, 5, 4, 0, 0, 0, 0, 0, 0, 0, 0, 4, 9, 4, 2, 9, 6, 5, 8, 3, 6, 5, 5, 4, 2, 5, 1, 5, 2, 2, 7, 5, 4, 16, 4, 8, 10, 23, 43], 'bins': [-0.0011567415203899145, -0.001120594097301364, -0.0010844467906281352, -0.0010482993675395846, -0.001012151944451034, -0.0009760045795701444, -0.0009398571564815938, -0.0009037097916007042, -0.0008675623685121536, -0.000831415003631264, -0.0007952676387503743, -0.0007591202156618237, -0.0007229728507809341, -0.0006868254276923835, -0.0006506780628114939, -0.0006145306397229433, -0.0005783832748420537, -0.000542235909961164, -0.0005060884868726134, -0.0004699410928878933, -0.0004337936989031732, -0.00039764633402228355, -0.00036149894003756344, -0.00032535154605284333, -0.0002892041520681232, -0.0002530567580834031, -0.000216909364098683, -0.00018076198466587812, -0.000144614590681158, -0.0001084671966964379, -7.23198099876754e-05, -3.6172419640934095e-05, -2.5029294192790985e-08, 3.612236105254851e-05, 7.226975139928982e-05, 0.00010841713810805231, 0.00014456453209277242, 0.00018071192607749254, 0.00021685930551029742, 0.00025300669949501753, 0.00028915409347973764, 0.00032530148746445775, 0.00036144888144917786, 0.00039759627543389797, 0.0004337436403147876, 0.00046989103429950774, 0.0005060384282842278, 0.0005421858513727784, 0.0005783332162536681, 0.0006144805811345577, 0.0006506280042231083, 0.0006867753691039979, 0.0007229227921925485, 0.0007590701570734382, 0.0007952175801619887, 0.0008313649450428784, 0.000867512309923768, 0.0009036597330123186, 0.0009398070978932083, 0.0009759545209817588, 0.0010121018858626485, 0.001048249308951199, 0.0010843967320397496, 0.0011205440387129784, 0.001156691461801529]}, '_timestamp': 1717511651.660331}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.weight': {'_type': 'histogram', 'values': [9, 1, 9, 9, 2, 4, 9, 12, 6, 7, 7, 10, 6, 7, 3, 6, 7, 5, 9, 3, 7, 4, 7, 10, 10, 10, 5, 9, 7, 9, 7, 4, 3, 6, 13, 7, 7, 4, 5, 7, 8, 6, 8, 8, 9, 8, 10, 6, 5, 1, 5, 8, 5, 6, 3, 7, 11, 8, 8, 8, 2, 8, 7, 5], 'bins': [-0.16473053395748138, -0.15956835448741913, -0.15440616011619568, -0.14924398064613342, -0.14408180117607117, -0.13891960680484772, -0.13375742733478546, -0.1285952478647232, -0.12343306094408035, -0.1182708740234375, -0.11310869455337524, -0.10794650763273239, -0.10278432071208954, -0.09762214124202728, -0.09245995432138443, -0.08729777485132217, -0.08213558793067932, -0.07697340101003647, -0.07181122153997421, -0.06664903461933136, -0.061486851423978806, -0.05632466822862625, -0.0511624813079834, -0.046000298112630844, -0.04083811491727829, -0.035675931721925735, -0.030513746663928032, -0.02535156160593033, -0.020189378410577774, -0.015027194283902645, -0.009865010157227516, -0.004702826030552387, 0.0004593580961227417, 0.005621542222797871, 0.010783726349473, 0.015945911407470703, 0.021108094602823257, 0.026270277798175812, 0.031432464718818665, 0.03659464791417122, 0.04175683110952377, 0.04691901430487633, 0.05208119750022888, 0.057243384420871735, 0.06240556761622429, 0.06756775081157684, 0.0727299377322197, 0.07789211720228195, 0.0830543041229248, 0.08821649104356766, 0.09337867051362991, 0.09854085743427277, 0.10370303690433502, 0.10886522382497787, 0.11402741074562073, 0.11918959021568298, 0.12435177713632584, 0.1295139640569687, 0.13467614352703094, 0.1398383229970932, 0.14500051736831665, 0.1501626968383789, 0.15532487630844116, 0.1604870706796646, 0.16564925014972687]}, '_timestamp': 1717511651.660505}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'weights/fc4.bias': {'_type': 'histogram', 'values': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'bins': [-0.4535875618457794, -0.4379625618457794, -0.4223375618457794, -0.4067125618457794, -0.3910875618457794, -0.3754625618457794, -0.3598375618457794, -0.3442125618457794, -0.3285875618457794, -0.3129625618457794, -0.2973375618457794, -0.2817125618457794, -0.2660875618457794, -0.2504625618457794, -0.23483756184577942, -0.21921256184577942, -0.20358756184577942, -0.18796256184577942, -0.17233756184577942, -0.15671256184577942, -0.14108756184577942, -0.12546256184577942, -0.10983756184577942, -0.09421256184577942, -0.07858756184577942, -0.06296256184577942, -0.04733755812048912, -0.03171255812048912, -0.01608755812048912, -0.0004625581204891205, 0.01516244187951088, 0.03078744187951088, 0.04641244187951088, 0.06203744187951088, 0.07766243815422058, 0.09328743815422058, 0.10891243815422058, 0.12453743815422058, 0.14016243815422058, 0.15578743815422058, 0.17141243815422058, 0.18703743815422058, 0.20266243815422058, 0.21828743815422058, 0.23391243815422058, 0.24953743815422058, 0.2651624381542206, 0.2807874381542206, 0.2964124381542206, 0.3120374381542206, 0.3276624381542206, 0.3432874381542206, 0.3589124381542206, 0.3745374381542206, 0.3901624381542206, 0.4057874381542206, 0.4214124381542206, 0.4370374381542206, 0.4526624381542206, 0.4682874381542206, 0.4839124381542206, 0.4995374381542206, 0.515162467956543, 0.530787467956543, 0.546412467956543]}, '_timestamp': 1717511651.660685}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TP Reward Distribution': {'_type': 'histogram', 'values': [5, 5, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [2.6666666666666665, 4.2749999999999995, 5.883333333333333, 7.491666666666665, 9.1, 10.708333333333332, 12.316666666666665, 13.924999999999999, 15.533333333333331, 17.141666666666666, 18.75, 20.358333333333334, 21.966666666666665, 23.575, 25.183333333333334, 26.791666666666664, 28.4, 30.008333333333333, 31.616666666666664, 33.224999999999994, 34.83333333333333, 36.44166666666666, 38.05, 39.658333333333324, 41.26666666666666, 42.87499999999999, 44.48333333333333, 46.09166666666666, 47.699999999999996, 49.30833333333332, 50.91666666666666, 52.52499999999999, 54.133333333333326, 55.74166666666666, 57.349999999999994, 58.95833333333333, 60.566666666666656, 62.17499999999999, 63.783333333333324, 65.39166666666667, 67.0, 68.60833333333333, 70.21666666666667, 71.825, 73.43333333333334, 75.04166666666666, 76.64999999999999, 78.25833333333333, 79.86666666666666, 81.475, 83.08333333333333, 84.69166666666666, 86.3, 87.90833333333333, 89.51666666666667, 91.125, 92.73333333333333, 94.34166666666667, 95.94999999999999, 97.55833333333332, 99.16666666666666, 100.77499999999999, 102.38333333333333, 103.99166666666666, 105.6]}, '_timestamp': 1717511651.660855}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'TN Reward Distribution': {'_type': 'histogram', 'values': [2, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [3.533333333333333, 3.8010416666666664, 4.06875, 4.336458333333333, 4.604166666666666, 4.871874999999999, 5.139583333333333, 5.407291666666667, 5.675, 5.942708333333333, 6.210416666666666, 6.478125, 6.745833333333334, 7.013541666666667, 7.28125, 7.548958333333333, 7.816666666666666, 8.084375, 8.352083333333333, 8.619791666666666, 8.8875, 9.155208333333334, 9.422916666666666, 9.690625, 9.958333333333332, 10.226041666666667, 10.493749999999999, 10.761458333333334, 11.029166666666667, 11.296875, 11.564583333333333, 11.832291666666666, 12.1, 12.367708333333333, 12.635416666666666, 12.903125, 13.170833333333333, 13.438541666666666, 13.706249999999999, 13.973958333333332, 14.241666666666665, 14.509375, 14.777083333333334, 15.044791666666667, 15.3125, 15.580208333333333, 15.847916666666666, 16.115625, 16.383333333333333, 16.651041666666664, 16.91875, 17.186458333333334, 17.454166666666666, 17.721874999999997, 17.989583333333332, 18.257291666666667, 18.525, 18.792708333333334, 19.06041666666667, 19.328125, 19.59583333333333, 19.863541666666663, 20.13125, 20.398958333333333, 20.666666666666668]}, '_timestamp': 1717511651.661005}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FP Reward Distribution': {'_type': 'histogram', 'values': [7, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'bins': [2.6666666666666665, 7.3125, 11.958333333333332, 16.604166666666668, 21.25, 25.895833333333332, 30.541666666666668, 35.18749999999999, 39.83333333333333, 44.479166666666664, 49.12499999999999, 53.77083333333333, 58.416666666666664, 63.06249999999999, 67.70833333333333, 72.35416666666667, 77.0, 81.64583333333333, 86.29166666666667, 90.9375, 95.58333333333333, 100.22916666666667, 104.875, 109.52083333333333, 114.16666666666667, 118.8125, 123.45833333333333, 128.10416666666666, 132.74999999999997, 137.39583333333331, 142.04166666666666, 146.68749999999997, 151.33333333333331, 155.97916666666666, 160.62499999999997, 165.27083333333331, 169.91666666666666, 174.56249999999997, 179.20833333333331, 183.85416666666666, 188.49999999999997, 193.14583333333331, 197.79166666666666, 202.43749999999997, 207.08333333333331, 211.72916666666666, 216.37499999999997, 221.02083333333331, 225.66666666666666, 230.31249999999997, 234.95833333333331, 239.60416666666663, 244.24999999999997, 248.89583333333331, 253.54166666666663, 258.1875, 262.8333333333333, 267.4791666666667, 272.125, 276.7708333333333, 281.4166666666667, 286.0625, 290.7083333333333, 295.3541666666667, 300.0]}, '_timestamp': 1717511651.661142}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 1. Dropping entry: {'FN Reward Distribution': {'_type': 'histogram', 'values': [1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], 'bins': [3.533333333333333, 3.8010416666666664, 4.06875, 4.336458333333333, 4.604166666666666, 4.871874999999999, 5.139583333333333, 5.407291666666667, 5.675, 5.942708333333333, 6.210416666666666, 6.478125, 6.745833333333334, 7.013541666666667, 7.28125, 7.548958333333333, 7.816666666666666, 8.084375, 8.352083333333333, 8.619791666666666, 8.8875, 9.155208333333334, 9.422916666666666, 9.690625, 9.958333333333332, 10.226041666666667, 10.493749999999999, 10.761458333333334, 11.029166666666667, 11.296875, 11.564583333333333, 11.832291666666666, 12.1, 12.367708333333333, 12.635416666666666, 12.903125, 13.170833333333333, 13.438541666666666, 13.706249999999999, 13.973958333333332, 14.241666666666665, 14.509375, 14.777083333333334, 15.044791666666667, 15.3125, 15.580208333333333, 15.847916666666666, 16.115625, 16.383333333333333, 16.651041666666664, 16.91875, 17.186458333333334, 17.454166666666666, 17.721874999999997, 17.989583333333332, 18.257291666666667, 18.525, 18.792708333333334, 19.06041666666667, 19.328125, 19.59583333333333, 19.863541666666663, 20.13125, 20.398958333333333, 20.666666666666668]}, '_timestamp': 1717511651.661278}).
Epoch 700/1000, Train Loss: 0.7124317288398743, Val Loss: 0.38761743903160095, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 710/1000, Train Loss: 0.6979758441448212, Val Loss: 0.3871755301952362, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 720/1000, Train Loss: 0.7514413297176361, Val Loss: 0.3880605399608612, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 730/1000, Train Loss: 0.6464721262454987, Val Loss: 0.38817089796066284, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 740/1000, Train Loss: 0.7337587475776672, Val Loss: 0.38851088285446167, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 750/1000, Train Loss: 0.7947150468826294, Val Loss: 0.3886588513851166, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 760/1000, Train Loss: 0.7787408828735352, Val Loss: 0.3883330225944519, Train Acc: 0.7, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 770/1000, Train Loss: 0.7844911813735962, Val Loss: 0.38846880197525024, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 780/1000, Train Loss: 0.7426581084728241, Val Loss: 0.3880361318588257, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 790/1000, Train Loss: 0.6575276553630829, Val Loss: 0.38753169775009155, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 800/1000, Train Loss: 0.6955111026763916, Val Loss: 0.3876342177391052, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 810/1000, Train Loss: 0.750829815864563, Val Loss: 0.3872872292995453, Train Acc: 0.75, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 820/1000, Train Loss: 0.6297328174114227, Val Loss: 0.3872390687465668, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 830/1000, Train Loss: 0.6139687895774841, Val Loss: 0.3879987895488739, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 840/1000, Train Loss: 0.6989782154560089, Val Loss: 0.3884572982788086, Train Acc: 0.85, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 850/1000, Train Loss: 0.7247901558876038, Val Loss: 0.3876540958881378, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 860/1000, Train Loss: 0.7257236242294312, Val Loss: 0.38743525743484497, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 870/1000, Train Loss: 0.5229261517524719, Val Loss: 0.3872797191143036, Train Acc: 0.875, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 880/1000, Train Loss: 0.638619139790535, Val Loss: 0.38702189922332764, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 890/1000, Train Loss: 0.7180233001708984, Val Loss: 0.3861977458000183, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 900/1000, Train Loss: 0.7327224612236023, Val Loss: 0.3863871693611145, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 910/1000, Train Loss: 0.6963759064674377, Val Loss: 0.38724809885025024, Train Acc: 0.8, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 920/1000, Train Loss: 0.716558963060379, Val Loss: 0.38712090253829956, Train Acc: 0.675, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 930/1000, Train Loss: 0.7162586450576782, Val Loss: 0.38745394349098206, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 940/1000, Train Loss: 0.7438000440597534, Val Loss: 0.3866984248161316, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 950/1000, Train Loss: 0.6941692233085632, Val Loss: 0.3854599595069885, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 960/1000, Train Loss: 0.6377758681774139, Val Loss: 0.38461771607398987, Train Acc: 0.825, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 970/1000, Train Loss: 0.6141155511140823, Val Loss: 0.3851172924041748, Train Acc: 0.725, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 980/1000, Train Loss: 0.6761990785598755, Val Loss: 0.38558000326156616, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
Epoch 990/1000, Train Loss: 0.6998244524002075, Val Loss: 0.38520535826683044, Train Acc: 0.775, Val Acc: 0.800000011920929, LR: 5.782743556839077e-06
[33m[W 2024-06-04 10:34:35,310][39m Trial 0 failed with parameters: {'hidden_size': 432, 'learning_rate': 0.0005782743556839103, 'weight_decay': 0.0007579442012578331} because of the following error: The value None could not be cast to float..
[33m[W 2024-06-04 10:34:35,310][39m Trial 0 failed with value None.